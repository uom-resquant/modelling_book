<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Regression I | Modelling Criminological Data CRIM20452</title>
  <meta name="description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Regression I | Modelling Criminological Data CRIM20452" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Regression I | Modelling Criminological Data CRIM20452" />
  
  <meta name="twitter:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

<meta name="author" content="" />


<meta name="date" content="2024-12-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="appendix.html"/>
<link rel="next" href="regression-ii.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelling Criminological Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html"><i class="fa fa-check"></i><b>1</b> A first lesson about R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#install-r-rstudio"><i class="fa fa-check"></i><b>1.1</b> Install R &amp; RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#open-up-and-explore-rstudio"><i class="fa fa-check"></i><b>1.2</b> Open up and explore RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#customising-the-rstudio-look"><i class="fa fa-check"></i><b>1.3</b> Customising the RStudio look</a></li>
<li class="chapter" data-level="1.4" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#getting-organised-r-projects"><i class="fa fa-check"></i><b>1.4</b> Getting organised: R Projects</a></li>
<li class="chapter" data-level="1.5" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#functions-talk-to-your-computer"><i class="fa fa-check"></i><b>1.5</b> Functions: Talk to your computer</a></li>
<li class="chapter" data-level="1.6" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-packages"><i class="fa fa-check"></i><b>1.6</b> More on packages</a></li>
<li class="chapter" data-level="1.7" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#objects-creating-an-object"><i class="fa fa-check"></i><b>1.7</b> Objects: creating an object</a></li>
<li class="chapter" data-level="1.8" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-objects"><i class="fa fa-check"></i><b>1.8</b> More on objects</a></li>
<li class="chapter" data-level="1.9" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#naming-conventions-for-objects-in-r"><i class="fa fa-check"></i><b>1.9</b> Naming conventions for objects in R</a></li>
<li class="chapter" data-level="1.10" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-vectors"><i class="fa fa-check"></i><b>1.10</b> R object types: vectors</a></li>
<li class="chapter" data-level="1.11" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-data-frame"><i class="fa fa-check"></i><b>1.11</b> R object types: Data frame</a></li>
<li class="chapter" data-level="1.12" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#exploring-data"><i class="fa fa-check"></i><b>1.12</b> Exploring data</a></li>
<li class="chapter" data-level="1.13" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-data-types-factors"><i class="fa fa-check"></i><b>1.13</b> R data types: Factors</a></li>
<li class="chapter" data-level="1.14" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-import-data"><i class="fa fa-check"></i><b>1.14</b> How to import data</a></li>
<li class="chapter" data-level="1.15" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-use-comment"><i class="fa fa-check"></i><b>1.15</b> How to use ‘comment’</a></li>
<li class="chapter" data-level="1.16" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-quit-rstudio"><i class="fa fa-check"></i><b>1.16</b> How to Quit RStudio</a></li>
<li class="chapter" data-level="1.17" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#summary"><i class="fa fa-check"></i><b>1.17</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html"><i class="fa fa-check"></i><b>2</b> Getting to know your data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#causality-in-social-sciences"><i class="fa fa-check"></i><b>2.1</b> Causality in Social Sciences</a></li>
<li class="chapter" data-level="2.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-data-thanks-to-reproducibility"><i class="fa fa-check"></i><b>2.2</b> Getting data thanks to reproducibility</a></li>
<li class="chapter" data-level="2.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-a-sense-of-your-data"><i class="fa fa-check"></i><b>2.3</b> Getting a sense of your data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#first-steps"><i class="fa fa-check"></i><b>2.3.1</b> First steps</a></li>
<li class="chapter" data-level="2.3.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#on-tibbles-and-labelled-vectors"><i class="fa fa-check"></i><b>2.3.2</b> On tibbles and labelled vectors</a></li>
<li class="chapter" data-level="2.3.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#turning-variables-into-factors-and-changing-the-labels"><i class="fa fa-check"></i><b>2.3.3</b> Turning variables into factors and changing the labels</a></li>
<li class="chapter" data-level="2.3.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#looking-for-missing-data-and-other-anomalies"><i class="fa fa-check"></i><b>2.3.4</b> Looking for missing data and other anomalies</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#data-wrangling-with-dplyr"><i class="fa fa-check"></i><b>2.4</b> Data wrangling with dplyr</a></li>
<li class="chapter" data-level="2.5" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-single-verbs"><i class="fa fa-check"></i><b>2.5</b> Using dplyr single verbs</a></li>
<li class="chapter" data-level="2.6" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-for-grouped-operations"><i class="fa fa-check"></i><b>2.6</b> Using dplyr for grouped operations</a></li>
<li class="chapter" data-level="2.7" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#making-comparisons-with-numerical-outcomes"><i class="fa fa-check"></i><b>2.7</b> Making comparisons with numerical outcomes</a></li>
<li class="chapter" data-level="2.8" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html"><i class="fa fa-check"></i><b>3</b> Data visualisation with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#anatomy-of-a-plot"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a plot</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#what-graph-should-i-use"><i class="fa fa-check"></i><b>3.3</b> What graph should I use?</a></li>
<li class="chapter" data-level="3.4" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-histograms"><i class="fa fa-check"></i><b>3.4</b> Visualising numerical variables: Histograms</a></li>
<li class="chapter" data-level="3.5" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-density-plots"><i class="fa fa-check"></i><b>3.5</b> Visualising numerical variables: Density plots</a></li>
<li class="chapter" data-level="3.6" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-box-plots"><i class="fa fa-check"></i><b>3.6</b> Visualising numerical variables: Box plots</a></li>
<li class="chapter" data-level="3.7" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#exploring-relationships-between-two-quantitative-variables-scatterplots"><i class="fa fa-check"></i><b>3.7</b> Exploring relationships between two quantitative variables: scatterplots</a></li>
<li class="chapter" data-level="3.8" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplots-conditioning-in-a-third-variable"><i class="fa fa-check"></i><b>3.8</b> Scatterplots conditioning in a third variable</a></li>
<li class="chapter" data-level="3.9" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplot-matrix"><i class="fa fa-check"></i><b>3.9</b> Scatterplot matrix</a></li>
<li class="chapter" data-level="3.10" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#titles-legends-and-themes-in-ggplot2"><i class="fa fa-check"></i><b>3.10</b> Titles, legends, and themes in ggplot2</a></li>
<li class="chapter" data-level="3.11" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#plotting-categorical-data-bar-charts"><i class="fa fa-check"></i><b>3.11</b> Plotting categorical data: bar charts</a></li>
<li class="chapter" data-level="3.12" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#further-resources"><i class="fa fa-check"></i><b>3.12</b> Further resources</a></li>
<li class="chapter" data-level="3.13" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#summary-2"><i class="fa fa-check"></i><b>3.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Refresher on descriptive statistics &amp; data carpentry</a>
<ul>
<li class="chapter" data-level="4.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#getting-some-data-from-eurobarometer"><i class="fa fa-check"></i><b>4.2</b> Getting some data from Eurobarometer</a></li>
<li class="chapter" data-level="4.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#thinking-about-your-data-filtering-cases"><i class="fa fa-check"></i><b>4.3</b> Thinking about your data: filtering cases</a></li>
<li class="chapter" data-level="4.4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#selecting-variables-using-dplyrselect"><i class="fa fa-check"></i><b>4.4</b> Selecting variables: using <code>dplyr::select</code></a></li>
<li class="chapter" data-level="4.5" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#creating-summated-scales"><i class="fa fa-check"></i><b>4.5</b> Creating summated scales</a></li>
<li class="chapter" data-level="4.6" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#collapsing-categories-in-character-variables"><i class="fa fa-check"></i><b>4.6</b> Collapsing categories in character variables</a></li>
<li class="chapter" data-level="4.7" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#working-with-apparently-cryptic-variable-names-and-levels"><i class="fa fa-check"></i><b>4.7</b> Working with apparently cryptic variable names and levels</a></li>
<li class="chapter" data-level="4.8" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#recoding-factors"><i class="fa fa-check"></i><b>4.8</b> Recoding factors</a></li>
<li class="chapter" data-level="4.9" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#understanding-missing-data"><i class="fa fa-check"></i><b>4.9</b> Understanding missing data</a></li>
<li class="chapter" data-level="4.10" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#exploring-data-frames-visually"><i class="fa fa-check"></i><b>4.10</b> Exploring data frames visually</a></li>
<li class="chapter" data-level="4.11" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#a-quick-recap-on-descriptive-statistics"><i class="fa fa-check"></i><b>4.11</b> A quick recap on descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#central-tendency"><i class="fa fa-check"></i><b>4.11.1</b> Central Tendency</a></li>
<li class="chapter" data-level="4.11.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#dispersion"><i class="fa fa-check"></i><b>4.11.2</b> Dispersion</a></li>
<li class="chapter" data-level="4.11.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#bivariate-analysis"><i class="fa fa-check"></i><b>4.11.3</b> Bivariate analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#further-resources-1"><i class="fa fa-check"></i><b>4.12</b> Further resources</a></li>
<li class="chapter" data-level="4.13" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#summary-3"><i class="fa fa-check"></i><b>4.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html"><i class="fa fa-check"></i><b>5</b> Foundations of statistical inference: confidence intervals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#generating-random-data"><i class="fa fa-check"></i><b>5.2</b> Generating random data</a></li>
<li class="chapter" data-level="5.3" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#sampling-data-and-sampling-variability"><i class="fa fa-check"></i><b>5.3</b> Sampling data and sampling variability</a></li>
<li class="chapter" data-level="5.4" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#sampling-distributions-and-sampling-experiments"><i class="fa fa-check"></i><b>5.4</b> Sampling distributions and sampling experiments</a></li>
<li class="chapter" data-level="5.5" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#the-normal-distribution-and-confidence-intervals-with-known-standard-errors"><i class="fa fa-check"></i><b>5.5</b> The normal distribution and confidence intervals with known standard errors</a></li>
<li class="chapter" data-level="5.6" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#asymptotic-confidence-intervals-for-means-and-proportions-using-r"><i class="fa fa-check"></i><b>5.6</b> Asymptotic confidence intervals for means and proportions using R</a></li>
<li class="chapter" data-level="5.7" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#a-brief-intro-to-resampling-and-bootstraping"><i class="fa fa-check"></i><b>5.7</b> A brief intro to resampling and bootstraping</a></li>
<li class="chapter" data-level="5.8" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#what-about-comparisons-sampling-distribution-for-the-difference-of-two-means"><i class="fa fa-check"></i><b>5.8</b> What about comparisons? Sampling distribution for the difference of two means</a></li>
<li class="chapter" data-level="5.9" data-path="foundations-of-statistical-inference-confidence-intervals.html"><a href="foundations-of-statistical-inference-confidence-intervals.html#comparing-means-visually-by-using-error-bars-representing-confidence-intervals-inference-by-eye"><i class="fa fa-check"></i><b>5.9</b> Comparing means visually by using error bars representing confidence intervals: inference by eye</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypotheses.html"><a href="hypotheses.html"><i class="fa fa-check"></i><b>6</b> Hypotheses</a>
<ul>
<li class="chapter" data-level="6.1" data-path="hypotheses.html"><a href="hypotheses.html#the-logic-of-hypothesis-testing"><i class="fa fa-check"></i><b>6.1</b> The logic of hypothesis testing</a></li>
<li class="chapter" data-level="6.2" data-path="hypotheses.html"><a href="hypotheses.html#comparing-means-across-two-groups-the-t-test"><i class="fa fa-check"></i><b>6.2</b> Comparing means across two groups (the t test)</a></li>
<li class="chapter" data-level="6.3" data-path="hypotheses.html"><a href="hypotheses.html#what-does-a-significant-effect-mean"><i class="fa fa-check"></i><b>6.3</b> What does a significant effect mean?</a></li>
<li class="chapter" data-level="6.4" data-path="hypotheses.html"><a href="hypotheses.html#power-analysis"><i class="fa fa-check"></i><b>6.4</b> Power analysis</a></li>
<li class="chapter" data-level="6.5" data-path="hypotheses.html"><a href="hypotheses.html#comparing-means-across-several-groups-anova"><i class="fa fa-check"></i><b>6.5</b> Comparing means across several groups (ANOVA)</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="hypotheses.html"><a href="hypotheses.html#the-problem-with-multiple-comparisons"><i class="fa fa-check"></i><b>6.5.1</b> The problem with multiple comparisons</a></li>
<li class="chapter" data-level="6.5.2" data-path="hypotheses.html"><a href="hypotheses.html#anova"><i class="fa fa-check"></i><b>6.5.2</b> ANOVA</a></li>
<li class="chapter" data-level="6.5.3" data-path="hypotheses.html"><a href="hypotheses.html#checking-homogeneity-of-variance-and-dealing-with-unequal-spread"><i class="fa fa-check"></i><b>6.5.3</b> Checking homogeneity of variance and dealing with unequal spread</a></li>
<li class="chapter" data-level="6.5.4" data-path="hypotheses.html"><a href="hypotheses.html#checking-normality-and-dealing-with-problems"><i class="fa fa-check"></i><b>6.5.4</b> Checking normality and dealing with problems</a></li>
<li class="chapter" data-level="6.5.5" data-path="hypotheses.html"><a href="hypotheses.html#robust-anova"><i class="fa fa-check"></i><b>6.5.5</b> Robust ANOVA</a></li>
<li class="chapter" data-level="6.5.6" data-path="hypotheses.html"><a href="hypotheses.html#post-hoc-comparisons"><i class="fa fa-check"></i><b>6.5.6</b> Post Hoc Comparisons</a></li>
<li class="chapter" data-level="6.5.7" data-path="hypotheses.html"><a href="hypotheses.html#effect-size-for-anova"><i class="fa fa-check"></i><b>6.5.7</b> Effect size for ANOVA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html"><i class="fa fa-check"></i><b>7</b> Studying relationships between two factors</a>
<ul>
<li class="chapter" data-level="7.1" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#producing-cross-tabulations"><i class="fa fa-check"></i><b>7.1</b> Producing cross tabulations</a></li>
<li class="chapter" data-level="7.2" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#expected-frequencies-and-chi-square"><i class="fa fa-check"></i><b>7.2</b> Expected frequencies and Chi-Square</a></li>
<li class="chapter" data-level="7.3" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#residuals"><i class="fa fa-check"></i><b>7.3</b> Residuals</a></li>
<li class="chapter" data-level="7.4" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#gamma"><i class="fa fa-check"></i><b>7.4</b> Gamma</a></li>
<li class="chapter" data-level="7.5" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#odds-and-odd-ratios"><i class="fa fa-check"></i><b>7.5</b> Odds and odd ratios</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html"><i class="fa fa-check"></i><b>8</b> An introduction to regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#introduction-models-in-scientific-research"><i class="fa fa-check"></i><b>8.1</b> Introduction: models in scientific research</a></li>
<li class="chapter" data-level="8.2" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#motivating-regression"><i class="fa fa-check"></i><b>8.2</b> Motivating regression</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#correlation-analysis"><i class="fa fa-check"></i><b>8.2.1</b> Correlation analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>8.3</b> Fitting a simple regression model</a></li>
<li class="chapter" data-level="8.4" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#residuals-revisited-r-squared"><i class="fa fa-check"></i><b>8.4</b> Residuals revisited: R squared</a></li>
<li class="chapter" data-level="8.5" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#inference-with-regression"><i class="fa fa-check"></i><b>8.5</b> Inference with regression</a></li>
<li class="chapter" data-level="8.6" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#fitting-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>8.6</b> Fitting regression with categorical predictors</a></li>
<li class="chapter" data-level="8.7" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#motivating-multiple-regression"><i class="fa fa-check"></i><b>8.7</b> Motivating multiple regression</a></li>
<li class="chapter" data-level="8.8" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#fitting-and-interpreting-a-multiple-regression-model"><i class="fa fa-check"></i><b>8.8</b> Fitting and interpreting a multiple regression model</a></li>
<li class="chapter" data-level="8.9" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#presenting-your-regression-results."><i class="fa fa-check"></i><b>8.9</b> Presenting your regression results.</a></li>
<li class="chapter" data-level="8.10" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#testing-conditional-hypothesis-interactions"><i class="fa fa-check"></i><b>8.10</b> Testing conditional hypothesis: interactions</a></li>
<li class="chapter" data-level="8.11" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#regression-assumptions"><i class="fa fa-check"></i><b>8.11</b> Regression assumptions</a></li>
<li class="chapter" data-level="8.12" data-path="an-introduction-to-regression.html"><a href="an-introduction-to-regression.html#model-building-and-variable-selection"><i class="fa fa-check"></i><b>8.12</b> Model building and variable selection</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>9</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-logistic-regression"><i class="fa fa-check"></i><b>9.2</b> Fitting logistic regression</a></li>
<li class="chapter" data-level="9.3" data-path="logistic-regression.html"><a href="logistic-regression.html#assessing-model-fit-i-deviance-and-pseudo-r-squared"><i class="fa fa-check"></i><b>9.3</b> Assessing model fit I: deviance and pseudo r squared</a></li>
<li class="chapter" data-level="9.4" data-path="logistic-regression.html"><a href="logistic-regression.html#assessing-model-fit-ii-confusion-matrix-and-roc-curves"><i class="fa fa-check"></i><b>9.4</b> Assessing model fit II: confusion matrix and ROC curves</a></li>
<li class="chapter" data-level="9.5" data-path="logistic-regression.html"><a href="logistic-regression.html#interactions"><i class="fa fa-check"></i><b>9.5</b> Interactions</a></li>
<li class="chapter" data-level="9.6" data-path="logistic-regression.html"><a href="logistic-regression.html#further-resources-2"><i class="fa fa-check"></i><b>9.6</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="wrapping-up.html"><a href="wrapping-up.html"><i class="fa fa-check"></i><b>10</b> Wrapping up</a></li>
<li class="chapter" data-level="11" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>11</b> Appendix</a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="appendix.html"><a href="appendix.html#expected-frequencies"><i class="fa fa-check"></i><b>11.0.1</b> Expected frequencies</a></li>
<li class="chapter" data-level="11.1" data-path="appendix.html"><a href="appendix.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.1</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="appendix.html"><a href="appendix.html#fitting-logistic-regression-alternative"><i class="fa fa-check"></i><b>11.1.1</b> Fitting logistic regression: alternative</a></li>
<li class="chapter" data-level="11.1.2" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-deviance-and-pseudo-r-squared"><i class="fa fa-check"></i><b>11.1.2</b> Assessing model fit: deviance and pseudo r squared</a></li>
<li class="chapter" data-level="11.1.3" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-roc-curves"><i class="fa fa-check"></i><b>11.1.3</b> Assessing model fit: ROC curves</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="regression-i.html"><a href="regression-i.html"><i class="fa fa-check"></i><b>12</b> Regression I</a>
<ul>
<li class="chapter" data-level="12.1" data-path="regression-i.html"><a href="regression-i.html#hypotheses-the-logic-of-hypothesis-testing"><i class="fa fa-check"></i><b>12.1</b> Hypotheses: The logic of hypothesis testing</a></li>
<li class="chapter" data-level="12.2" data-path="regression-i.html"><a href="regression-i.html#introduction-models-in-scientific-research-1"><i class="fa fa-check"></i><b>12.2</b> Introduction: models in scientific research</a></li>
<li class="chapter" data-level="12.3" data-path="regression-i.html"><a href="regression-i.html#motivating-regression-1"><i class="fa fa-check"></i><b>12.3</b> Motivating regression</a></li>
<li class="chapter" data-level="12.4" data-path="regression-i.html"><a href="regression-i.html#fitting-a-simple-regression-model-1"><i class="fa fa-check"></i><b>12.4</b> Fitting a simple regression model</a></li>
<li class="chapter" data-level="12.5" data-path="regression-i.html"><a href="regression-i.html#residuals-revisited-r-squared-1"><i class="fa fa-check"></i><b>12.5</b> Residuals revisited: R squared</a></li>
<li class="chapter" data-level="12.6" data-path="regression-i.html"><a href="regression-i.html#inference-with-regression-1"><i class="fa fa-check"></i><b>12.6</b> Inference with regression</a></li>
<li class="chapter" data-level="12.7" data-path="regression-i.html"><a href="regression-i.html#fitting-regression-with-categorical-predictors-1"><i class="fa fa-check"></i><b>12.7</b> Fitting regression with categorical predictors</a></li>
<li class="chapter" data-level="12.8" data-path="regression-i.html"><a href="regression-i.html#power-analysis-1"><i class="fa fa-check"></i><b>12.8</b> Power analysis</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="regression-ii.html"><a href="regression-ii.html"><i class="fa fa-check"></i><b>13</b> Regression II</a>
<ul>
<li class="chapter" data-level="13.1" data-path="regression-ii.html"><a href="regression-ii.html#motivating-multiple-regression-1"><i class="fa fa-check"></i><b>13.1</b> Motivating multiple regression</a></li>
<li class="chapter" data-level="13.2" data-path="regression-ii.html"><a href="regression-ii.html#fitting-and-interpreting-a-multiple-regression-model-1"><i class="fa fa-check"></i><b>13.2</b> Fitting and interpreting a multiple regression model</a></li>
<li class="chapter" data-level="13.3" data-path="regression-ii.html"><a href="regression-ii.html#presenting-your-regression-results.-1"><i class="fa fa-check"></i><b>13.3</b> Presenting your regression results.</a></li>
<li class="chapter" data-level="13.4" data-path="regression-ii.html"><a href="regression-ii.html#rescaling-input-variables-to-assist-interpretation"><i class="fa fa-check"></i><b>13.4</b> Rescaling input variables to assist interpretation</a></li>
<li class="chapter" data-level="13.5" data-path="regression-ii.html"><a href="regression-ii.html#testing-conditional-hypothesis-interactions-1"><i class="fa fa-check"></i><b>13.5</b> Testing conditional hypothesis: interactions</a></li>
<li class="chapter" data-level="13.6" data-path="regression-ii.html"><a href="regression-ii.html#regression-assumptions-1"><i class="fa fa-check"></i><b>13.6</b> Regression assumptions</a></li>
<li class="chapter" data-level="13.7" data-path="regression-ii.html"><a href="regression-ii.html#plotting-residuals"><i class="fa fa-check"></i><b>13.7</b> Plotting residuals</a></li>
<li class="chapter" data-level="13.8" data-path="regression-ii.html"><a href="regression-ii.html#model-building-and-variable-selection-1"><i class="fa fa-check"></i><b>13.8</b> Model building and variable selection</a></li>
<li class="chapter" data-level="13.9" data-path="regression-ii.html"><a href="regression-ii.html#summary-exercise-for-this-week"><i class="fa fa-check"></i><b>13.9</b> Summary: exercise for this week</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html"><i class="fa fa-check"></i><b>14</b> Studying relationships between two factors</a>
<ul>
<li class="chapter" data-level="14.1" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#cross-tabulations"><i class="fa fa-check"></i><b>14.1</b> Cross-tabulations</a></li>
<li class="chapter" data-level="14.2" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#expected-frequencies-and-chi-square-1"><i class="fa fa-check"></i><b>14.2</b> Expected frequencies and Chi-Square</a></li>
<li class="chapter" data-level="14.3" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#odds-and-odd-ratios-1"><i class="fa fa-check"></i><b>14.3</b> Odds and odd ratios</a></li>
<li class="chapter" data-level="14.4" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#logistic-regression-2"><i class="fa fa-check"></i><b>14.4</b> Logistic regression</a></li>
<li class="chapter" data-level="14.5" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#fitting-logistic-regression-1"><i class="fa fa-check"></i><b>14.5</b> Fitting logistic regression</a></li>
<li class="chapter" data-level="14.6" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#assessing-model-fit-confusion-matrix"><i class="fa fa-check"></i><b>14.6</b> Assessing model fit: confusion matrix</a></li>
<li class="chapter" data-level="14.7" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#interactions-1"><i class="fa fa-check"></i><b>14.7</b> Interactions</a></li>
<li class="chapter" data-level="14.8" data-path="studying-relationships-between-two-factors-1.html"><a href="studying-relationships-between-two-factors-1.html#further-resources-3"><i class="fa fa-check"></i><b>14.8</b> Further resources</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelling Criminological Data CRIM20452</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-i" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> Regression I<a href="regression-i.html#regression-i" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="hypotheses-the-logic-of-hypothesis-testing" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Hypotheses: The logic of hypothesis testing<a href="regression-i.html#hypotheses-the-logic-of-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the last unit, we learned how to think about and build confidence intervals. We explained how we could use confidence intervals to deal with uncertainty when estimating population parameters. Confidence intervals as we will see are very commonly used in statistics. The other key inferential tool in data analysis is the hypothesis test. Today, we will focus on this second tool.</p>
<p>Last week, we saw how we could use confidence intervals as well to form a view about whether there are differences across groups in the population. We also saw how we could do “inference by eye” by virtue of visually comparing the confidence interval for the estimated mean value of fear for men and the estimated mean value of fear for women. Now, we are going to use a different approach to make inferences about the existence of these differences in the population: hypothesis testing.</p>
<p>Wikipedia defines a statistical hypothesis test as “a method of making decisions using data from a scientific study”. The logic of hypothesis testing is based on the work of a number of pioneers in the field of statistics, the British Ronald Fisher and Egon Pearson and the Polish Jerzy Neyman. This work is so important that <a href="https://simplystatistics.tumblr.com/post/18903448428/ra-fisher-is-the-most-influential-scientist-ever">some people argue</a> that Sir Ronald Fisher is one of the most influential academics in the history of science; not a small feat!</p>
<p>Hypothesis testing, or null hypothesis testing (NHST) as it is often referred to, proceeds in a number of steps.</p>
<p>1.<strong>We always start with a research question</strong></p>
<p>Our research questions in criminology can vary: Are ethnic minorities more likely to be stopped and searched? Does punishing offenders reduce crime? Is crime going down? Is self-control associated with offending? Here we are asking, are women more afraid of violent crime than men?</p>
<p>2.<strong>To answer a research question, we have to formulate at least one and sometimes several research hypotheses related to it</strong></p>
<p>A research hypothesis is simply a proposed answer to our research question that we can test by carrying out some research. Research hypothesis can be directional and non-directional:</p>
<blockquote>
<p>“When the research hypothesis does not indicate a specific type of outcome, stating only that there is a relationship or a difference, we say that it is a <strong>non-directional hypothesis</strong>. However, in those cases where a researcher has a very clear idea of what to expect -based on prior research evidence and/or theory -the research hypothesis may be more precise. In this case, the researcher may specify the nature of the relationship that is expected. Such a research hypothesis is called a <strong>directional hypothesis</strong>. When a directional hypothesis is used, the researcher states at the outset that he or she is interested in a specific type of outcome -for example, that one group has more arrests than another. Suppose we are interested in comparing the arrest records of drug-involved offenders with those of offenders who do not use drugs. Our research hypothesis might be simply that the arrest records of drug-involved offenders and offenders who do not use drugs are different (a nondirectional hypothesis). But based on prior knowledge of criminal behaviour among drug-involved offenders, we might want to state a directional hypothesis -that drug-involved offenders have more serious arrest records than non-drug-involved offenders do. One problem with choosing the latter option is that if we state our research hypothesis as a directional hypothesis, we are stating that we are not interested in outcomes that fall in the opposite direction. In criminal justice research, we can often be surprised by what we learn in a study. Accordingly, researchers generally are cautious in defining a directional research hypothesis” (Weisburd and Britt, 2010: 120)</p>
</blockquote>
<p>In our example, the research hypothesis will be non-directional and simply state that there are differences in the fear of violent crime among men and women.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>The following step is to formulate what is called a null hypothesis</strong></li>
</ol>
<p>In frequentist statistical inference we test hypothesis not in reference to the research hypothesis but in reference to the <strong>null hypothesis</strong>. The null hypothesis gains its name from the fact that it usually states that there is no relationship or no difference. “We make decisions about the hypothesis in relation to the null hypothesis rather than the research hypothesis. This is because the null hypothesis states that the parameter in which we are interested is a particular value” (Weisburd and Britt. 2010: 122).</p>
<p>In the example that we are using, the null hypothesis would be that there is no difference in the mean level of fear for males and females. This is the same than saying that the difference on fear for these two groups is zero. So, using the null hypothesis gives us a specific value. Typically, this value is zero, whereas the research hypothesis would be consistent with any of many values other than zero. We will see in a second why working with a precise value such as zero is helpful.</p>
<p>De Veaux et al. (2012) explain the logic of hypothesis testing as being similar to the logic of jury trials. In jury trials within the Common Law tradition, somebody is innocent until proven guilty:</p>
<blockquote>
<p>“the null hypothesis is that the defendant is innocent… The evidence takes the form of facts that seem to contradict the presumption of innocence. For us” (researchers) “this means collecting data… The next step is to judge the evidence. Evaluating the evidence is the responsibility of the jury in a trial, but it falls on your shoulders in hypothesis testing. The jury considers the evidence in light of the presumption of innocence and judges whether the evidence against the defendant would be plausible if the defendant were in fact innocent. Like the jury, you ask, <strong>‘Could these data plausibly have happened by chance if the null hypothesis were true?’</strong> If they are unlikely to have occurred, then the evidence raises a reasonable doubt about the null hypothesis. Ultimately, you must make a decision. The standard of beyond a reasonable doubt is wonderfully ambiguous… But when you ask the same question of your null hypothesis, you have the advantage of being able to quantify exactly how surprising the evidence would be were the null hypothesis true” (De Veaux et al. 2012: 479)</p>
</blockquote>
<p>So, in hypothesis testing, we look at our observed sample data. In our case, we look at the difference in fear of violent crime for males and females, and we ask ourselves the question: is the observed difference likely to have come from a population where the real difference is zero (as our null hypothesis specifies)? As you can see, testing against the null gives us the advantage of testing against a specific value. We can compare the value that we observe with zero, the precise value hypothesised by the null hypothesis. <em>The downside of it is that few things are exactly the same in nature</em>. So, to say that the level of fear of crime in men and women is probably not exactly the same (e.g., a difference of zero) is arguably not always going to give us the answer that we want.</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>The fundamental step in hypothesis testing, therefore, is the question: are the observed data surprising, given the null hypothesis? The key question is to determine exactly how likely the data we observed would be were the null hypothesis a true model of the world</strong></li>
</ol>
<p>So in essence we are after a probability, specifically a conditional probability (i.e, <em>the probability of our data if the null hypothesis were true</em>). We are trying to quantify the probability of seeing data like the one we have observed (a difference of 0.79 in our example) if we take as given that the null hypothesis is true (and the value “should be” zero). We call this probability the <strong>p value</strong>. You may have heard this term before. All it means, it bears repeating, is <strong>the probability of observing our data if the null hypothesis were true</strong>.</p>
<blockquote>
<p>“<strong>When the p value is high</strong>, then we can conclude that we have not seeing anything unusual. Events that have a high probability of happening happen often. The data are thus consistent with the model from the null hypothesis, and we have no reason to reject the null hypothesis. But we realize many other similar hypotheses could also account for the data we’ve seen, so we haven’t proven that the null hypothesis is true. The most we can say is that it doesn’t appear to be false. Formally, we fail to reject the null hypothesis. That’s a pretty weak conclusion, but it’s all we’re entitled to. <strong>When the p value is low enough</strong>, it says that it’s very unlikely we’d observed data like these if our null hypothesis were true. We started with a model. Now, the model tells us that the data are unlikely to have happened. The model and the data are at odds with each other, so we have to make a choice. Either the null hypothesis is correct, and we’ve just seen something remarkable, or the null hypothesis is wrong…” (De Veaux et al. 2012: 480)</p>
</blockquote>
<p>When is a p value high and when is low? Typically, we use criteria similar to those we use when constructing confidence intervals: we would consider a p value low enough if 95% of the time the observed data was considered to be inconsistent with the model proposed by our null hypothesis. So, we look for p-values that are smaller or bigger than 0.05.</p>
<p>That is, we look for differences that happen less than 5% of the time before we tentatively reject the null hypothesis. However, there is nothing sacrosanct about 95% and you could have good reasons to depart from this criterion (read page 123 to 128 of Weisburd and Britt, 2010 for further details). In fact, only last year, a number of researchers argued we should use a more stringent p value to address the crisis of reproducibility in science.</p>
<p>You will see that statistics books refer to the threshold we use to define a p-value as high or low as our <strong>level of statistical significance</strong> (also often referred to as the <strong>alpha level</strong>). In our example here (and all the others we will use this semester), we will use an alpha level of 0.05. That is, we will reject the null hypothesis <em>only if our p level is below that threshold</em>.</p>
<ol start="5" style="list-style-type: decimal">
<li><strong>After defining our research and null hypothesis and having taken a decision of how low our p value ought to be in order to reject the null hypothesis, we need to specify a model for testing this null hypothesis. All models make assumptions, so an important part of specifying a model is stating your assumptions and checking that they are not being violated. </strong></li>
</ol>
<p>Throughout the semester, we will cover a number of statistical tests, all with their own assumptions. These tests are appropriate in different circumstances (defined by their assumptions). Basically, what we will be doing in the remaining thematic units this semester is to explain what those circumstances are for each test so that you can choose the right one on each occasion. We will see later the assumptions made by the sort of hypothesis tests you use to compare means across groups.</p>
<ol start="6" style="list-style-type: decimal">
<li><strong>Once we’ve gone through all those steps comes the calculation of the test statistics and, based on the results, our decision</strong></li>
</ol>
<p>Different tests that we will encounter this semester have different formulas. Sometimes I will give you a basic description of what those formulas are doing, because it is good to know what is being computed for conceptual understanding. But the mechanics are handled by the computer. You won’t need to memorise those formulas nor calculate anything yourself.</p>
<p>The ultimate goal of these statistical tests for hypothesis testing is to obtain a p-value: the probability that the observed statistic (or a more extreme value) occurs if the null model is correct. If the p value is small enough (smaller than our alpha level: such as 0.05) then we will <strong>“reject the null hypothesis”</strong>. If it is not, we will <strong>“fail to reject the null hypothesis”</strong>. The language is important.</p>
<p>Whatever you decide, the <em>American Psychological Association Statistical Committee</em> recommends that it is always a good idea to report the p-value as an indication of the strength of the evidence. That is, not only report the finding to be significant or not, also report your actual p value.</p>
<p>One final word. P values have attracted a lot of debate over the years. They are often misunderstood, and people often read too much into them. They have also been used in a too simplistic way as a yardstick to decide what research findings are “worthy”. It is important to know what they are and how they work. It is particularly important not to overinterpret them either. The term statistical <strong>significance</strong> is particularly misleading because in common usage we think of something significant as important. But in our context is basically equivalent to say that we have observed in our sample/study may not be noise. That’s it. You will find all sorts of reactions to p values. Some people think we should ban them and use alternative approaches to data analysis (like Bayesian statistics). Others think that we should use more stringent thresholds (like p-values below .01 or .001). Yet most scientists still rely on them, so it is important that you learn what they are, their limitations, and how to interpret them in a correct manner.</p>
</div>
<div id="introduction-models-in-scientific-research-1" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Introduction: models in scientific research<a href="regression-i.html#introduction-models-in-scientific-research-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In science one of our main concerns is to develop models of the world, models that help us to understand the world a bit better or to predict how things will develop better. You can read more about modelling in scientific research <a href="https://www.visionlearning.com/en/library/Process-of-Science/49/Modeling-in-Scientific-Research/153">here</a>. Statistics provides a set of tools that help researchers build and test scientific models.</p>
<p>Our models can be simple. We can think that unemployment is a factor that may help us to understand why cities differ in their level of violent crime. We could express such a model like this:</p>
<p><img src="imgs/model1.PNG" /></p>
<p>Surely we know the world is complex and likely there are other things that may help us to understand why some cities have more crime than others. So, we may want to have tools that allow us to examine such models. Like, for example, the one below:</p>
<p><img src="imgs/model2.PNG" /></p>
<p>In this session we are going to cover regression analysis or, rather, we are beginning to talk about regression modelling. This form of analysis has been one the main technique of data analysis in the social sciences for many years and it belongs to a family of techniques called generalised linear models. Regression is a flexible model that allows you to “explain” or “predict” a given outcome (Y), variously called your outcome, response or dependent variable, as a function of a number of what is variously called inputs, features or independent, explanatory, or predictive variables (X1, X2, X3, etc.). Following Gelman and Hill (2007), we will try to stick for the most part to the terms outputs and inputs.</p>
<p>Today we will cover something that is called linear regression or ordinary least squares regression (OLS), which is a technique that you use when you are interested in explaining variation in an interval level variable. First we will see how you can use regression analysis when you only have one input, like in our first model, and then we will move to situations when we have several explanatory variables or inputs, like in our second model.</p>
<p>We will use a new dataset today, specifically the data used by Patrick Sharkey and his colleagues to study the effect of non profit organisations in the levels of crime. In <a href="https://books.wwnorton.com/books/Uneasy-Peace/"><em>“Uneasy Peace”</em></a> Prof Sharkey argues that one of the factors that contributed to the decline of crime from the 90s onwards was the role played by non profit community organisations to bring peace and services to deteriorated neighbourhoods. Watch this video and let’s have more theoretical background and learn about the research.</p>
<iframe src="https://www.youtube.com/embed/47IISvRXmpA" width="672" height="400px" data-external="1">
</iframe>
<p>In this session we will use the replication data from one of the papers that Prof Sharkey published studying this question. We can find this data in the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/46WIH0">Harvard Dataverse</a>. If you are interested in the specific study analysing this data you can find it <a href="https://journals.sagepub.com/doi/abs/10.1177/0003122417736289">here</a>.</p>
<div class="sourceCode" id="cb749"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb749-1"><a href="regression-i.html#cb749-1" tabindex="-1"></a>urlfile <span class="ot">&lt;-</span> <span class="st">&quot;https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/46WIH0/ARS2VS&quot;</span></span>
<span id="cb749-2"><a href="regression-i.html#cb749-2" tabindex="-1"></a>communitycrime <span class="ot">&lt;-</span> <span class="fu">read.table</span>(urlfile, <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\t</span><span class="st">&#39;</span>,<span class="at">header =</span> T)</span></code></pre></div>
<p>As before we create an object with the permanent <code>url</code> address and then we use a function to read the data into R. The data that can be saved using an <code>api</code> is in tab separated format. For this then we use the <code>read.table</code> function from base R. We pass two arguments to the function <code>sep= '\t'</code> is telling R this file is tab separated. The <code>header = T</code> function is telling R that is TRUE (T) that this file has a first row that acts as a header (this row has the name of the variables).</p>
<p>There are many more variables here that we are going to need, so let’s do some filtering and selection. Let’s just focus on a single year, 2012, the most recent in the dataset and just a few select variables.</p>
<div class="sourceCode" id="cb750"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb750-1"><a href="regression-i.html#cb750-1" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb750-2"><a href="regression-i.html#cb750-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">filter</span>(communitycrime, year <span class="sc">==</span> <span class="st">&quot;2012&quot;</span>)</span>
<span id="cb750-3"><a href="regression-i.html#cb750-3" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">select</span>(df, place_name, state_name, viol_r, black, lesshs, unemployed, fborn, incarceration, log_incarceraton, swornftime_r, log_viol_r, largest50)</span></code></pre></div>
<p>So now we have a more manageable data set that we can use for this session. The file includes a sample of 264 US cities (see <em>place_name</em>) across 44 of states:</p>
<div class="sourceCode" id="cb751"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb751-1"><a href="regression-i.html#cb751-1" tabindex="-1"></a><span class="fu">table</span>(df<span class="sc">$</span>state_name)</span></code></pre></div>
<pre><code>## 
##              Alabama               Alaska              Arizona 
##                    4                    1                    9 
##             Arkansas           California             Colorado 
##                    1                   65                   10 
##          Connecticut District of Columbia              Florida 
##                    5                    1                   18 
##              Georgia                Idaho             Illinois 
##                    2                    1                    8 
##              Indiana                 Iowa               Kansas 
##                    3                    3                    5 
##            Louisiana             Maryland        Massachusetts 
##                    4                    1                    3 
##             Michigan            Minnesota          Mississippi 
##                    6                    3                    1 
##             Missouri              Montana             Nebraska 
##                    5                    1                    2 
##               Nevada        New Hampshire           New Jersey 
##                    3                    1                    4 
##           New Mexico             New York       North Carolina 
##                    1                    5                    9 
##         North Dakota                 Ohio             Oklahoma 
##                    1                    5                    4 
##               Oregon         Pennsylvania         Rhode Island 
##                    4                    4                    1 
##       South Carolina         South Dakota            Tennessee 
##                    3                    1                    6 
##                Texas                 Utah             Virginia 
##                   30                    4                    7 
##           Washington            Wisconsin 
##                    6                    3</code></pre>
<p>The variables we have extracted contain information on the demographic composition of those cities (percent black population, percent without high school degree, percent unemployed, percent foreign born), some criminal justice ones (incarceration rate and the rate of sworn full time police officers). We also have measures of the violence rate and a binary indicator that tell us if the city is one of the 50 largest in the country.</p>
<p>We are going to look at the relationship between violent crime with a variable measuring unemployment (<em>unemployed</em>). Let’s look at the violence rate:</p>
<div class="sourceCode" id="cb753"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb753-1"><a href="regression-i.html#cb753-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span></code></pre></div>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.3.2</code></pre>
<div class="sourceCode" id="cb755"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb755-1"><a href="regression-i.html#cb755-1" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> viol_r)) <span class="sc">+</span></span>
<span id="cb755-2"><a href="regression-i.html#cb755-2" tabindex="-1"></a>  <span class="fu">geom_histogram</span>()</span></code></pre></div>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>As you can see is skewed. Violence is our target variable, the one we want to better understand. You may remember from when we cover ANOVA that some times we have to make transformations to variables so that the assumptions of the models we use are better respected. We will discuss this a bit in greater depth later. For now, just trust us in that rather than using <em>viol_r</em> we are going to use the logarithmic transformation of the violence rate, <em>log_viol_r</em>.</p>
<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb756-1"><a href="regression-i.html#cb756-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb756-2"><a href="regression-i.html#cb756-2" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> log_viol_r)) <span class="sc">+</span></span>
<span id="cb756-3"><a href="regression-i.html#cb756-3" tabindex="-1"></a>  <span class="fu">geom_histogram</span>()</span></code></pre></div>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Let’s look at the scatterplot between the log of the violence rate and unemployment:</p>
<div class="sourceCode" id="cb757"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb757-1"><a href="regression-i.html#cb757-1" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> unemployed, <span class="at">y =</span> log_viol_r)) <span class="sc">+</span></span>
<span id="cb757-2"><a href="regression-i.html#cb757-2" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha=</span>.<span class="dv">2</span>, <span class="at">position=</span><span class="st">&quot;jitter&quot;</span>) </span></code></pre></div>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>What do you think when looking at this scatterplot? Is there a relationship between violence and unemployment? Does it look as if cities that have a high score on the X axis (unemployment) also have a high score on the Y axis (violent crime)? It may be a bit hard to see but we would think there is certainly a trend.</p>
</div>
<div id="motivating-regression-1" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Motivating regression<a href="regression-i.html#motivating-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now, imagine that we play a game. Imagine we have all the names of the cities in a hat, and we randomly take one of names from the hat. You’re sitting in the audience, and you have to guess the level of violence (<em>log_viol_r</em>) for that city. Imagine that we pay £150 to the student that gets the closest to the right value. What would you guess if you only have one guess and you knew (as we do) how the log of violent crime is distributed?</p>
<div class="sourceCode" id="cb758"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb758-1"><a href="regression-i.html#cb758-1" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> log_viol_r)) <span class="sc">+</span> </span>
<span id="cb758-2"><a href="regression-i.html#cb758-2" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb758-3"><a href="regression-i.html#cb758-3" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">6.061</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb758-4"><a href="regression-i.html#cb758-4" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Density estimate and mean of log violent crime rate&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="sourceCode" id="cb760"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb760-1"><a href="regression-i.html#cb760-1" tabindex="-1"></a><span class="fu">summary</span>(df<span class="sc">$</span>log_viol_r)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   3.831   5.532   6.149   6.061   6.595   7.634</code></pre>
<p>If we only had one shot, we would go for the mean or the median (given the skew). Most of the cities have values clustered around those values, which is another way of saying they are bound to be not too far from them. It would be silly to say 4, for example, since there are very few cities with such low level of violence (as measured by <em>log_viol_r</em>).</p>
<p>Imagine, however, that now when we take the name of the city from the hat, you are also told how much unemployment there is in that city - so the value of the <em>unemployed</em> variable for the city that has been selected (for example 9). Imagine as well that you have the scatterplot that we produced earlier in front of you. Would you still go for the value of “six” (the mean) as your best guess for the value of the selected city?</p>
<p>We certainly would not go with the overall mean or median as my prediction any more. If somebody said to me, the value <em>unemployed</em> for the selected respondent is 9, we would be more inclined to guess the mean value for the cities <em>with that level of unemployment</em> (the conditional mean), rather than the overall mean across all the cities. Wouldn’t you?</p>
<p>If we plot the conditional means we can see that the mean <em>log_viol_r</em> for cities that report an unemployment of 9 is around 6.5. So you may be better off guessing that.</p>
<pre><code>## Warning in geom_line(data = df, aes(x = round(unemployed/0.12) * 0.12, y =
## log_viol_r), : Ignoring unknown parameters: `fun.y`</code></pre>
<pre><code>## No summary function supplied, defaulting to `mean_se()`</code></pre>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Linear regression tackles this problem using a slightly different approach. Rather than focusing on the conditional mean (smoothed or not), it draws a straight line that tries to capture the trend in the data. If we focus in the region of the scatterplot that are less sparse we see that this is an upward trend, suggesting that as the level of unemployment increases so does the level of violent crime.</p>
<p>Simple linear regression draws a single straight line of predicted values <strong>as the model for the data</strong>. This line would be a <strong>model</strong>, a <em>simplification</em> of the real world like any other model (e.g., a toy pistol, an architectural drawing, a subway map), that assumes that there is approximately a linear relationship between X and Y. Let’s draw the regression line:</p>
<div class="sourceCode" id="cb764"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb764-1"><a href="regression-i.html#cb764-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> unemployed, <span class="at">y =</span> log_viol_r)) <span class="sc">+</span></span>
<span id="cb764-2"><a href="regression-i.html#cb764-2" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> .<span class="dv">2</span>, <span class="at">position =</span> <span class="st">&quot;jitter&quot;</span>) <span class="sc">+</span></span>
<span id="cb764-3"><a href="regression-i.html#cb764-3" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) </span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The <code>geom_smooth</code> function asks for a geom with the regression line, <code>method=lm</code> asks for the linear regression line, <code>se=FALSE</code> asks for just the line to be printed, the other arguments specify the colour and thickness of the line.</p>
<p>What that line is doing is giving you guesses (predictions) for the values of violent crime based in the information that we have about the level of unemployment. It gives you one possible guess for the value of violence for every possible value of unemployment and links them all together in a straight line.</p>
<p>Another way of thinking about this line is as the best possible summary of the cloud of points that are represented in the scatterplot (if we can assume that a straight line would do a good job doing this). If we were to tell you to draw a straight line that best represents this pattern of points the regression line would be the one that best does it (if certain assumptions are met).</p>
<p>The linear model then is a model that takes the form of the equation of a straight line through the data. The line does not go through all the points. In fact, you can see is a slightly less accurate representation than the (smoothed) conditional means:</p>
<pre><code>## Warning in geom_line(data = df, aes(x = round(unemployed/0.12) * 0.12, y =
## log_viol_r), : Ignoring unknown parameters: `fun.y`</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## No summary function supplied, defaulting to `mean_se()`</code></pre>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>As De Veaux et al (2012: 179) highlight: “like all models of the real world, the line will be wrong, wrong in the sense that it can’t match reality exactly. But it can help us understand how the variables are associated”. A map is never a perfect representation of the world, the same happens with statistical models. Yet, as with maps, models can be helpful.</p>
</div>
<div id="fitting-a-simple-regression-model-1" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Fitting a simple regression model<a href="regression-i.html#fitting-a-simple-regression-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to draw a regression line we need to know two things:
(1) We need to know where the line begins, what is the value of Y (our dependent variable) when X (our independent variable) is 0, so that we have a point from which to start drawing the line. The technical name for this point is the <strong>intercept</strong>.
(2) And we need to know what is the <strong>slope</strong> of that line, that is, how inclined the line is, the angle of the line.</p>
<p>If you recall from elementary algebra (and you may not), the equation for any straight line is:
<span class="math inline">\(y = mx + b\)</span>
In statistics we use a slightly different notation, although the equation remains the same:
<span class="math inline">\(y = b_0 + b_1x\)</span></p>
<p>We need the origin of the line (<span class="math inline">\(b_0\)</span>) and the slope of the line (<span class="math inline">\(b_1\)</span>). How does R get the intercept and the slope for the green line? How does R know where to draw this line? We need to estimate these <strong>parameters</strong> (or <strong>coefficients</strong>) from the data. How? We don’t have the time to get into these more mathematical details now. You should study the <a href="http://link.springer.com/chapter/10.1007/978-1-4614-7138-7_3">required reading</a> to understand this (<em>required means it is required, it is not optional</em>)<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>. For now, suffice to say that for linear regression models like the one we cover here, when drawing the line, R tries to minimise the distance from every point in the scatterplot to the regression line using a method called <strong>least squares estimation</strong>.</p>
<p>In order to fit the model we use the <code>lm()</code> function using the formula specification <code>(Y ~ X)</code>. Typically you want to store your regression model in a “variable”, let’s call it <code>fit_1</code>:</p>
<div class="sourceCode" id="cb768"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb768-1"><a href="regression-i.html#cb768-1" tabindex="-1"></a>fit_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_viol_r <span class="sc">~</span> unemployed, <span class="at">data =</span> df)</span></code></pre></div>
<p>You will see in your R Studio global environment space that there is a new object called <code>fit_1</code> with 12 elements on it. We can get a sense for what this object is and includes using the functions we introduced in Week 1:</p>
<div class="sourceCode" id="cb769"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb769-1"><a href="regression-i.html#cb769-1" tabindex="-1"></a><span class="fu">class</span>(fit_1)</span></code></pre></div>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<div class="sourceCode" id="cb771"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb771-1"><a href="regression-i.html#cb771-1" tabindex="-1"></a><span class="fu">attributes</span>(fit_1)</span></code></pre></div>
<pre><code>## $names
##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;        
## 
## $class
## [1] &quot;lm&quot;</code></pre>
<p>R is telling us that this is an object of class <code>lm</code> and that it includes a number of attributes. One of the beauties of R is that you are producing all the results from running the model, putting them in an object, and then giving you the opportunity for using them later on. If you want to simply see the basic results from running the model you can use the <code>summary()</code> function.</p>
<div class="sourceCode" id="cb773"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb773-1"><a href="regression-i.html#cb773-1" tabindex="-1"></a><span class="fu">summary</span>(fit_1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log_viol_r ~ unemployed, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.81195 -0.44612  0.06817  0.45424  1.50438 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.57817    0.14899   30.73   &lt;2e-16 ***
## unemployed   0.23710    0.02302   10.30   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6254 on 262 degrees of freedom
## Multiple R-squared:  0.2882, Adjusted R-squared:  0.2855 
## F-statistic: 106.1 on 1 and 262 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Or if you prefer more parsimonious presentation you could use the <code>display()</code> function of the <code>arm</code> package:</p>
<div class="sourceCode" id="cb775"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb775-1"><a href="regression-i.html#cb775-1" tabindex="-1"></a>arm<span class="sc">::</span><span class="fu">display</span>(fit_1)</span></code></pre></div>
<pre><code>## lm(formula = log_viol_r ~ unemployed, data = df)
##             coef.est coef.se
## (Intercept) 4.58     0.15   
## unemployed  0.24     0.02   
## ---
## n = 264, k = 2
## residual sd = 0.63, R-Squared = 0.29</code></pre>
<p>For now we just want you to focus on the numbers in the “Estimate” column. The value of 4.58 estimated for the <strong>intercept</strong> is the “predicted” value for Y when X equals zero. This is the predicted value of the violence score <em>when the level of unemployment is zero</em>.</p>
<p>We then need the <span class="math inline">\(b_1\)</span> regression coefficient for for our independent variable, the value that will shape the <strong>slope</strong> in this scenario. This value is 0.24. This estimated regression coefficient for our independent variable has a convenient interpretation. When the value is positive, it tells us that <em>for every one unit increase in X there is a</em> <span class="math inline">\(b_1\)</span> <em>increase on Y</em>. If the coefficient is negative then it represents a decrease on Y. Here, we can read it as “for every one unit increase in the percentage of people unemployed, there is a 0.24 unit increase in the logarithm of the violence rate.”</p>
<p>Knowing these two parameters not only allows us to draw the line, we can also solve for any given value of X. Let’s go back to our guess-the-violence game. Imagine we tell you the level of unemployment is 4. What would be your best bet now? We can simply go back to our regression line equation and insert the estimated parameters:</p>
<p><span class="math inline">\(y = b_0 + b_1x\)</span><br />
<span class="math inline">\(y = 4.58 + 0.24 * 4\)</span><br />
<span class="math inline">\(y = 5.526564\)</span></p>
<p>Or if you don’t want to do the calculation yourself, you can use the <code>predict</code> function (differences are due to rounding error):</p>
<div class="sourceCode" id="cb777"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb777-1"><a href="regression-i.html#cb777-1" tabindex="-1"></a><span class="co">#First you name your stored model and then you identify the new data </span></span>
<span id="cb777-2"><a href="regression-i.html#cb777-2" tabindex="-1"></a><span class="co">#(which has to be in a data frame format and with a variable name matching the one in the original data set)</span></span>
<span id="cb777-3"><a href="regression-i.html#cb777-3" tabindex="-1"></a><span class="fu">predict</span>(fit_1, <span class="fu">data.frame</span>(<span class="at">unemployed =</span> <span class="fu">c</span>(<span class="dv">4</span>))) </span></code></pre></div>
<pre><code>##        1 
## 5.526564</code></pre>
<p>This is the expected value of Y, log of the violence rate, when X, unemployment is 5% of the population <em>according to our model</em> (according to our simplification of the real world, our simplification of the whole cloud of points into just one straight line). Look back at the scatterplot we produced earlier with the green line. Does it look as if the green line when X is 4 corresponds to a value of Y of 5.5?</p>
</div>
<div id="residuals-revisited-r-squared-1" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Residuals revisited: R squared<a href="regression-i.html#residuals-revisited-r-squared-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the output when we run the model above we saw there was something called the residuals. The residuals (in regression) are the differences between the observed values of Y for each case minus the predicted or expected value of Y, in other words the distances between each point in the dataset and the regression line (see the visual example below).</p>
<div class="float">
<img src="imgs/residual_01.png" alt="@http://www.shodor.org/" />
<div class="figcaption"><span class="citation">@http://www.shodor.org</span>/</div>
</div>
<p>You see that we have our line, which is our predicted values, and then we have the black dots which are our actually observed values. The distance between them is essentially the amount by which we were wrong, and all these distances between observed and predicted values are our residuals. <strong>Least square estimation</strong>, the “machine” we use to build the regression line, essentially aims to reduce the squared average of all these distances: that’s how it draws the line.</p>
<p>Why do we have residuals? Well, think about it. The fact that the line is not a perfect representation of the cloud of points makes sense, doesn’t it? You cannot predict perfectly what the value of Y is for every city just by looking ONLY at unemployment! This line only uses information regarding unemployment. This means that there’s bound to be some difference between our predicted level of violence given our knowledge of unemployment (the regression line) and the actual level of violence (the actual location of the points in the scatterplot). There are other things that matter not being taken into account by our model to predict the values of Y. There are other things that surely matter in terms of understanding violence. And then, of course, we have measurement error and other forms of noise.</p>
<p>We can re-write our equation like this if we want to represent each value of Y (rather than the predicted value of Y) then:
<span class="math inline">\(y = b_0 + b_1x + e(residuals)\)</span></p>
<p>The residuals capture how much variation is unexplained, how much we still have to learn if we want to understand variation in Y. A good model tries to maximise explained variation and reduce the magnitude of the residuals.</p>
<p>We can use information from the residuals to produce a measure of effect size, of how good our model is in predicting variation in our dependent variables. Remember our game where we try to guess violence (Y)? If we did not have any information about X our best bet for Y would be the mean of Y. The regression line aims to improve that prediction. By knowing the values of X we can build a regression line that aims to get us closer to the actual values of Y.</p>
<p><img src="https://people.richland.edu/james/ictcm/2004/weight2.png" /></p>
<p>The distance between the mean (our best guess without any other piece of information) and the observed value of Y is what we call the <strong>total variation</strong>. The residual is the difference between our predicted value of Y and the observed value of Y. This is what we cannot explain (i.e, variation in Y that is <em>unexplained</em>). The difference between the mean value of Y and the expected value of Y (the value given by our regression line) is how much better we are doing with our prediction by using information about X (i.e., in our previous example it would be variation in Y that can be <em>explained</em> by knowing about unemployment). How much closer the regression line gets us to the observed values. We can then contrast these two different sources of variation (explained and unexplained) to produce a single measure of how good our model is. The formula is as follows:</p>
<div class="float">
<img src="http://docs.oracle.com/cd/E40248_01/epm.1112/cb_statistical_11123500/images/graphics/r_squared_constant.gif" alt="formula" />
<div class="figcaption">formula</div>
</div>
<p>All this formula is doing is taking a ratio of the explained variation (the squared differences between the regression line and the mean of Y for each observation) by the total variation (the squared differences of the observed values of Y for each observation from the mean of Y). This gives us a measure of the <strong>percentage of variation in Y that is “explained” by X</strong>. If this sounds familiar is because it is a measure similar to eta squared in ANOVA that we cover in an earlier session.</p>
<p>As then we can take this value as a measure of the strength of our model. If you look at the R output you will see that the <span class="math inline">\(R^2\)</span> for our model was .29 (look at the multiple R square value in the output) . We can say that our model explains 29% of the variance in the fear of violent crime measure.</p>
<div class="sourceCode" id="cb779"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb779-1"><a href="regression-i.html#cb779-1" tabindex="-1"></a><span class="co">#As an aside, and to continue emphasising your appreciation of the object oriented nature of R, when we run the summary() function we are simply generating a list object of the class summary.lm.</span></span>
<span id="cb779-2"><a href="regression-i.html#cb779-2" tabindex="-1"></a><span class="fu">attributes</span>(<span class="fu">summary</span>(fit_1))</span></code></pre></div>
<pre><code>## $names
##  [1] &quot;call&quot;          &quot;terms&quot;         &quot;residuals&quot;     &quot;coefficients&quot; 
##  [5] &quot;aliased&quot;       &quot;sigma&quot;         &quot;df&quot;            &quot;r.squared&quot;    
##  [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot;    &quot;cov.unscaled&quot; 
## 
## $class
## [1] &quot;summary.lm&quot;</code></pre>
<div class="sourceCode" id="cb781"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb781-1"><a href="regression-i.html#cb781-1" tabindex="-1"></a><span class="co">#This means that we can access its elements if so we wish. So, for example, to obtain just the R Squared, we could ask for:</span></span>
<span id="cb781-2"><a href="regression-i.html#cb781-2" tabindex="-1"></a><span class="fu">summary</span>(fit_1)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.2881989</code></pre>
<p>Knowing how to interpret this is important. <span class="math inline">\(R^2\)</span> ranges from 0 to 1. The greater it is the more powerful our model is, the more explaining we are doing, the better we are able to account for variation in our outcome <span class="math inline">\(Y\)</span> with our input. In other words, the stronger the relationship is between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. As with all the other measures of effect size interpretation is a matter of judgement. You are advised to see what other researchers report in relation to the particular outcome that you may be exploring.<a href="http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit">This</a> is a reasonable explanation of how to interpret R-Squared.</p>
<p>Weisburd and Britt (2009: 437) suggest that in criminal justice you rarely see values for <span class="math inline">\(R^2\)</span> greater than .40. Thus, if your <span class="math inline">\(R^2\)</span> is larger than .40, you can assume you have a powerful model. When, on the other hand, <span class="math inline">\(R^2\)</span> is lower than .15 or .2 the model is likely to be viewed as relatively weak. Our observed r squared here is not too bad. There is considerably room for improvement if we want to develop a better model to explain violence<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>. In any case, many people would argue that <span class="math inline">\(R^2\)</span> is a bit overrated. You need to be aware of what it measures and the context in which you are using it. Read <a href="http://blog.minitab.com/blog/adventures-in-statistics/how-high-should-r-squared-be-in-regression-analysis">here</a> for some additional detail.</p>
</div>
<div id="inference-with-regression-1" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">12.6</span> Inference with regression<a href="regression-i.html#inference-with-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In real applications, we have access to a set of observations from which we can compute the least squares line, but the population regression line is unobserved. So our regression line is one of many that could be estimated. A different sample would produce a different regression line. The same sort of ideas that we introduced when discussing the estimation of sample means or proportions also apply here. If we estimate <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> from a particular sample, then our estimates won’t be exactly equal to <span class="math inline">\(b_0\)</span> and b1 in the population. But if we could average the estimates obtained over a very large number of data sets, the average of these estimates would equal the coefficients of the regression line in the population.</p>
<p>In the same way that we can compute the standard error when estimating the mean and explained in Week 5, we can compute standard errors for the regression coefficients to quantify our uncertainty about these estimates. These standard errors can in turn be used to produce confidence intervals. This would require us to assume that the residuals are normally distributed. As seen in the image, and for a simple regression model, you are assuming that the values of <span class="math inline">\(Y\)</span> are approximately normally distributed for each level of <span class="math inline">\(X\)</span>:</p>
<div class="float">
<img src="http://reliawiki.org/images/2/28/Doe4.3.png" alt="normalityresiduals" />
<div class="figcaption">normalityresiduals</div>
</div>
<p>In those circumstances we can trust the confidence intervals that we can draw around the regression line as in the image below:</p>
<div class="float">
<img src="http://2.bp.blogspot.com/-5e1_FibUjg4/Uv5BItdZqmI/AAAAAAAAA00/o1EfWZ0fk-g/s1600/SimpleLinearRegressionJags-ICON-FREQ-EST.png" title="Image taken from John Krushcke blog http://doingbayesiandataanalysis.blogspot.co.uk/" alt="estimated" />
<div class="figcaption">estimated</div>
</div>
<p>The dark-blue line marks the best fit. The two dark-pink lines mark the limits of the confidence interval. The light-pink lines show the sampling distributions around each of the confidence-interval limits (the many regression lines that would result from repeated sampling); notice that the best-fit line falls at the extreme of each sampling distribution.</p>
<p>You can also then perform standard hypothesis test on the coefficients. As we saw before when summarising the model, R will compute the standard errors and a <strong>t test</strong> for each of the coefficients.</p>
<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb783-1"><a href="regression-i.html#cb783-1" tabindex="-1"></a><span class="fu">summary</span>(fit_1)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##              Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 4.5781742 0.14898596 30.72890 7.474077e-89
## unemployed  0.2370975 0.02302022 10.29953 4.145166e-21</code></pre>
<p>In our example, we can see that the coefficient for our predictor here is statistically significant<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>.</p>
<p>We can also obtain confidence intervals for the estimated coefficients using the <code>confint()</code> function:</p>
<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb785-1"><a href="regression-i.html#cb785-1" tabindex="-1"></a><span class="fu">confint</span>(fit_1)</span></code></pre></div>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 4.2848120 4.8715365
## unemployed  0.1917693 0.2824257</code></pre>
<p><a href="http://www.sumsar.net/blog/2013/12/an-animation-of-the-construction-of-a-confidence-interval/">This blog post</a> provides a nice animation of the confidence interval and hypothesis testing.</p>
</div>
<div id="fitting-regression-with-categorical-predictors-1" class="section level2 hasAnchor" number="12.7">
<h2><span class="header-section-number">12.7</span> Fitting regression with categorical predictors<a href="regression-i.html#fitting-regression-with-categorical-predictors-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we have explained regression using a numeric input. It turns out we can also use regression with categorical explanatory variables. It is quite straightforward to run it.</p>
<p>We have one categorical variable in the dataset, <em>largest50</em>, identifying whether the city is one of the 50 largest in the country.</p>
<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb787-1"><a href="regression-i.html#cb787-1" tabindex="-1"></a><span class="fu">table</span>(df<span class="sc">$</span>largest50)</span></code></pre></div>
<pre><code>## 
##   0   1 
## 216  48</code></pre>
<div class="sourceCode" id="cb789"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb789-1"><a href="regression-i.html#cb789-1" tabindex="-1"></a><span class="fu">class</span>(df<span class="sc">$</span>largest50)</span></code></pre></div>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
<p>This variable is however stored in a numeric vector. We may want to change this to reflect the fact it is actually categorical.</p>
<div class="sourceCode" id="cb791"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb791-1"><a href="regression-i.html#cb791-1" tabindex="-1"></a>df<span class="sc">$</span>largest50 <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(df<span class="sc">$</span>largest50)</span>
<span id="cb791-2"><a href="regression-i.html#cb791-2" tabindex="-1"></a><span class="fu">class</span>(df<span class="sc">$</span>largest50)</span></code></pre></div>
<pre><code>## [1] &quot;factor&quot;</code></pre>
<p>Let’s rename the levels. In previous sessions we have illustrated how to do that with base R functions. Here we introduce a new package <code>forcats</code> that is worth considering when doing any work with factor variables. You can read more about it <a href="https://forcats.tidyverse.org">here</a>.</p>
<div class="sourceCode" id="cb793"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb793-1"><a href="regression-i.html#cb793-1" tabindex="-1"></a><span class="fu">library</span>(forcats)</span>
<span id="cb793-2"><a href="regression-i.html#cb793-2" tabindex="-1"></a>df<span class="sc">$</span>largest50 <span class="ot">&lt;-</span> <span class="fu">fct_recode</span>(df<span class="sc">$</span>largest50, <span class="at">Yes =</span> <span class="st">&quot;1&quot;</span>, <span class="at">No =</span> <span class="st">&quot;0&quot;</span>)</span>
<span id="cb793-3"><a href="regression-i.html#cb793-3" tabindex="-1"></a><span class="fu">table</span>(df<span class="sc">$</span>largest50)</span></code></pre></div>
<pre><code>## 
##  No Yes 
## 216  48</code></pre>
<p>We can explore if particularly large cities have higher rates of violence (remember a rate controls for population size, so if this were to be significant it would be telling us that it’s not just because there is more people in them!). This is how you would express the model:</p>
<div class="sourceCode" id="cb795"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb795-1"><a href="regression-i.html#cb795-1" tabindex="-1"></a>fit_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_viol_r <span class="sc">~</span> largest50, <span class="at">data=</span>df)</span></code></pre></div>
<p>Notice that there is nothing different in how we ask for the model. And see below the regression line:</p>
<p><img src="new_week_6_regression_I_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Although in the plot we still see a line, what we are really estimating here is the average of <em>log_viol_r</em> for each of the two categories.</p>
<p>Let’s have a look at the results:</p>
<div class="sourceCode" id="cb796"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb796-1"><a href="regression-i.html#cb796-1" tabindex="-1"></a><span class="fu">summary</span>(fit_2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log_viol_r ~ largest50, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.11616 -0.48286  0.02965  0.51523  1.49789 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.94763    0.04771 124.661  &lt; 2e-16 ***
## largest50Yes  0.62114    0.11189   5.551 6.94e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7012 on 262 degrees of freedom
## Multiple R-squared:  0.1052, Adjusted R-squared:  0.1018 
## F-statistic: 30.82 on 1 and 262 DF,  p-value: 6.943e-08</code></pre>
<p>As you will see the output does not look too different. But notice that in the print out you see how the row with the coefficient and other values for our input variable <em>largest50</em> we see that R is printing <code>largest50Yes</code>. What does this mean?</p>
<p>Remember week 6 and t tests? It turns out that a linear regression model with just one dichotomous categorical predictor is just the equivalent of a t test. When you only have one predictor the value of the intercept is the mean value of what we call the <strong>reference category</strong> and the coefficient for the slope tells you how much higher (if it is positive) or how much lower (if it is negative) is the mean value for the other category in your factor.</p>
<p>The reference category is the one for which R does not print the <em>level</em> next to the name of the variable for which it gives you the regression coefficient. Here we see that the named level is “Yes” (<code>largest50Yes</code>). That’s telling you that the reference category here is “No”. Therefore the Y intercept in this case is the mean value of violence for cities that are not the largest in the country, whereas the coefficient for the slope is telling you how much higher the mean value is for the largest cities in the country. Don’t believe me?</p>
<div class="sourceCode" id="cb798"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb798-1"><a href="regression-i.html#cb798-1" tabindex="-1"></a><span class="co">#Compute the mean for the smaller cities</span></span>
<span id="cb798-2"><a href="regression-i.html#cb798-2" tabindex="-1"></a><span class="fu">mean</span>(df<span class="sc">$</span>log_viol_r[df<span class="sc">$</span>largest50 <span class="sc">==</span> <span class="st">&quot;No&quot;</span>], <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 5.947628</code></pre>
<div class="sourceCode" id="cb800"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb800-1"><a href="regression-i.html#cb800-1" tabindex="-1"></a><span class="co">#Compute the difference between the two means</span></span>
<span id="cb800-2"><a href="regression-i.html#cb800-2" tabindex="-1"></a><span class="fu">mean</span>(df<span class="sc">$</span>log_viol_r[df<span class="sc">$</span>largest50 <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>], <span class="at">na.rm=</span><span class="cn">TRUE</span>) <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>log_viol_r[df<span class="sc">$</span>largest50 <span class="sc">==</span> <span class="st">&quot;No&quot;</span>], <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 0.6211428</code></pre>
<p>So, to reiterate, for a single binary predictor, the coefficient is nothing else than the difference between the mean of the two levels in your factor variable, between the averages in your two groups.</p>
<p>With categorical variables encoded as <strong>factors</strong> you always have a situation like this: a reference category and then as many additional coefficients as there are additional levels in your categorical variable. Each of these additional categories is input into the model as <strong>“dummy” variables</strong>. Here our categorical variable has two levels, thus we have only one dummy variable. There will always be one fewer dummy variable than the number of levels. The level with no dummy variable, females in this example, is known as the <strong>reference category</strong> or the <strong>baseline</strong>.</p>
<p>It turns out then that the regression table is printing out for us a t test of statistical significance for every input in the model. If we look at the table above this t value is 5.55 and the p value associated with it is near 0. This is indeed considerably lower than the conventional significance level of 0.05. So we could conclude that the probability of obtaining this value if the null hypothesis is true is very low. However, the observed r squared is also kind of poor. Read <a href="http://blog.minitab.com/blog/adventures-in-statistics/how-to-interpret-a-regression-model-with-low-r-squared-and-low-p-values">this</a> to understand a bit more this phenomenon of low p, but also low r-squared.</p>
<p>If rather than a binary explanatory variable, you had a factor with five levels. Then if you were to run a regression model this would result in a model with 4 dummy variables. The coefficient of each of these dummies would be telling you how much higher or lower (if the sign were negative) was the level of violence for each of the levels for which you have a dummy compared to your reference category or baseline. One thing that is important to keep in mind is that R by default will use as the baseline category the first level in your factor.</p>
<div class="sourceCode" id="cb802"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb802-1"><a href="regression-i.html#cb802-1" tabindex="-1"></a><span class="fu">levels</span>(df<span class="sc">$</span>largest50)</span></code></pre></div>
<pre><code>## [1] &quot;No&quot;  &quot;Yes&quot;</code></pre>
<p>In our case you can see “No” is listed first. Keep in mind for your assignment that levels in factors are often alphabetically listed, not in a particularly meaningful or useful way.
If you want to change this you may need to reorder the levels. See <a href="https://forcats.tidyverse.org/reference/fct_relevel.html">here</a> how to do this.</p>
</div>
<div id="power-analysis-1" class="section level2 hasAnchor" number="12.8">
<h2><span class="header-section-number">12.8</span> Power analysis<a href="regression-i.html#power-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we introduce the <code>pwr</code> package for power analysis. In the video lectures you had to watch as preparation for today we introduced the notion of power analysis. In order for a statistical test to be able to be effective, to be able to detect an effect, you need to have sufficient power. Power is related to the magnitude of the effect (it will be easier to detect stronger rather than weaker effects) and sample size (it will be easier to detect effects with large samples than with small samples). A problem with many scientific studies is that they are <em>underpowered</em>, the fail to reject the null hypothesis simply because they do not have sufficient power (often because the sample size is not large enough). Power analysis is generally done during the planning of an analysis so that you know what kind of sample you are going to need if you want to be able to run meaningfull hypothesis tests. That is you do your power analysis before you collect your data.</p>
<p>But we can also check how much power we have after the fact, just to ensure we are not failing to reject the null hypothesis as a consequence of insufficient power. For this purposes we can use the <code>pwr</code> package. For computing the power when comparing two sample means we use the <code>pwr.t2n.test</code> function. This function expects we provide the sample size of each group (if we are doing the power calculation after we have collated our data) and the effect size we may want to be able to detect. Let’s see how many people we have in each of our two groups (male and female) when assessing differences in fear of violent crime.</p>
<div class="sourceCode" id="cb804"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb804-1"><a href="regression-i.html#cb804-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;psych&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     %+%, alpha</code></pre>
<div class="sourceCode" id="cb807"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb807-1"><a href="regression-i.html#cb807-1" tabindex="-1"></a>BCS0708<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/eonk/dar_book/main/datasets/BCS0708.csv&quot;</span>)</span>
<span id="cb807-2"><a href="regression-i.html#cb807-2" tabindex="-1"></a><span class="fu">describeBy</span>(BCS0708<span class="sc">$</span>tcviolent, BCS0708<span class="sc">$</span>sex)</span></code></pre></div>
<pre><code>## 
##  Descriptive statistics by group 
## group: female
##    vars    n mean   sd median trimmed  mad   min  max range skew kurtosis   se
## X1    1 4475 0.33 1.04   0.23    0.25 0.96 -2.35 3.56  5.91 0.61     0.02 0.02
## ------------------------------------------------------------ 
## group: male
##    vars    n  mean   sd median trimmed  mad   min  max range skew kurtosis   se
## X1    1 3959 -0.27 0.86  -0.44   -0.36 0.69 -2.35 3.81  6.16  1.1     1.91 0.01</code></pre>
<p>Ok, so that is 4475 and 3959. Let’s say we want to detect even very small effect sizes. The functions in this package assume a default .05 level of statistical significance, although this is something we could change. So if we go with the default, we would write as follows:</p>
<div class="sourceCode" id="cb809"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb809-1"><a href="regression-i.html#cb809-1" tabindex="-1"></a><span class="fu">library</span>(pwr)</span></code></pre></div>
<pre><code>## Warning: package &#39;pwr&#39; was built under R version 4.3.2</code></pre>
<div class="sourceCode" id="cb811"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb811-1"><a href="regression-i.html#cb811-1" tabindex="-1"></a><span class="fu">pwr.t2n.test</span>(<span class="at">n1 =</span> <span class="dv">4475</span>, <span class="at">n2 =</span> <span class="dv">3959</span>, <span class="at">d=</span> <span class="fl">0.2</span>)</span></code></pre></div>
<pre><code>## 
##      t test power calculation 
## 
##              n1 = 4475
##              n2 = 3959
##               d = 0.2
##       sig.level = 0.05
##           power = 1
##     alternative = two.sided</code></pre>
<p>The function is telling us that we have a power of 1. The statistical power ranges from 0 to 1, and as statistical power increases, the probability of making a type II error (wrongly failing to reject the null) decreases. So with a power of 1 we are very unlikely indeed to failing to reject the null hypothesis when we should.</p>
<p>With sample sizes this large, you are unlikely to run into problems with power. But these things do matter in particular applications. Think for example of cases when you are trying to evaluate if a particular criminal justice intervention works. If you work with small samples you may wrongly conclude that your intervention didn’t make a difference (you fail to reject the null hypothesis) because you did not have sufficient statistical power. This was a common problem in older studies (see <a href="https://www.sciencedirect.com/science/article/pii/0047235289900044">here</a> for a review) and it is a problem that still persist to some extent (read <a href="https://www.tandfonline.com/doi/abs/10.1080/07418825.2018.1495252">this</a> more recent review).</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p><a href="http://link.springer.com/chapter/10.1007/978-1-4614-9170-5_15">This</a> is a fine chapter too if you struggle with the explanations in the required reading. Many universities, like the University of Manchester, have full access to Springer ebooks. You can also have a look at <a href="http://people.stern.nyu.edu/wgreene/Statistics/MultipleRegressionBasicsCollection.pdf">these notes</a>.<a href="regression-i.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p><a href="http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit">This</a> is a reasonable explanation of how to interpret R-Squared.<a href="regression-i.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p><a href="http://www.sumsar.net/blog/2013/12/an-animation-of-the-construction-of-a-confidence-interval/">This blog post</a> provides a nice animation of the confidence interval and hypothesis testing.<a href="regression-i.html#fnref13" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="appendix.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-ii.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

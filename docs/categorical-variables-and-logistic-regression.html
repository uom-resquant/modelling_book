<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Categorical variables and logistic regression | Modelling Criminological Data CRIM20452</title>
  <meta name="description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Categorical variables and logistic regression | Modelling Criminological Data CRIM20452" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Categorical variables and logistic regression | Modelling Criminological Data CRIM20452" />
  
  <meta name="twitter:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

<meta name="author" content="" />


<meta name="date" content="2025-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"/>
<link rel="next" href="statistical-inference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelling Criminological Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html"><i class="fa fa-check"></i><b>1</b> A first lesson about R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#install-r-rstudio"><i class="fa fa-check"></i><b>1.1</b> Install R &amp; RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#open-up-and-explore-rstudio"><i class="fa fa-check"></i><b>1.2</b> Open up and explore RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#customising-the-rstudio-look"><i class="fa fa-check"></i><b>1.3</b> Customising the RStudio look</a></li>
<li class="chapter" data-level="1.4" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#getting-organised-r-projects"><i class="fa fa-check"></i><b>1.4</b> Getting organised: R Projects</a></li>
<li class="chapter" data-level="1.5" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#functions-talk-to-your-computer"><i class="fa fa-check"></i><b>1.5</b> Functions: Talk to your computer</a></li>
<li class="chapter" data-level="1.6" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-packages"><i class="fa fa-check"></i><b>1.6</b> More on packages</a></li>
<li class="chapter" data-level="1.7" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#objects-creating-an-object"><i class="fa fa-check"></i><b>1.7</b> Objects: creating an object</a></li>
<li class="chapter" data-level="1.8" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-objects"><i class="fa fa-check"></i><b>1.8</b> More on objects</a></li>
<li class="chapter" data-level="1.9" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#naming-conventions-for-objects-in-r"><i class="fa fa-check"></i><b>1.9</b> Naming conventions for objects in R</a></li>
<li class="chapter" data-level="1.10" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-vectors"><i class="fa fa-check"></i><b>1.10</b> R object types: vectors</a></li>
<li class="chapter" data-level="1.11" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-data-frame"><i class="fa fa-check"></i><b>1.11</b> R object types: Data frame</a></li>
<li class="chapter" data-level="1.12" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#exploring-data"><i class="fa fa-check"></i><b>1.12</b> Exploring data</a></li>
<li class="chapter" data-level="1.13" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-data-types-factors"><i class="fa fa-check"></i><b>1.13</b> R data types: Factors</a></li>
<li class="chapter" data-level="1.14" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-import-data"><i class="fa fa-check"></i><b>1.14</b> How to import data</a></li>
<li class="chapter" data-level="1.15" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-use-comment"><i class="fa fa-check"></i><b>1.15</b> How to use ‘comment’</a></li>
<li class="chapter" data-level="1.16" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-quit-rstudio"><i class="fa fa-check"></i><b>1.16</b> How to Quit RStudio</a></li>
<li class="chapter" data-level="1.17" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#summary"><i class="fa fa-check"></i><b>1.17</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html"><i class="fa fa-check"></i><b>2</b> Getting to know your data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#causality-in-social-sciences"><i class="fa fa-check"></i><b>2.1</b> Causality in Social Sciences</a></li>
<li class="chapter" data-level="2.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-data-thanks-to-reproducibility"><i class="fa fa-check"></i><b>2.2</b> Getting data thanks to reproducibility</a></li>
<li class="chapter" data-level="2.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-a-sense-of-your-data"><i class="fa fa-check"></i><b>2.3</b> Getting a sense of your data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#first-steps"><i class="fa fa-check"></i><b>2.3.1</b> First steps</a></li>
<li class="chapter" data-level="2.3.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#on-tibbles-and-labelled-vectors"><i class="fa fa-check"></i><b>2.3.2</b> On tibbles and labelled vectors</a></li>
<li class="chapter" data-level="2.3.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#turning-variables-into-factors-and-changing-the-labels"><i class="fa fa-check"></i><b>2.3.3</b> Turning variables into factors and changing the labels</a></li>
<li class="chapter" data-level="2.3.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#looking-for-missing-data-and-other-anomalies"><i class="fa fa-check"></i><b>2.3.4</b> Looking for missing data and other anomalies</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#data-wrangling-with-dplyr"><i class="fa fa-check"></i><b>2.4</b> Data wrangling with dplyr</a></li>
<li class="chapter" data-level="2.5" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-single-verbs"><i class="fa fa-check"></i><b>2.5</b> Using dplyr single verbs</a></li>
<li class="chapter" data-level="2.6" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-for-grouped-operations"><i class="fa fa-check"></i><b>2.6</b> Using dplyr for grouped operations</a></li>
<li class="chapter" data-level="2.7" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#making-comparisons-with-numerical-outcomes"><i class="fa fa-check"></i><b>2.7</b> Making comparisons with numerical outcomes</a></li>
<li class="chapter" data-level="2.8" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html"><i class="fa fa-check"></i><b>3</b> Data visualisation with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#anatomy-of-a-plot"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a plot</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#what-graph-should-i-use"><i class="fa fa-check"></i><b>3.3</b> What graph should I use?</a></li>
<li class="chapter" data-level="3.4" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-histograms"><i class="fa fa-check"></i><b>3.4</b> Visualising numerical variables: Histograms</a></li>
<li class="chapter" data-level="3.5" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-density-plots"><i class="fa fa-check"></i><b>3.5</b> Visualising numerical variables: Density plots</a></li>
<li class="chapter" data-level="3.6" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-box-plots"><i class="fa fa-check"></i><b>3.6</b> Visualising numerical variables: Box plots</a></li>
<li class="chapter" data-level="3.7" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#exploring-relationships-between-two-quantitative-variables-scatterplots"><i class="fa fa-check"></i><b>3.7</b> Exploring relationships between two quantitative variables: scatterplots</a></li>
<li class="chapter" data-level="3.8" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplots-conditioning-in-a-third-variable"><i class="fa fa-check"></i><b>3.8</b> Scatterplots conditioning in a third variable</a></li>
<li class="chapter" data-level="3.9" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplot-matrix"><i class="fa fa-check"></i><b>3.9</b> Scatterplot matrix</a></li>
<li class="chapter" data-level="3.10" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#titles-legends-and-themes-in-ggplot2"><i class="fa fa-check"></i><b>3.10</b> Titles, legends, and themes in ggplot2</a></li>
<li class="chapter" data-level="3.11" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#plotting-categorical-data-bar-charts"><i class="fa fa-check"></i><b>3.11</b> Plotting categorical data: bar charts</a></li>
<li class="chapter" data-level="3.12" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#further-resources"><i class="fa fa-check"></i><b>3.12</b> Further resources</a></li>
<li class="chapter" data-level="3.13" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#summary-2"><i class="fa fa-check"></i><b>3.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Refresher on descriptive statistics &amp; data carpentry</a>
<ul>
<li class="chapter" data-level="4.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#getting-some-data-from-eurobarometer"><i class="fa fa-check"></i><b>4.2</b> Getting some data from Eurobarometer</a></li>
<li class="chapter" data-level="4.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#thinking-about-your-data-filtering-cases"><i class="fa fa-check"></i><b>4.3</b> Thinking about your data: filtering cases</a></li>
<li class="chapter" data-level="4.4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#selecting-variables-using-dplyrselect"><i class="fa fa-check"></i><b>4.4</b> Selecting variables: using <code>dplyr::select</code></a></li>
<li class="chapter" data-level="4.5" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#creating-summated-scales"><i class="fa fa-check"></i><b>4.5</b> Creating summated scales</a></li>
<li class="chapter" data-level="4.6" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#collapsing-categories-in-character-variables"><i class="fa fa-check"></i><b>4.6</b> Collapsing categories in character variables</a></li>
<li class="chapter" data-level="4.7" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#working-with-apparently-cryptic-variable-names-and-levels"><i class="fa fa-check"></i><b>4.7</b> Working with apparently cryptic variable names and levels</a></li>
<li class="chapter" data-level="4.8" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#recoding-factors"><i class="fa fa-check"></i><b>4.8</b> Recoding factors</a></li>
<li class="chapter" data-level="4.9" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#understanding-missing-data"><i class="fa fa-check"></i><b>4.9</b> Understanding missing data</a></li>
<li class="chapter" data-level="4.10" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#exploring-data-frames-visually"><i class="fa fa-check"></i><b>4.10</b> Exploring data frames visually</a></li>
<li class="chapter" data-level="4.11" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#a-quick-recap-on-descriptive-statistics"><i class="fa fa-check"></i><b>4.11</b> A quick recap on descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#central-tendency"><i class="fa fa-check"></i><b>4.11.1</b> Central Tendency</a></li>
<li class="chapter" data-level="4.11.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#dispersion"><i class="fa fa-check"></i><b>4.11.2</b> Dispersion</a></li>
<li class="chapter" data-level="4.11.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#bivariate-analysis"><i class="fa fa-check"></i><b>4.11.3</b> Bivariate analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#further-resources-1"><i class="fa fa-check"></i><b>4.12</b> Further resources</a></li>
<li class="chapter" data-level="4.13" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#summary-3"><i class="fa fa-check"></i><b>4.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html"><i class="fa fa-check"></i><b>5</b> Regression I: Mean differences</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#dependent-variable-numerical-independent-variable-binary"><i class="fa fa-check"></i><b>5.2</b> Dependent variable: numerical | Independent variable: binary</a></li>
<li class="chapter" data-level="5.3" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#calculating-mean-differences-in-r"><i class="fa fa-check"></i><b>5.3</b> Calculating mean differences in <code>R</code></a></li>
<li class="chapter" data-level="5.4" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#visual-exploration"><i class="fa fa-check"></i><b>5.4</b> Visual exploration</a></li>
<li class="chapter" data-level="5.5" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#using-linear-regression-to-calculate-mean-differences"><i class="fa fa-check"></i><b>5.5</b> Using linear regression to calculate mean differences</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#linear-regression"><i class="fa fa-check"></i><b>5.5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.5.2" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#linear-regression-as-a-difference-in-means-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Linear Regression as a Difference-in-Means Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size</a></li>
<li class="chapter" data-level="5.7" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#lab-exercises"><i class="fa fa-check"></i><b>5.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html"><i class="fa fa-check"></i><b>6</b> Regression II: numerical independent variables</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#motivating-regression"><i class="fa fa-check"></i><b>6.2</b> Motivating regression</a></li>
<li class="chapter" data-level="6.3" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>6.3</b> Fitting a simple regression model</a></li>
<li class="chapter" data-level="6.4" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#residuals-r-squared"><i class="fa fa-check"></i><b>6.4</b> Residuals: R squared</a></li>
<li class="chapter" data-level="6.5" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#is-this-the-same-as-last-week"><i class="fa fa-check"></i><b>6.5</b> Is this the same as last week?</a></li>
<li class="chapter" data-level="6.6" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#regression-assumptions"><i class="fa fa-check"></i><b>6.6</b> Regression assumptions</a></li>
<li class="chapter" data-level="6.7" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#lab-exercises-1"><i class="fa fa-check"></i><b>6.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><i class="fa fa-check"></i><b>7</b> Regression III: categorical independent variables and multiple regression models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#fitting-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>7.1</b> Fitting regression with categorical predictors</a></li>
<li class="chapter" data-level="7.2" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#motivating-multiple-regression"><i class="fa fa-check"></i><b>7.2</b> Motivating multiple regression</a></li>
<li class="chapter" data-level="7.3" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#fitting-and-interpreting-a-multiple-regression-model"><i class="fa fa-check"></i><b>7.3</b> Fitting and interpreting a multiple regression model</a></li>
<li class="chapter" data-level="7.4" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#rescaling-input-variables-to-assist-interpretation"><i class="fa fa-check"></i><b>7.4</b> Rescaling input variables to assist interpretation</a></li>
<li class="chapter" data-level="7.5" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#testing-conditional-hypothesis-interactions"><i class="fa fa-check"></i><b>7.5</b> Testing conditional hypothesis: interactions</a></li>
<li class="chapter" data-level="7.6" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#multicollinearity"><i class="fa fa-check"></i><b>7.6</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Categorical variables and logistic regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#cross-tabulations"><i class="fa fa-check"></i><b>8.1</b> Cross-tabulations</a></li>
<li class="chapter" data-level="8.2" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#regression-modelling-why-not-linear-regression"><i class="fa fa-check"></i><b>8.2</b> Regression modelling: why not linear regression?</a></li>
<li class="chapter" data-level="8.3" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#logistic-regression"><i class="fa fa-check"></i><b>8.3</b> Logistic regression</a></li>
<li class="chapter" data-level="8.4" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#fitting-logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Fitting logistic regression</a></li>
<li class="chapter" data-level="8.5" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#interactions"><i class="fa fa-check"></i><b>8.5</b> Interactions</a></li>
<li class="chapter" data-level="8.6" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#assessing-model-fit-confusion-matrix"><i class="fa fa-check"></i><b>8.6</b> Assessing model fit: confusion matrix</a></li>
<li class="chapter" data-level="8.7" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#lab-exercises-2"><i class="fa fa-check"></i><b>8.7</b> Lab Exercises</a></li>
<li class="chapter" data-level="8.8" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#further-resources-2"><i class="fa fa-check"></i><b>8.8</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference.html"><a href="statistical-inference.html#introduction-4"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference.html"><a href="statistical-inference.html#brief-theoretical-overview"><i class="fa fa-check"></i><b>9.2</b> Brief theoretical overview</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#sampling-distribution-central-limit-theorem"><i class="fa fa-check"></i>Sampling distribution &amp; Central limit theorem</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-confidence-intervals"><i class="fa fa-check"></i>Statistical inference: confidence intervals</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-hypothesis-testing"><i class="fa fa-check"></i>Statistical inference: hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-practice"><i class="fa fa-check"></i><b>9.3</b> Statistical inference in practice</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#means-and-proportions"><i class="fa fa-check"></i>Means and proportions</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-linear-regression-models"><i class="fa fa-check"></i>Statistical inference in linear regression models</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-multiple-linear-regression-models"><i class="fa fa-check"></i>Statistical inference in multiple linear regression models</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#presenting-results-from-regression-analysis"><i class="fa fa-check"></i>Presenting results from regression analysis</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-logistic-regression-models"><i class="fa fa-check"></i>Statistical inference in logistic regression models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="wrapping-up.html"><a href="wrapping-up.html"><i class="fa fa-check"></i><b>10</b> Wrapping up</a>
<ul>
<li class="chapter" data-level="10.1" data-path="wrapping-up.html"><a href="wrapping-up.html#tips-for-the-assignment"><i class="fa fa-check"></i><b>10.1</b> Tips for the assignment</a></li>
<li class="chapter" data-level="10.2" data-path="wrapping-up.html"><a href="wrapping-up.html#summarising-your-variables"><i class="fa fa-check"></i><b>10.2</b> Summarising your variables</a></li>
<li class="chapter" data-level="10.3" data-path="wrapping-up.html"><a href="wrapping-up.html#some-final-words"><i class="fa fa-check"></i><b>10.3</b> Some final words</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>11</b> Appendix</a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="appendix.html"><a href="appendix.html#expected-frequencies"><i class="fa fa-check"></i><b>11.0.1</b> Expected frequencies</a></li>
<li class="chapter" data-level="11.1" data-path="appendix.html"><a href="appendix.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.1</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="appendix.html"><a href="appendix.html#fitting-logistic-regression-alternative"><i class="fa fa-check"></i><b>11.1.1</b> Fitting logistic regression: alternative</a></li>
<li class="chapter" data-level="11.1.2" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-deviance-and-pseudo-r-squared"><i class="fa fa-check"></i><b>11.1.2</b> Assessing model fit: deviance and pseudo r squared</a></li>
<li class="chapter" data-level="11.1.3" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-roc-curves"><i class="fa fa-check"></i><b>11.1.3</b> Assessing model fit: ROC curves</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelling Criminological Data CRIM20452</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="categorical-variables-and-logistic-regression" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Categorical variables and logistic regression<a href="categorical-variables-and-logistic-regression.html#categorical-variables-and-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In previous weeks, we ventured into the world of bivariate analysis and even multivariate analysis. We learned that we always have a dependent variable (aka outcome or response variable), which measures the phenomenon we aim to explain, and one or more independent variables (aka explanatory or predictor variables), which represent the phenomena we think explain that.</p>
<p>We essentially learned how to handle every possible scenario when the dependent variable is numerical. For instance, if the independent variable is binary, we are interested in the mean difference—i.e., the extent to which the average score of the dependent variable is different in one group in relation to the other group. If the independent variable is categorical with more than two groups, we are interested in the extent to which average scores of the dependent variable are different in each of the groups in relation to a reference group. If the independent variable is numerical, we are interested in the extent to which average scores of the dependent variable are expected to change based on every one-unit increase in the independent variable. In all these cases, we saw that the linear regression model provides a powerful framework for quantifying these relationships. In every scenario, the slope coefficient represents the association between any independent variable and a numerical variable.</p>
<p>Now, however powerful and versatile linear regression models are, they rely on a key assumption: the dependent variable must be numerical. Models are very flexible as to what can go into the right-hand side of the linear equation, but the left-hand side is pretty restrictive. So, what do we do when we have a categorical dependent variable? That’s our focus this week! This week is all about categorical variables. We begin by introducing a common method for <em>describing</em> relationships between any two categorical variables: cross-tabulations. We then move on and learn about a new type of regression model—one that is <em>very</em> similar to everything we already know about linear regression models but designed to accommodate binary dependent variables: <strong>logistic regression models</strong>.</p>
<div id="cross-tabulations" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Cross-tabulations<a href="categorical-variables-and-logistic-regression.html#cross-tabulations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In earlier sessions, we covered how to run frequency distributions using the <code>table()</code> function. Cross tabulations, also called <strong>contingency tables</strong>, are essentially crossed frequency distributions, where you plot the frequency distributions of more than one categorical variable simultaneously. This semester, we are only going to explore <strong>two-way cross-tabulations</strong>, that is, contingency tables where we plot the frequency distribution of <em>two</em> categorical variables at the same time. Frequency distributions are a useful way of exploring categorical variables that do not have too many categories. By extension, cross-tabulations are a useful way of exploring relationships between two categorical variables that do not have too many levels or categories.</p>
<p>This week, we will use the <code>Arrests</code> dataset from the <code>effects</code> package. You can obtain details about this dataset and the variables included by using <code>help(Arrests, package="effects")</code>. If you don’t have that package, you must install and load it.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="categorical-variables-and-logistic-regression.html#cb477-1" tabindex="-1"></a><span class="co"># install package if you haven&#39;t done so</span></span>
<span id="cb477-2"><a href="categorical-variables-and-logistic-regression.html#cb477-2" tabindex="-1"></a><span class="do">## install.packages(&quot;effects&quot;)</span></span>
<span id="cb477-3"><a href="categorical-variables-and-logistic-regression.html#cb477-3" tabindex="-1"></a></span>
<span id="cb477-4"><a href="categorical-variables-and-logistic-regression.html#cb477-4" tabindex="-1"></a><span class="co"># load the &#39;effects&#39; package</span></span>
<span id="cb477-5"><a href="categorical-variables-and-logistic-regression.html#cb477-5" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb477-6"><a href="categorical-variables-and-logistic-regression.html#cb477-6" tabindex="-1"></a></span>
<span id="cb477-7"><a href="categorical-variables-and-logistic-regression.html#cb477-7" tabindex="-1"></a><span class="co"># load the dataset &quot;Arrests&quot;</span></span>
<span id="cb477-8"><a href="categorical-variables-and-logistic-regression.html#cb477-8" tabindex="-1"></a><span class="fu">data</span>(Arrests)<span class="co">#, package = &quot;effects&quot;)</span></span></code></pre></div>
<p>Please note that when importing data into <code>R</code> using the <code>data()</code> function, the imported object may not immediately appear in the Environment panel. Worry not—if you didn’t receive any error messages, the data frame <code>Arrests</code> (note the capitalised letter!) should be loaded into your <code>R</code> session. To double-check, let’s check the dimensions of this data frame.</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="categorical-variables-and-logistic-regression.html#cb478-1" tabindex="-1"></a><span class="co"># check the dimensions of the &#39;Arrests&#39; object</span></span>
<span id="cb478-2"><a href="categorical-variables-and-logistic-regression.html#cb478-2" tabindex="-1"></a><span class="fu">dim</span>(Arrests)</span></code></pre></div>
<pre><code>## [1] 5226    8</code></pre>
<p>We can see that the data frame <code>Arrests</code> has 5226 observations and 8 columns. The unit of analysis here is individuals arrested in Toronto for possession of marijuana. For each of these 5226 individuals, we have information on eight characteristics. Let’s check the names of the variables.</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="categorical-variables-and-logistic-regression.html#cb480-1" tabindex="-1"></a><span class="co"># check the names of all columns in &#39;Arrests&#39;</span></span>
<span id="cb480-2"><a href="categorical-variables-and-logistic-regression.html#cb480-2" tabindex="-1"></a><span class="fu">names</span>(Arrests)</span></code></pre></div>
<pre><code>## [1] &quot;released&quot; &quot;colour&quot;   &quot;year&quot;     &quot;age&quot;      &quot;sex&quot;      &quot;employed&quot; &quot;citizen&quot; 
## [8] &quot;checks&quot;</code></pre>
<p>This dataset includes information on police treatment of individuals arrested in Toronto for possession of marijuana. For example, it includes information on whether they are White or Black (<code>colour</code>), the year (<code>year</code>), their age (<code>age</code>), their gender (<code>sex</code>), whether they are currently employed (<code>employed</code>), whether they are Canadian citizens (<code>citizen</code>), and the number of entries in the criminal justice system (<code>checks</code>). Crucially, the dataset also contains information indicating whether the arrestee was released with a summons. In this case, the police could:</p>
<ul>
<li><p>Release the arrestee with a summons—like a parking ticket</p></li>
<li><p>Bring to the police station, held for bail, etc.—harsher treatment</p></li>
</ul>
<p>Let’s use the <code>table()</code> function to examine the frequency distribution of this variable</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="categorical-variables-and-logistic-regression.html#cb482-1" tabindex="-1"></a><span class="co"># produce frequency table</span></span>
<span id="cb482-2"><a href="categorical-variables-and-logistic-regression.html#cb482-2" tabindex="-1"></a><span class="fu">table</span>(Arrests<span class="sc">$</span>released)</span></code></pre></div>
<pre><code>## 
##   No  Yes 
##  892 4334</code></pre>
<p>4334 arrestees were released with a summons, whereas 892 received a harsher treatment. As usual, when dealing with frequency tables, it is often better to examine results in relation to proportions (or percentages) instead of raw counts. We can obtain proportions using the <code>prop.table()</code> function (do note that the <code>prop.table()</code> function requires a <code>table()</code> object as its argument!).</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="categorical-variables-and-logistic-regression.html#cb484-1" tabindex="-1"></a><span class="co"># store the frequency table under the name &quot;table_released&quot;</span></span>
<span id="cb484-2"><a href="categorical-variables-and-logistic-regression.html#cb484-2" tabindex="-1"></a>table_released <span class="ot">&lt;-</span> <span class="fu">table</span>(Arrests<span class="sc">$</span>released)</span>
<span id="cb484-3"><a href="categorical-variables-and-logistic-regression.html#cb484-3" tabindex="-1"></a></span>
<span id="cb484-4"><a href="categorical-variables-and-logistic-regression.html#cb484-4" tabindex="-1"></a><span class="co"># compute proportions of the frequency table</span></span>
<span id="cb484-5"><a href="categorical-variables-and-logistic-regression.html#cb484-5" tabindex="-1"></a><span class="fu">prop.table</span>(table_released)</span></code></pre></div>
<pre><code>## 
##       No      Yes 
## 0.170685 0.829315</code></pre>
<p>In other words, 82.93% of arrestees were released with a summons, and 17.07% received a harsher treatment. Let’s see if we can develop an understanding of the factors that affect this outcome. In particular, let’s assume our research goal is to investigate whether race is associated with harsher treatment. In this case, race is our independent variable, and the treatment offered to the arrestees is our dependent variable.</p>
<p>Let’s quickly examine our independent variable too.</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="categorical-variables-and-logistic-regression.html#cb486-1" tabindex="-1"></a><span class="co"># store the frequency table under the name &quot;table_colour&quot;</span></span>
<span id="cb486-2"><a href="categorical-variables-and-logistic-regression.html#cb486-2" tabindex="-1"></a>table_colour <span class="ot">&lt;-</span> <span class="fu">table</span>(Arrests<span class="sc">$</span>colour)</span>
<span id="cb486-3"><a href="categorical-variables-and-logistic-regression.html#cb486-3" tabindex="-1"></a></span>
<span id="cb486-4"><a href="categorical-variables-and-logistic-regression.html#cb486-4" tabindex="-1"></a><span class="co"># print the frequency table</span></span>
<span id="cb486-5"><a href="categorical-variables-and-logistic-regression.html#cb486-5" tabindex="-1"></a>table_colour</span></code></pre></div>
<pre><code>## 
## Black White 
##  1288  3938</code></pre>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="categorical-variables-and-logistic-regression.html#cb488-1" tabindex="-1"></a><span class="co"># compute proportions of the frequency table</span></span>
<span id="cb488-2"><a href="categorical-variables-and-logistic-regression.html#cb488-2" tabindex="-1"></a><span class="fu">prop.table</span>(table_colour)</span></code></pre></div>
<pre><code>## 
##   Black   White 
## 0.24646 0.75354</code></pre>
<p>Now we know that our sample contains 1288 (24.65%) Black arrestees and 3938 (75.35%) White arrestees.</p>
<p>Given that they are both categorical variables, we can start producing a cross-tabulation. We can simply use the <code>table()</code> function and another variable as a second argument in the function. The first variable included in the function will be represented by rows, whereas the second variable will be represented by columns. Conventionally, we usually include the dependent variable in the columns and the independent variable in the rows</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="categorical-variables-and-logistic-regression.html#cb490-1" tabindex="-1"></a><span class="co"># produce a cross-tabulation and store it under &quot;crosstab_released&quot;</span></span>
<span id="cb490-2"><a href="categorical-variables-and-logistic-regression.html#cb490-2" tabindex="-1"></a>crosstab_released <span class="ot">&lt;-</span> <span class="fu">table</span>(Arrests<span class="sc">$</span>colour, Arrests<span class="sc">$</span>released)</span>
<span id="cb490-3"><a href="categorical-variables-and-logistic-regression.html#cb490-3" tabindex="-1"></a></span>
<span id="cb490-4"><a href="categorical-variables-and-logistic-regression.html#cb490-4" tabindex="-1"></a><span class="co"># print the cross-tabulation</span></span>
<span id="cb490-5"><a href="categorical-variables-and-logistic-regression.html#cb490-5" tabindex="-1"></a>crosstab_released</span></code></pre></div>
<pre><code>##        
##           No  Yes
##   Black  333  955
##   White  559 3379</code></pre>
<p>By checking both frequency distributions at the same time, we can see that 333 arrestees are Black and received a harsher treatment, 955 are Black and were released with a summons, 559 are White and received a harsher treatment, and 3379 are White and were released with a summons. So, is there an association? Do we think that Black arrestees are treated more harshly than White arrestees?</p>
<p>As before, just looking at the raw counts is tricky. A better approach is to look at <em>proportions</em>. But which proportions? If we simply use the <code>prop.table()</code> function, <code>R</code> will calculate the <strong>total percentages</strong>—but that is not very helpful in addressing our question.</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="categorical-variables-and-logistic-regression.html#cb492-1" tabindex="-1"></a><span class="co"># compute total proportions of the cross-tabulation</span></span>
<span id="cb492-2"><a href="categorical-variables-and-logistic-regression.html#cb492-2" tabindex="-1"></a><span class="fu">prop.table</span>(crosstab_released)</span></code></pre></div>
<pre><code>##        
##                 No        Yes
##   Black 0.06371986 0.18274015
##   White 0.10696517 0.64657482</code></pre>
<p>We can see now that 6.37% of all arrestees are Black and received a harsher treatment, 18.27% are Black and were released with a summons, 10.7% are White and received a harsher treatment, and 64.66% are White and were released with a summons. In other words, what the <code>prop.table()</code> function did was just divide the raw count in each cell of the contingency table by the total number of arrestees in the dataset (i.e., 5226). Not very helpful.</p>
<p>Instead, we are only interested in the proportions that allow us to make meaningful comparisons. We already knew that Black arrestees were only 24.65% of our sample, so calculating <em>total percentages</em> does not seem to be very useful. Aside from <em>total percentages</em>, we can also compute <strong>row percentages</strong> and <strong>column percentages</strong>. For example, among only Black arrestees, what is the percentage that received a harsher treatment? What about among only White respondents? This is the idea of marginal frequencies and percentages!</p>
<p>Given that we always have an independent variable doing the explanation and a dependent variable being explained, we only need percentages that allow us to make comparisons for the dependent variable across our independent variable. In this case, we want to examine percentages of police treatment across groups of racial identity. In other words, because we conventionally display the dependent variable in the columns and the independent variable in the rows, we only want to request <strong>row percentages</strong>. We can request row percentages by adding the argument <code>margin = 1</code> into the <code>prop.table()</code> function:</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="categorical-variables-and-logistic-regression.html#cb494-1" tabindex="-1"></a><span class="co"># compute row percentages</span></span>
<span id="cb494-2"><a href="categorical-variables-and-logistic-regression.html#cb494-2" tabindex="-1"></a><span class="fu">prop.table</span>(crosstab_released, <span class="at">margin =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##        
##                No       Yes
##   Black 0.2585404 0.7414596
##   White 0.1419502 0.8580498</code></pre>
<p>What this argument does is calculate proportions one row at a time. Among Black arrestees only, 25.85% received a harsher treatment and 74.15% were released with a summons. Among White respondents only, 14.2% received a harsher treatment and 85.8% were received with a summons. Now, comparisons are easier to make! We can see that Black arrestees seem to be more likely to receive a harsher treatment than White arrestees (i.e., 25.85% vs. 14.2%); accordingly, White respondents are more likely to be released with a summons than Black respondents (i.e., 85.8% vs. 74.15%).</p>
<style>
details {
  margin-bottom: 1em; /* Adds space below each details block */
}
</style>
<details>
<summary>
<i>Attention! What about column percentages?</i>
</summary>
<p>We can also request column percentages by specifying <code>margin = 2</code> in the <code>prop.table()</code> function.</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="categorical-variables-and-logistic-regression.html#cb496-1" tabindex="-1"></a><span class="co"># compute column percentages</span></span>
<span id="cb496-2"><a href="categorical-variables-and-logistic-regression.html#cb496-2" tabindex="-1"></a><span class="fu">prop.table</span>(crosstab_released, <span class="at">margin =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##        
##                No       Yes
##   Black 0.3733184 0.2203507
##   White 0.6266816 0.7796493</code></pre>
<p>For example, among all arrestees who received a harsher treatment, 37.33% are Black and 62.67% are White. Among all arrestees who were released with a summons, 62.67% are Black and 77.96% are White.</p>
<p>This is of less substantial interest to us, as we are typically interested in assessing the distribution of the dependent variable across groups of the independent variable. If you include your independent variable in the columns (which is not conventional), then you might want to compute column percentages. Pay very close attention to this. It is a very common mistake to interpret a crosstab the wrong way if you don’t do it as explained here.</p>
<p>To reiterate, there are two rules for producing and reading cross tabs the right way. The first rule for reading cross-tabulations is that ** if your dependent variable defines the columns (as seen here), then you would need to ask for the row percentages. If, on the other hand, you decided that you preferred to have your dependent variable defining the rows, then you ask for the column percentages.**. Make sure you remember this.</p>
<br>
</details>
<p>While the <code>table()</code> and <code>prop.table()</code> functions do everything we need, some other functions in <code>R</code> allow a better-formatted inspection of cross-tabulations. For example, the <code>tbl_cross()</code> function from the <code>gtsummary</code> package provides a neat table with all the information we need. This is how it works.</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="categorical-variables-and-logistic-regression.html#cb498-1" tabindex="-1"></a><span class="co"># template for the tbl_cross() function</span></span>
<span id="cb498-2"><a href="categorical-variables-and-logistic-regression.html#cb498-2" tabindex="-1"></a><span class="fu">tbl_cross</span>(<span class="at">data =</span> mydataset, <span class="at">row =</span> independent_variable, <span class="at">col =</span> dependent_variable, <span class="at">percent =</span> <span class="st">&quot;row&quot;</span>)</span></code></pre></div>
<p>In our case, we can now use the <code>tbl_cross()</code> function to produce a neat cross-tabulation!</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="categorical-variables-and-logistic-regression.html#cb499-1" tabindex="-1"></a><span class="co"># remember to install the &#39;gtsummary&#39; package</span></span>
<span id="cb499-2"><a href="categorical-variables-and-logistic-regression.html#cb499-2" tabindex="-1"></a><span class="do">## install.packages(&quot;gtsummary&quot;)</span></span>
<span id="cb499-3"><a href="categorical-variables-and-logistic-regression.html#cb499-3" tabindex="-1"></a></span>
<span id="cb499-4"><a href="categorical-variables-and-logistic-regression.html#cb499-4" tabindex="-1"></a><span class="co"># load the &#39;gtsummary&#39; package</span></span>
<span id="cb499-5"><a href="categorical-variables-and-logistic-regression.html#cb499-5" tabindex="-1"></a><span class="fu">library</span>(gtsummary)</span>
<span id="cb499-6"><a href="categorical-variables-and-logistic-regression.html#cb499-6" tabindex="-1"></a></span>
<span id="cb499-7"><a href="categorical-variables-and-logistic-regression.html#cb499-7" tabindex="-1"></a><span class="co"># produce a neatly formatted cross-tabulation</span></span>
<span id="cb499-8"><a href="categorical-variables-and-logistic-regression.html#cb499-8" tabindex="-1"></a><span class="fu">tbl_cross</span>(<span class="at">data =</span> Arrests, <span class="at">row =</span> colour, <span class="at">col =</span> released, <span class="at">percent =</span> <span class="st">&quot;row&quot;</span>)</span></code></pre></div>
<div id="ncaghmdkus" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#ncaghmdkus table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#ncaghmdkus thead, #ncaghmdkus tbody, #ncaghmdkus tfoot, #ncaghmdkus tr, #ncaghmdkus td, #ncaghmdkus th {
  border-style: none;
}

#ncaghmdkus p {
  margin: 0;
  padding: 0;
}

#ncaghmdkus .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ncaghmdkus .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#ncaghmdkus .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ncaghmdkus .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ncaghmdkus .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ncaghmdkus .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ncaghmdkus .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ncaghmdkus .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ncaghmdkus .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ncaghmdkus .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ncaghmdkus .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ncaghmdkus .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ncaghmdkus .gt_spanner_row {
  border-bottom-style: hidden;
}

#ncaghmdkus .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#ncaghmdkus .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ncaghmdkus .gt_from_md > :first-child {
  margin-top: 0;
}

#ncaghmdkus .gt_from_md > :last-child {
  margin-bottom: 0;
}

#ncaghmdkus .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ncaghmdkus .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#ncaghmdkus .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#ncaghmdkus .gt_row_group_first td {
  border-top-width: 2px;
}

#ncaghmdkus .gt_row_group_first th {
  border-top-width: 2px;
}

#ncaghmdkus .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ncaghmdkus .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#ncaghmdkus .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#ncaghmdkus .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ncaghmdkus .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ncaghmdkus .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ncaghmdkus .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#ncaghmdkus .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ncaghmdkus .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ncaghmdkus .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ncaghmdkus .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ncaghmdkus .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ncaghmdkus .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ncaghmdkus .gt_left {
  text-align: left;
}

#ncaghmdkus .gt_center {
  text-align: center;
}

#ncaghmdkus .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ncaghmdkus .gt_font_normal {
  font-weight: normal;
}

#ncaghmdkus .gt_font_bold {
  font-weight: bold;
}

#ncaghmdkus .gt_font_italic {
  font-style: italic;
}

#ncaghmdkus .gt_super {
  font-size: 65%;
}

#ncaghmdkus .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#ncaghmdkus .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#ncaghmdkus .gt_indent_1 {
  text-indent: 5px;
}

#ncaghmdkus .gt_indent_2 {
  text-indent: 10px;
}

#ncaghmdkus .gt_indent_3 {
  text-indent: 15px;
}

#ncaghmdkus .gt_indent_4 {
  text-indent: 20px;
}

#ncaghmdkus .gt_indent_5 {
  text-indent: 25px;
}

#ncaghmdkus .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#ncaghmdkus div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>
<table class="gt_table" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
  <thead>
    <tr class="gt_col_headings gt_spanner_row">
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="2" colspan="1" scope="col" id="label"></th>
      <th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="2" scope="colgroup" id="released">
        <div class="gt_column_spanner"><span class='gt_from_md'>released</span></div>
      </th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="2" colspan="1" scope="col" id="stat_0"><span class='gt_from_md'>Total</span></th>
    </tr>
    <tr class="gt_col_headings">
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="stat_1"><span class='gt_from_md'>No</span></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="stat_2"><span class='gt_from_md'>Yes</span></th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td headers="label" class="gt_row gt_left">colour</td>
<td headers="stat_1" class="gt_row gt_center"><br /></td>
<td headers="stat_2" class="gt_row gt_center"><br /></td>
<td headers="stat_0" class="gt_row gt_center"><br /></td></tr>
    <tr><td headers="label" class="gt_row gt_left">    Black</td>
<td headers="stat_1" class="gt_row gt_center">333 (26%)</td>
<td headers="stat_2" class="gt_row gt_center">955 (74%)</td>
<td headers="stat_0" class="gt_row gt_center">1,288 (100%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    White</td>
<td headers="stat_1" class="gt_row gt_center">559 (14%)</td>
<td headers="stat_2" class="gt_row gt_center">3,379 (86%)</td>
<td headers="stat_0" class="gt_row gt_center">3,938 (100%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">Total</td>
<td headers="stat_1" class="gt_row gt_center">892 (17%)</td>
<td headers="stat_2" class="gt_row gt_center">4,334 (83%)</td>
<td headers="stat_0" class="gt_row gt_center">5,226 (100%)</td></tr>
  </tbody>
  
  
</table>
</div>
<p>It’s the same information but much better! Much less cluttered. We can see the frequency distribution <em>and</em> the <strong>marginal frequencies</strong>—i.e., totals by rows and columns which appear along the right and the bottom. We can also see the row percentages, which allows us to conclude that Black arrestees tend to receive a harsher treatment than White arrestees.</p>
<p>This can sound a bit confusing now. But as long as you remember, your dependent should always define the columns, and therefore, you should ask for the row percentages, and you should be fine. There are always students who get this wrong in the assignments and lose points as a result. Don’t let it be you.</p>
<p>The second rule for reading cross-tabulations the right way is this: <strong>You make the comparisons across the right percentages in the direction where they do not add up to a hundred</strong>. Another way of saying this is that you compare the percentages for each level of your dependent variable across the levels of your independent variable. In this case, we would, for example, compare the police decision to release arrestees with a summons or take them to the station. Looking at the first column only—arrestees who were not released, i.e., who received a harsher treatment—we can see that Black individuals are 26% and White individuals are 14%. Looking at the second column only—arrestees who were released with a summons—we can see that Black individuals are 74% and White individuals are 86%.</p>
<p><strong>Your turn!</strong> Using cross-tabulations, what do you conclude about the association between the following variables:</p>
<ul>
<li>Do arrestees with a citizenship status tend to receive harsher treatment by the police?</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>In this case, police treatment is our dependent variable. We operationalise it using the <code>released</code> variable, which indicates whether arrestees were released with a summons or received harsher treatment. Citizenship status is our independent variable, and the variable <code>citizen</code> indicates whether arrestees are Canadian citizens or not. Given that they are both categorical (binary) variables, we produce a cross-tabulation to assess whether they are associated.</p>
<p>We know that 17.07% arrestees received a harsher treatment and 82.93% were released with a summons. If citizenship status is not associated with police treatment (i.e., if the null hypothesis is true), we should expect roughly similar proportions among both citizens and non-citizens. If, however, proportions across citizens and non-citizens are different, that would serve as evidence that the two variables could be associated.</p>
<p>Let’s have a look at the cross-tabulation.</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="categorical-variables-and-logistic-regression.html#cb500-1" tabindex="-1"></a><span class="co"># produce a cross-tabulation between &#39;citizen&#39; and &#39;released&#39;</span></span>
<span id="cb500-2"><a href="categorical-variables-and-logistic-regression.html#cb500-2" tabindex="-1"></a><span class="fu">tbl_cross</span>(<span class="at">data =</span> Arrests, <span class="at">row =</span> citizen, <span class="at">col =</span> released, <span class="at">percent =</span> <span class="st">&quot;row&quot;</span>)</span></code></pre></div>
<div id="ocxfybvwbp" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#ocxfybvwbp table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#ocxfybvwbp thead, #ocxfybvwbp tbody, #ocxfybvwbp tfoot, #ocxfybvwbp tr, #ocxfybvwbp td, #ocxfybvwbp th {
  border-style: none;
}

#ocxfybvwbp p {
  margin: 0;
  padding: 0;
}

#ocxfybvwbp .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ocxfybvwbp .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#ocxfybvwbp .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ocxfybvwbp .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ocxfybvwbp .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ocxfybvwbp .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ocxfybvwbp .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ocxfybvwbp .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ocxfybvwbp .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ocxfybvwbp .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ocxfybvwbp .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ocxfybvwbp .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ocxfybvwbp .gt_spanner_row {
  border-bottom-style: hidden;
}

#ocxfybvwbp .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#ocxfybvwbp .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ocxfybvwbp .gt_from_md > :first-child {
  margin-top: 0;
}

#ocxfybvwbp .gt_from_md > :last-child {
  margin-bottom: 0;
}

#ocxfybvwbp .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ocxfybvwbp .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#ocxfybvwbp .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#ocxfybvwbp .gt_row_group_first td {
  border-top-width: 2px;
}

#ocxfybvwbp .gt_row_group_first th {
  border-top-width: 2px;
}

#ocxfybvwbp .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ocxfybvwbp .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#ocxfybvwbp .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#ocxfybvwbp .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ocxfybvwbp .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ocxfybvwbp .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ocxfybvwbp .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#ocxfybvwbp .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ocxfybvwbp .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ocxfybvwbp .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ocxfybvwbp .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ocxfybvwbp .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ocxfybvwbp .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ocxfybvwbp .gt_left {
  text-align: left;
}

#ocxfybvwbp .gt_center {
  text-align: center;
}

#ocxfybvwbp .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ocxfybvwbp .gt_font_normal {
  font-weight: normal;
}

#ocxfybvwbp .gt_font_bold {
  font-weight: bold;
}

#ocxfybvwbp .gt_font_italic {
  font-style: italic;
}

#ocxfybvwbp .gt_super {
  font-size: 65%;
}

#ocxfybvwbp .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#ocxfybvwbp .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#ocxfybvwbp .gt_indent_1 {
  text-indent: 5px;
}

#ocxfybvwbp .gt_indent_2 {
  text-indent: 10px;
}

#ocxfybvwbp .gt_indent_3 {
  text-indent: 15px;
}

#ocxfybvwbp .gt_indent_4 {
  text-indent: 20px;
}

#ocxfybvwbp .gt_indent_5 {
  text-indent: 25px;
}

#ocxfybvwbp .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#ocxfybvwbp div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>
<table class="gt_table" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
  <thead>
    <tr class="gt_col_headings gt_spanner_row">
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="2" colspan="1" scope="col" id="label"></th>
      <th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="2" scope="colgroup" id="released">
        <div class="gt_column_spanner"><span class='gt_from_md'>released</span></div>
      </th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="2" colspan="1" scope="col" id="stat_0"><span class='gt_from_md'>Total</span></th>
    </tr>
    <tr class="gt_col_headings">
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="stat_1"><span class='gt_from_md'>No</span></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="stat_2"><span class='gt_from_md'>Yes</span></th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td headers="label" class="gt_row gt_left">citizen</td>
<td headers="stat_1" class="gt_row gt_center"><br /></td>
<td headers="stat_2" class="gt_row gt_center"><br /></td>
<td headers="stat_0" class="gt_row gt_center"><br /></td></tr>
    <tr><td headers="label" class="gt_row gt_left">    No</td>
<td headers="stat_1" class="gt_row gt_center">212 (27%)</td>
<td headers="stat_2" class="gt_row gt_center">559 (73%)</td>
<td headers="stat_0" class="gt_row gt_center">771 (100%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    Yes</td>
<td headers="stat_1" class="gt_row gt_center">680 (15%)</td>
<td headers="stat_2" class="gt_row gt_center">3,775 (85%)</td>
<td headers="stat_0" class="gt_row gt_center">4,455 (100%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">Total</td>
<td headers="stat_1" class="gt_row gt_center">892 (17%)</td>
<td headers="stat_2" class="gt_row gt_center">4,334 (83%)</td>
<td headers="stat_0" class="gt_row gt_center">5,226 (100%)</td></tr>
  </tbody>
  
  
</table>
</div>
<p>The table indicates that, while only 15% of arrestees received harsher treatment among citizens, 27% of arrestees without citizenship status had the same decision. Similarly, while 85% of those with citizenship status were released with a summons, only 73% of those without citizenship status were released. This suggests that citizenship status seems to be associated with police treatment, as non-citizen individuals appear to be more likely to receive harsher treatment.</p>
<br>
</details>
<ul>
<li>Do male arrestees tend to receive harsher treatment by the police?</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>In this case, arrestees’ gender is our independent variable, and the variable <code>sex</code> indicates whether arrestees are recorded as male or female. Given that they are both categorical (binary) variables, we produce a cross-tabulation to assess whether they are associated.</p>
<p>We know that 17.07% arrestees received a harsher treatment and 82.93% were released with a summons. If gender is not associated with police treatment (i.e., if the null hypothesis is true), we should expect roughly similar proportions among both males and females. If, however, proportions across male and female arrestees are different, that would serve as evidence that the two variables could be associated.</p>
<p>Let’s have a look at the cross-tabulation.</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="categorical-variables-and-logistic-regression.html#cb501-1" tabindex="-1"></a><span class="co"># produce a cross-tabulation between &#39;sex&#39; and &#39;released&#39;</span></span>
<span id="cb501-2"><a href="categorical-variables-and-logistic-regression.html#cb501-2" tabindex="-1"></a><span class="fu">tbl_cross</span>(<span class="at">data =</span> Arrests, <span class="at">row =</span> sex, <span class="at">col =</span> released, <span class="at">percent =</span> <span class="st">&quot;row&quot;</span>)</span></code></pre></div>
<div id="hkzqdizppt" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#hkzqdizppt table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#hkzqdizppt thead, #hkzqdizppt tbody, #hkzqdizppt tfoot, #hkzqdizppt tr, #hkzqdizppt td, #hkzqdizppt th {
  border-style: none;
}

#hkzqdizppt p {
  margin: 0;
  padding: 0;
}

#hkzqdizppt .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#hkzqdizppt .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#hkzqdizppt .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#hkzqdizppt .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#hkzqdizppt .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#hkzqdizppt .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#hkzqdizppt .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#hkzqdizppt .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#hkzqdizppt .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#hkzqdizppt .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#hkzqdizppt .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#hkzqdizppt .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#hkzqdizppt .gt_spanner_row {
  border-bottom-style: hidden;
}

#hkzqdizppt .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#hkzqdizppt .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#hkzqdizppt .gt_from_md > :first-child {
  margin-top: 0;
}

#hkzqdizppt .gt_from_md > :last-child {
  margin-bottom: 0;
}

#hkzqdizppt .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#hkzqdizppt .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#hkzqdizppt .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#hkzqdizppt .gt_row_group_first td {
  border-top-width: 2px;
}

#hkzqdizppt .gt_row_group_first th {
  border-top-width: 2px;
}

#hkzqdizppt .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#hkzqdizppt .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#hkzqdizppt .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#hkzqdizppt .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#hkzqdizppt .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#hkzqdizppt .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#hkzqdizppt .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#hkzqdizppt .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#hkzqdizppt .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#hkzqdizppt .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#hkzqdizppt .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#hkzqdizppt .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#hkzqdizppt .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#hkzqdizppt .gt_left {
  text-align: left;
}

#hkzqdizppt .gt_center {
  text-align: center;
}

#hkzqdizppt .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#hkzqdizppt .gt_font_normal {
  font-weight: normal;
}

#hkzqdizppt .gt_font_bold {
  font-weight: bold;
}

#hkzqdizppt .gt_font_italic {
  font-style: italic;
}

#hkzqdizppt .gt_super {
  font-size: 65%;
}

#hkzqdizppt .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#hkzqdizppt .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#hkzqdizppt .gt_indent_1 {
  text-indent: 5px;
}

#hkzqdizppt .gt_indent_2 {
  text-indent: 10px;
}

#hkzqdizppt .gt_indent_3 {
  text-indent: 15px;
}

#hkzqdizppt .gt_indent_4 {
  text-indent: 20px;
}

#hkzqdizppt .gt_indent_5 {
  text-indent: 25px;
}

#hkzqdizppt .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#hkzqdizppt div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>
<table class="gt_table" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
  <thead>
    <tr class="gt_col_headings gt_spanner_row">
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="2" colspan="1" scope="col" id="label"></th>
      <th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="2" scope="colgroup" id="released">
        <div class="gt_column_spanner"><span class='gt_from_md'>released</span></div>
      </th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="2" colspan="1" scope="col" id="stat_0"><span class='gt_from_md'>Total</span></th>
    </tr>
    <tr class="gt_col_headings">
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="stat_1"><span class='gt_from_md'>No</span></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="stat_2"><span class='gt_from_md'>Yes</span></th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td headers="label" class="gt_row gt_left">sex</td>
<td headers="stat_1" class="gt_row gt_center"><br /></td>
<td headers="stat_2" class="gt_row gt_center"><br /></td>
<td headers="stat_0" class="gt_row gt_center"><br /></td></tr>
    <tr><td headers="label" class="gt_row gt_left">    Female</td>
<td headers="stat_1" class="gt_row gt_center">63 (14%)</td>
<td headers="stat_2" class="gt_row gt_center">380 (86%)</td>
<td headers="stat_0" class="gt_row gt_center">443 (100%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    Male</td>
<td headers="stat_1" class="gt_row gt_center">829 (17%)</td>
<td headers="stat_2" class="gt_row gt_center">3,954 (83%)</td>
<td headers="stat_0" class="gt_row gt_center">4,783 (100%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">Total</td>
<td headers="stat_1" class="gt_row gt_center">892 (17%)</td>
<td headers="stat_2" class="gt_row gt_center">4,334 (83%)</td>
<td headers="stat_0" class="gt_row gt_center">5,226 (100%)</td></tr>
  </tbody>
  
  
</table>
</div>
<p>The table indicates that 17% of male arrestees and 14% of female arrestees received harsher treatment, while 83% of male arrestees and 86% of female arrestees were released with a summons. These numbers are very similar and approximately follow the overall tendencies of arrestees who were (83%) and who were not released with a summons (17%). This suggests that gender does not seem to be associated with police treatment.</p>
<br>
</details>
</div>
<div id="regression-modelling-why-not-linear-regression" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Regression modelling: why not linear regression?<a href="categorical-variables-and-logistic-regression.html#regression-modelling-why-not-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!--
(Thiago) NOTE TO THE FUTURE

LET'S JUST TEACH THEM LINEAR PROBABILITY MODELS! This should be a "Regression IV: Linear probability models" class. We can also teach logistic regression, but focus should be on the LPM

This class could go: cross-tabs, odds ratios, LPMs. Done.
-->
<p>Ok, great. If we have two categorical variables, we can produce a cross-tabulation. This cross-tabulation can be helpful in providing evidence for or against the hypothesis that those two variables are associated with each other.</p>
<p>But what if the independent variable is numerical? Or, what if we have more than one independent variable? Or, what if we need to control for a third common factor to remove confounding bias? Or, what if we have two independent variables whose effects on the dependent variable are conditional upon each other? We spent three weeks studying linear regression models. When the dependent variable is numerical, we already know how to handle all these scenarios. Now, we need to handle these scenarios when the dependent variable is not numerical as well.</p>
<p>Can’t we simply fit linear regression models with categorical dependent variables?</p>
<p>No. Rather, the short answer is no. The long answer is… sometimes. When the dependent variable is binary, in some circumstances, we can fit a linear regression model—these are called linear probability models. But that’s beyond our scope this semester. As far as this course unit goes, let’s stick to the short answer: no, we cannot.</p>
<p>The reason why we cannot fit linear regression models with categorical dependent variables is simple: this violates some of the key assumptions in linear regression models. All assumptions somehow get back to the fact that the dependent variable is numerical and as close as possible to normally distributed. If the dependent variable is categorical, there is not much we can do…</p>
<p>…in terms of <em>linear</em> regression. But there are other types of regression models! One of them, <strong>logistic regression models</strong>, was designed specifically to handle categorical binary variables. That’s our focus now! We have binomial logistic regression models for binary dependent variables, ordinal logistic regression models for ordinal dependent variables, and multinomial logistic regression models for nominal (unordered) dependent variables. In this course unit, we are only going to learn about <strong>binomial logistic regression models</strong>—when the dependent variable is <strong>binary</strong>. Other types of categorical dependent variables will, unfortunately, not be covered this semester.</p>
<p>Now, let’s see how we can adapt everything we know about linear regression models!</p>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Logistic regression<a href="categorical-variables-and-logistic-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous sessions, we covered linear regression models, which can be used to model variation in a numerical response variable. Here, we introduce logistic regression, a technique you may use when your dependent variable is binary—i.e., categorical and has two possible levels.</p>
<p>In criminology, very often, you will be interested in binary outcomes—e.g., we might want to investigate why some people have been stopped by the police whereas others have not; or why some people have been victimised while others have not; or why some people engage in criminal conduct while others do not, etc.—and want to use a number of independent variables to study these outcomes. It is, then, helpful to understand how to use these models. Logistic regression is part of a broader family of models called <strong>generalised linear models</strong>—it’s essentially a technical expansion of linear regression models for dependent variables that are not numerical and do not follow a normal distribution. You should read the Wikipedia entry for this concept <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">here</a>.</p>
<p>With logistic regression, we model the probability of belonging to one of the levels in the binary outcome. For any combination of values for our independent variables, the model estimates the probability of presenting the outcome of interest. The mathematics behind this estimation is beyond the scope of our course unit; it suffices to say that while linear regression coefficients are estimated using <em>ordinary least squares</em> (see Chapter 6), logistic regression coefficients are estimated using <strong>maximum likelihood</strong>—one of the most important estimators in statistics.</p>
<p>In logistic regression, we also start with a model in which variables of interest (e.g., <span class="math inline">\(Y, X_1, X_2, ...\)</span>) are related to each other based on parameters that we need to estimate (e.g., <span class="math inline">\(\alpha, \beta_1, \beta_2, ...\)</span>). Remember what the linear model looks like:</p>
<p><span class="math display">\[
Y = \alpha + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + ... + \beta_n\cdot X_n
\]</span>
With logistic regression, we want to estimate a similar model. The right-hand side will remain unchanged: a combination of independent variables influencing the outcome based on estimated <span class="math inline">\(\beta\)</span> coefficients. However, the left-hand side of the equation does not work anymore. Because the dependent variable is now binary, the left-hand side of this linear model is no longer appropriate. If <span class="math inline">\(Y\)</span> is binary, that means that it can only have two values—e.g., 0 or 1. To avoid confusion and ensure that we know that this is a binary variable, let’s call it <span class="math inline">\(p\)</span> when <span class="math inline">\(y=1\)</span> and <span class="math inline">\(1-p\)</span> when <span class="math inline">\(y=0\)</span>. If we were to simply estimate a linear model with a binary dependent variable, then this linear equation could end up yielding all sorts of values of <span class="math inline">\(Y\)</span>—values that are not 0 or 1.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<p>The logistic regression model solves this issue by modelling the <em>odds</em> of an event occurring. Unlike linear models, which directly model the dependent variable, logistic regression focuses on the ratio <span class="math inline">\(\frac{p}{1-p}\)</span>, known as <strong>odds</strong>. If you are familiar with betting, you may already know a thing or two about odds. Odds represent how much more likely an event is to happen than not to happen.</p>
<p>Let’s go back to our example: marijuana-possession arrestees in Toronto. We know, from above, that most arrestees are released with a summons. Let’s remember the numbers:</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="categorical-variables-and-logistic-regression.html#cb502-1" tabindex="-1"></a><span class="co"># frequency table of variable &#39;released&#39;</span></span>
<span id="cb502-2"><a href="categorical-variables-and-logistic-regression.html#cb502-2" tabindex="-1"></a>table_released</span></code></pre></div>
<pre><code>## 
##   No  Yes 
##  892 4334</code></pre>
<p>So, we know that <span class="math inline">\(p=\)</span> 4334 arrestees were released with a summons and that <span class="math inline">\(1-p=\)</span> 892 received a harsher treatment. If we want to calculate the <strong>odds of being released with a summons</strong>, we simply need to calculate the ratio <span class="math inline">\(\frac{p}{1-p}=\frac{4334}{892}\)</span>. In this case, the odds are 4.86, which means that being released with a summons is 4.86 times more likely to occur than receiving harsher treatment.</p>
<p>Our goal will be to treat these odds as our main target! Is the colour of the arrestee associated with increases or decreases in the odds of being released with a summons? What about citizenship status? Is it associated with increases or decreases in the odds of being released with a summons? Going back to the linear model above, we are going to put the odds of an event occurring in the left-hand side of the equation!</p>
<p>Unfortunately, for a bunch of mathematical reasons that we don’t need to worry about for now, we cannot simply put the odds <span class="math inline">\(\frac{p}{1-p}\)</span> in the left-hand side of the equation. Instead, we need to put the <em>natural logarithm of the odds</em> in the left-hand side of the equation:</p>
<p><span class="math display">\[
log \bigg( \frac{p}{1-p} \bigg) = \alpha + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + ... + \beta_n\cdot X_n
\]</span>
Don’t worry about that logarithm in the equation. We will get rid of it soon. For now, what we can see is that the right-hand side of the equation remains unchanged, and the left-hand side of the equation includes some stuff that we still don’t know how to interpret. Let’s give a name for the stuff on the left-hand side of the equation: let’s call it <strong>logit</strong>. The <em>logit</em> of a binary variable is the natural logarithm of the odds of the event occurring. That’s why this model is called <strong>logistic regression</strong>!</p>
<p>But because the right-hand side of the equation remains unchanged, so will our interpretation! For instance, we can simply say that a one-unit increase in <span class="math inline">\(X_1\)</span> is associated with a <span class="math inline">\(\beta_1\)</span>-increase in the <em>log-odds</em> of <span class="math inline">\(Y\)</span>. The same applies to binary independent variables, categorical independent variables, multiple regression models, and interactions—everything that we learned about interpreting linear regression models applies here, as long we remember that everything refers to the <em>log-odds</em> of the dependent variable.</p>
<p>So, that’s great! Or almost… there’s only one problem: what does an increase or decrease in the <em>log-odds</em> of a variable even mean? Interpreting the log odds scale is something some people do not find very intuitive. To make things more intuitive, we need to get rid of the logarithm and get back to odds! After all, what we want to know is the extent to which independent variables are associated with increases or decreases in the <em>odds</em> of an event occurring.</p>
<p>How do we get rid of the log-odds scale? By exponentiating coefficients!</p>
<details>
<summary>
<i>Don’t remember logarithms? Don’t worry! Click here for a quick refresher</i>
</summary>
<p>A logarithm is the inverse operation of exponentiation, meaning it undoes the effect of raising a number to a power. If we have an equation like <span class="math inline">\(a^b = c\)</span>, the logarithm allows us to solve for <span class="math inline">\(b\)</span> by rewriting it as <span class="math inline">\(\log_a(c) = b\)</span>. In other words, the logarithm tells us what exponent we need to raise the base <span class="math inline">\(a\)</span> to in order to get <span class="math inline">\(c\)</span>. For example, since <span class="math inline">\(2^3 = 8\)</span>, it follows that <span class="math inline">\(\log_2(8) = 3\)</span>.</p>
<p>This inverse relationship is similar to how subtraction undoes addition or how division undoes multiplication. Logarithms are particularly useful in mathematics and science for handling exponential growth or decay, compressing large numbers into manageable scales, and solving equations where the exponent is the unknown. The most common logarithms are base 10 (common logarithm, <span class="math inline">\(\log\)</span>), base <span class="math inline">\(e\)</span> (natural logarithm, <span class="math inline">\(\ln\)</span>), and base 2 (binary logarithm, used in computing).</p>
<p>In logistic regression, we use the natural logarithm—base <span class="math inline">\(e\)</span>. Therefore, to get rid of the log-odds scale, we simply exponentiate coefficients: <span class="math inline">\(e^{\beta_1}\)</span>, which we can obtain using the <code>R</code> function <code>exp()</code>.</p>
<br>
</details>
<p>If we simply exponentiate the regression coefficients (e.g., <span class="math inline">\(e^{\beta_1}\)</span>, which we can obtain using the <code>R</code> function <code>exp()</code>), we get rid of the log-odds scale and obtain what we call <strong>odds ratios</strong>. Using odds ratios when interpreting logistic regression coefficients is very common. The key word to remember here is <strong>multiplies</strong>.</p>
<style>
div {
  margin-bottom: 1em; /* Adds space below each details block */
}
</style>
<div style="border: 1px solid #ccc; padding: 10px; background-color: #f9f9f9;">
<p><b>Interpreting logistic regression coefficients as odds ratios </b></p>
<p><b>A one-unit increase in <span class="math inline">\(X_1\)</span> multiplies the odds of the event <span class="math inline">\(Y\)</span> occurring by <span class="math inline">\(exp^{\beta_1}\)</span></b></p>
</div>
<p>Odds ratios, by definition, will always be greater than 0. Multiplying something by a number between 0 and 1 actually reduces the number: for example, if we multiply 10 by 0.4, the result is 4, a decrease of 60%. If we multiply something by 1, it stays the same: for example, if we multiply 10 by 1, the result is 1. If we multiply something by a number greater than 1, it implies an increase: for example, if we multiply 10 by 1.5, the result is 5, an increase of 50%.</p>
<ul>
<li>An estimated odds ratio (<span class="math inline">\(exp^{\beta}\)</span>) between 0 and 1 implies a negative association</li>
<li>An estimated odds ratio (<span class="math inline">\(exp^{\beta}\)</span>) equal to 1 implies no association</li>
<li>An estimated odds ratio (<span class="math inline">\(exp^{\beta}\)</span>) greater than 1 implies a positive association</li>
</ul>
<p>Let’s practice all that, seeking to understand what factors are associated with greater or lower odds of marijuana-possession arrestees being released with a summons!</p>
</div>
<div id="fitting-logistic-regression" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Fitting logistic regression<a href="categorical-variables-and-logistic-regression.html#fitting-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is fairly straightforward to run a logistic model. Assuming that the dependent variable is binary, we can use the <code>glm()</code> function in <code>R</code>. It works exactly the same way as the <code>lm()</code> function—we only need to specify that we will be using a <em>logit</em> function link (we can do that by including the argument <code>family = binomial</code>).</p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="categorical-variables-and-logistic-regression.html#cb504-1" tabindex="-1"></a><span class="fu">glm</span>(dependent_var <span class="sc">~</span> independent_var1 <span class="sc">+</span> independent_var2 <span class="sc">+</span> independent_var3, <span class="at">data =</span> dataset, <span class="at">family =</span> binomial)</span></code></pre></div>
<p>Now, let’s fit a multiple logistic regression. The dependent variable, as before, indicates whether arrestees were released with a summons or not. To ensure that we are modeling the odds of being released with a summons (and not the odds of receiving harsher treatment), we recoded the released variable so that “Yes” (released) is coded as 1 and “No” (not released) is coded as 0.</p>
<p>In logistic regression, the event coded as 1 is the comparison group (the outcome for which we are modeling the odds), and the group coded as 0 serves as the reference category.</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="categorical-variables-and-logistic-regression.html#cb505-1" tabindex="-1"></a><span class="co"># load the dplyr package</span></span>
<span id="cb505-2"><a href="categorical-variables-and-logistic-regression.html#cb505-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb505-3"><a href="categorical-variables-and-logistic-regression.html#cb505-3" tabindex="-1"></a></span>
<span id="cb505-4"><a href="categorical-variables-and-logistic-regression.html#cb505-4" tabindex="-1"></a><span class="co"># recode the &#39;released&#39; variable to ensure that the</span></span>
<span id="cb505-5"><a href="categorical-variables-and-logistic-regression.html#cb505-5" tabindex="-1"></a><span class="co"># comparison group is coded as 1</span></span>
<span id="cb505-6"><a href="categorical-variables-and-logistic-regression.html#cb505-6" tabindex="-1"></a>Arrests <span class="ot">&lt;-</span> <span class="fu">mutate</span>(Arrests, </span>
<span id="cb505-7"><a href="categorical-variables-and-logistic-regression.html#cb505-7" tabindex="-1"></a>                  <span class="at">released =</span> <span class="fu">case_when</span>(</span>
<span id="cb505-8"><a href="categorical-variables-and-logistic-regression.html#cb505-8" tabindex="-1"></a>                    released <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span> <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb505-9"><a href="categorical-variables-and-logistic-regression.html#cb505-9" tabindex="-1"></a>                    released <span class="sc">==</span> <span class="st">&quot;No&quot;</span> <span class="sc">~</span> <span class="dv">0</span></span>
<span id="cb505-10"><a href="categorical-variables-and-logistic-regression.html#cb505-10" tabindex="-1"></a>                  ))</span></code></pre></div>
<p>Now we can fit the multiple logistic regression. As independent variables, let’s include arrestees’ racial identity (<code>colour</code>), gender (<code>sex</code>), the number of entries in the criminal justice system (<code>checks</code>), and whether they were employed at the time of arrest (<code>employed</code>).</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="categorical-variables-and-logistic-regression.html#cb506-1" tabindex="-1"></a><span class="co"># Fit a logistic regression model and store it under &#39;logistic_reg&#39;</span></span>
<span id="cb506-2"><a href="categorical-variables-and-logistic-regression.html#cb506-2" tabindex="-1"></a>logistic_reg <span class="ot">&lt;-</span> <span class="fu">glm</span>(released <span class="sc">~</span> colour <span class="sc">+</span> sex <span class="sc">+</span> checks <span class="sc">+</span> employed, <span class="at">data =</span> Arrests, <span class="at">family =</span> binomial)</span>
<span id="cb506-3"><a href="categorical-variables-and-logistic-regression.html#cb506-3" tabindex="-1"></a></span>
<span id="cb506-4"><a href="categorical-variables-and-logistic-regression.html#cb506-4" tabindex="-1"></a><span class="co"># print the estimated coefficients</span></span>
<span id="cb506-5"><a href="categorical-variables-and-logistic-regression.html#cb506-5" tabindex="-1"></a>logistic_reg</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = released ~ colour + sex + checks + employed, family = binomial, 
##     data = Arrests)
## 
## Coefficients:
## (Intercept)  colourWhite      sexMale       checks  employedYes  
##     1.40739      0.49608     -0.04215     -0.35796      0.77973  
## 
## Degrees of Freedom: 5225 Total (i.e. Null);  5221 Residual
## Null Deviance:       4776 
## Residual Deviance: 4331  AIC: 4341</code></pre>
<p>The table, as you will see, is similar to the one you get when running linear regression. For now, we can ignore the new information at the very bottom of the output (these are fit indices, including the null and deviance residuals and the Akaike Information Criteria—AIC). Let’s focus on the estimated coefficients. We could rewrite the logit equation with the estimated parameters now:</p>
<p><span class="math display">\[
log \bigg( \frac{release\_summons}{harsher\_treatment} \bigg) = 1.407 + 0.496\cdot colour - 0.042\cdot sex - 0.358\cdot checks + 0.780\cdot employed
\]</span></p>
<p>Based on this, we can conclude that White arrestees’ <em>log-odds</em> of being released with a summons are <span class="math inline">\(\beta_1=0.496\)</span> larger than Black arrestees’ <em>log-odds</em>, controlling for sex, previous checks in the criminal justice system, and employment status; that male’s <em>log-odds</em> of being released are <span class="math inline">\(\beta_2=-0.042\)</span> lower than females’, controlling for race, previous checks in the criminal justice system, and employment status; that every additional check in the criminal justice system is associated with a decrease of <span class="math inline">\(\beta_3=-0.358\)</span> in the <em>log-odds</em> of being released, controlling for race, sex, and employment status; and that employed arrestees’ <em>log-odds</em> are <span class="math inline">\(\beta_4=0.780\)</span> larger than the <em>log-odds</em> among unemployed arrestees, controlling for race, sex, and checks in the criminal justice system.</p>
<p>So what does that actually mean? As mentioned above, interpreting the log odds scale is something some people do not find very intuitive. So, using <strong>odd ratios</strong> when interpreting logistic regression is common. To do this, all we need to do is to exponentiate the coefficients. To get the exponentiated coefficients, you tell <code>R</code> that you want to exponentiate (<code>exp()</code>), that the object you want to exponentiate is called coefficients, and it is part of the model you just ran. We can do this in several steps or in one step.</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="categorical-variables-and-logistic-regression.html#cb508-1" tabindex="-1"></a><span class="co"># store regression coefficients under &#39;coefficients&#39;</span></span>
<span id="cb508-2"><a href="categorical-variables-and-logistic-regression.html#cb508-2" tabindex="-1"></a>coefficients <span class="ot">&lt;-</span> <span class="fu">coef</span>(logistic_reg)</span>
<span id="cb508-3"><a href="categorical-variables-and-logistic-regression.html#cb508-3" tabindex="-1"></a></span>
<span id="cb508-4"><a href="categorical-variables-and-logistic-regression.html#cb508-4" tabindex="-1"></a><span class="co"># exponentiate all coefficients to obtain odds ratios</span></span>
<span id="cb508-5"><a href="categorical-variables-and-logistic-regression.html#cb508-5" tabindex="-1"></a><span class="fu">exp</span>(coefficients)</span></code></pre></div>
<pre><code>## (Intercept) colourWhite     sexMale      checks employedYes 
##   4.0852619   1.6422658   0.9587242   0.6990998   2.1808765</code></pre>
<p>Much better! Now that we have exponentiated the coefficients, we have <strong>odds ratios</strong>—, which are much easier to interpret than coefficients on the <em>log-odds</em> scale.</p>
<ul>
<li>White arrestees’ odds of being released with a summons are 64% higher than Black arrestees’ odds of being released with a summons, controlling for sex, checks in the criminal justice system, and employment status.
<ul>
<li>i.e., being White multiplies the odds of release with a summons by 1.64.</li>
</ul></li>
<li>Male arrestees’ odds of being released with a summons are 4.13% lower than female arrestees’ odds of being released with a summons, controlling for race, checks in the criminal justice system, and employment status.
<ul>
<li>i.e., being male multiplies the odds of release with a summons by 0.96.</li>
</ul></li>
<li>Every additional check in the criminal justice system is associated with a decrease of 30.1% in the odds of being released with a summons, controlling for race, sex, and employment status
<ul>
<li>i.e., every additional check in the criminal justice system multiplies the odds of being released with a summons by 0.70.</li>
</ul></li>
<li>Employed arrestees’ odds of being released with a summons are more than twice as high as unemployed arrestees’ odds of being released with a summons, controlling for race, sex, and employment status.
<ul>
<li>i.e., being employed multiplies the odds of release with a summons by 2.18.</li>
</ul></li>
</ul>
<p>For more details on interpreting odd ratios in logistic regression, you may want to read <a href="https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/">this</a>. Some people do not like odd ratios. For other ways of interpreting logistic regression coefficients, you may want to consult <a href="http://www.cambridge.org/gb/academic/subjects/statistics-probability/statistical-theory-and-methods/data-analysis-using-regression-and-multilevelhierarchical-models?format=PB">chapter 5 of the book</a> by Gelman and Hill (2007).</p>
<p>You can read more about how to interpret odd ratios in logistic regression <a href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/">here</a>.</p>
<p>See <a href="https://uom-resquant.github.io/modelling_book/appendix.html#fitting-logistic-regression-alternative">the appendix</a> for an alternative way of getting the same results with less typing.</p>
<p>As with linear regression, the interpretation of regression coefficients is sensitive to the scale of measurement of the predictors. This means one cannot compare the magnitude of the coefficients to compare the relevance of variables to predict the response variable. The same applies to the odd ratios. Tempting and common as this might be, unless the independent variables use the same metric (or maybe if they are all categorical), there is little point in comparing the magnitude of the odd ratios in logistic regression. Like the unstandardised logistic regression coefficients, odd ratios are <strong>not</strong> a measure of effect size that allows comparisons across inputs (Menard, 2012).</p>
<p>We can also produce effect plots using the <code>effects</code> package:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="categorical-variables-and-logistic-regression.html#cb510-1" tabindex="-1"></a><span class="co"># load the &#39;effects&#39; package</span></span>
<span id="cb510-2"><a href="categorical-variables-and-logistic-regression.html#cb510-2" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb510-3"><a href="categorical-variables-and-logistic-regression.html#cb510-3" tabindex="-1"></a></span>
<span id="cb510-4"><a href="categorical-variables-and-logistic-regression.html#cb510-4" tabindex="-1"></a><span class="co"># produce effect plots</span></span>
<span id="cb510-5"><a href="categorical-variables-and-logistic-regression.html#cb510-5" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(logistic_reg), <span class="at">ask =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="08_logistic_regression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Effect plots in this context are particularly helpful because they summarise the results using probabilities, which is what you see plotted on the y-axis.</p>
<p>We don’t have to print them all. When we are primarily concerned with one of them, as in this case, that’s the one we want to emphasise when presenting and discussing our results. There isn’t much point in discussing the results for the variables we simply defined as control (given what our research goal was). So, in this case, we would ask for the plot for our input measuring race/ethnicity:</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="categorical-variables-and-logistic-regression.html#cb511-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">effect</span>(<span class="st">&quot;colour&quot;</span>, logistic_reg), <span class="at">multiline =</span> <span class="cn">FALSE</span>, <span class="at">ylab =</span> <span class="st">&quot;Probability(harsher)&quot;</span>)</span></code></pre></div>
<p><img src="08_logistic_regression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>We can use the predict() function to generate the predicted probability that the arrestees will be released given what we know about their inputs in the model, given values of the predictors. By default, R will compute the probabilities for the dataset we fitted the model with. Here, we have printed only the first ten probabilities, but the way we use the predict() function here will generate a predicted probability for each case in the dataset.</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="categorical-variables-and-logistic-regression.html#cb512-1" tabindex="-1"></a>logistic_reg_prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(logistic_reg, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="co">#If you want to add this to your dataframe, you could designate your object as Arrests$fitl_1_prob</span></span>
<span id="cb512-2"><a href="categorical-variables-and-logistic-regression.html#cb512-2" tabindex="-1"></a>logistic_reg_prob[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code>##         1         2         3         4         5         6         7         8 
## 0.8273773 0.7448014 0.8273773 0.8565590 0.8616606 0.8990862 0.8654497 0.9109450 
##         9        10 
## 0.6710889 0.8273773</code></pre>
<p>It is important to understand that with this type of model, we usually generate two types of predictions. On the one hand, we are producing a continuous valued prediction in the form of a probability, but we can also generate a predicted class for each case. In many applied settings, the latter will be relevant. A discrete category prediction may be required in order to make a decision. Imagine a probation officer evaluating the future risk of a client. She/He would want to know whether the case is high risk or not.</p>
</div>
<div id="interactions" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Interactions<a href="categorical-variables-and-logistic-regression.html#interactions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The data we have been using were obtained by the author of the <code>effects</code> package from <a href="http://www.datavis.ca/">Michael Friendly</a>, another prominent contributor to the development of R packages. The data are related to a series of stories revealed by the Toronto Star and further analysed by Professor Friendly as seen <a href="http://www.datavis.ca/courses/VCD/vcd4-handout-2x2.pdf">here</a>. In this further analysis, Friendly proposes a slightly more complex model than the one we have specified so far. This model adds three new predictors (citizenship, age, and year in which the case was processed) and also allows for interactions between race (colour) and year, as well as race and age.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="categorical-variables-and-logistic-regression.html#cb514-1" tabindex="-1"></a><span class="co"># fit new multiple logistic regression</span></span>
<span id="cb514-2"><a href="categorical-variables-and-logistic-regression.html#cb514-2" tabindex="-1"></a>logistic_reg_2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(released <span class="sc">~</span> employed <span class="sc">+</span> citizen <span class="sc">+</span> checks <span class="sc">+</span> colour <span class="sc">*</span> year <span class="sc">+</span> colour <span class="sc">*</span> age, </span>
<span id="cb514-3"><a href="categorical-variables-and-logistic-regression.html#cb514-3" tabindex="-1"></a>              <span class="at">family =</span> binomial, <span class="at">data =</span> Arrests) </span>
<span id="cb514-4"><a href="categorical-variables-and-logistic-regression.html#cb514-4" tabindex="-1"></a></span>
<span id="cb514-5"><a href="categorical-variables-and-logistic-regression.html#cb514-5" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb514-6"><a href="categorical-variables-and-logistic-regression.html#cb514-6" tabindex="-1"></a>logistic_reg_2</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = released ~ employed + citizen + checks + colour * 
##     year + colour * age, family = binomial, data = Arrests)
## 
## Coefficients:
##      (Intercept)       employedYes        citizenYes            checks  
##       -227.48053           0.74748           0.62016          -0.36472  
##      colourWhite              year               age  colourWhite:year  
##        361.66832           0.11391           0.02879          -0.18022  
##  colourWhite:age  
##         -0.03813  
## 
## Degrees of Freedom: 5225 Total (i.e. Null);  5217 Residual
## Null Deviance:       4776 
## Residual Deviance: 4275  AIC: 4293</code></pre>
<p>What we see here is that the two interactions included are somewhat different from 0. To assist in the interpretation of interactions, it is helpful to look at effect plots.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="categorical-variables-and-logistic-regression.html#cb516-1" tabindex="-1"></a><span class="co"># produce effects plot to assess interaction effects</span></span>
<span id="cb516-2"><a href="categorical-variables-and-logistic-regression.html#cb516-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">effect</span>(<span class="st">&quot;colour:year&quot;</span>, logistic_reg_2))</span></code></pre></div>
<p><img src="08_logistic_regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>First, we see that up to 2000, there is strong evidence for differential treatment of blacks and whites. However, we also see evidence to support Police claims of the effect of training to reduce racial effects.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="categorical-variables-and-logistic-regression.html#cb517-1" tabindex="-1"></a><span class="co"># produce effects plot to assess interaction effects</span></span>
<span id="cb517-2"><a href="categorical-variables-and-logistic-regression.html#cb517-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">effect</span>(<span class="st">&quot;colour:age&quot;</span>, logistic_reg_2))</span></code></pre></div>
<p><img src="08_logistic_regression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>On the other hand, we see a significant interaction between race and age. Young blacks are treated more harshly than young whites. However, older blacks were treated less harshly than older whites.</p>
<p>In a previous session, we discussed the difficulties of interpreting regression coefficients in models with interactions. Centring and standardising in the way discussed earlier can actually be of help for this purpose.</p>
</div>
<div id="assessing-model-fit-confusion-matrix" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Assessing model fit: confusion matrix<a href="categorical-variables-and-logistic-regression.html#assessing-model-fit-confusion-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we are interested in “qualitative” prediction, we also need to consider other measures of fit. In many applied settings, such as in applied predictive modelling, this can be the case. Imagine you are developing a tool to be used to forecast the probability of repeat victimisation in cases of domestic violence. This type of prediction may then be used to determine the type of police response to cases defined as high-risk. Clearly, you want to make sure the classification you make is as accurate as possible.</p>
<p>In these contexts, it is common to start from a <strong>classification table</strong> or <a href="http://en.wikipedia.org/wiki/Confusion_matrix"><strong>confusion matrix</strong></a>. A confusion matrix is simply a cross-tabulation of the observed outcome in relation to the predicted outcome. We saw earlier how the <code>predict()</code> function generated a set of predicted probabilities for each of the subjects in the study. To produce a classification table in this context, we must define a <strong>cut-off point</strong>, a particular probability that we will use to classify cases. We will define anybody above that cut-off as belonging to the level of interest and anybody below we will define as not. We could, for example, say that anybody with a probability larger than .5 should be predicted to receive harsher treatment.</p>
<p>The confusion matrix typically follows this layout:</p>
<div class="float">
<img src="http://3.bp.blogspot.com/_txFWHHNYMJQ/THyADzbutYI/AAAAAAAAAf8/TAXL7lySrko/s1600/Picture+8.png" alt="confusion" />
<div class="figcaption">confusion</div>
</div>
<p>The diagonal entries correspond to observations that are classified correctly according to our model and our cut-off point, whereas the off-diagonal entries are misclassifications. <strong>False negatives</strong> are observations that were classified as zeros but turned out to be ones (the outcome of interest). <strong>False positives</strong> are observations that were classified as ones (the outcome of interest) but turned out to be zeros.</p>
<p>There are various ways of producing a confusion matrix in R. The most basic one is to ask for the cross-tabulation of the predicted classes (determined by the cut-off criterion) versus the observed classes.</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="categorical-variables-and-logistic-regression.html#cb518-1" tabindex="-1"></a><span class="co">#to create a confusion matrix, we want </span></span>
<span id="cb518-2"><a href="categorical-variables-and-logistic-regression.html#cb518-2" tabindex="-1"></a><span class="co">#our variable to be back to a factor</span></span>
<span id="cb518-3"><a href="categorical-variables-and-logistic-regression.html#cb518-3" tabindex="-1"></a>Arrests <span class="ot">&lt;-</span> <span class="fu">mutate</span>(Arrests, </span>
<span id="cb518-4"><a href="categorical-variables-and-logistic-regression.html#cb518-4" tabindex="-1"></a>                  <span class="at">released =</span> <span class="fu">case_when</span>(</span>
<span id="cb518-5"><a href="categorical-variables-and-logistic-regression.html#cb518-5" tabindex="-1"></a>                    released <span class="sc">==</span> <span class="dv">1</span> <span class="sc">~</span> <span class="st">&quot;Yes&quot;</span>,</span>
<span id="cb518-6"><a href="categorical-variables-and-logistic-regression.html#cb518-6" tabindex="-1"></a>                    released <span class="sc">==</span> <span class="dv">0</span> <span class="sc">~</span> <span class="st">&quot;No&quot;</span></span>
<span id="cb518-7"><a href="categorical-variables-and-logistic-regression.html#cb518-7" tabindex="-1"></a>                  ),</span>
<span id="cb518-8"><a href="categorical-variables-and-logistic-regression.html#cb518-8" tabindex="-1"></a>                  <span class="at">released =</span> <span class="fu">as.factor</span>(released))</span>
<span id="cb518-9"><a href="categorical-variables-and-logistic-regression.html#cb518-9" tabindex="-1"></a><span class="co">#First, we define the classes according to the cut-off</span></span>
<span id="cb518-10"><a href="categorical-variables-and-logistic-regression.html#cb518-10" tabindex="-1"></a>logistic_reg_pred_class <span class="ot">&lt;-</span> logistic_reg_prob <span class="sc">&gt;</span> .<span class="dv">5</span></span>
<span id="cb518-11"><a href="categorical-variables-and-logistic-regression.html#cb518-11" tabindex="-1"></a><span class="co">#This creates a logical vector that returns TRUE </span></span>
<span id="cb518-12"><a href="categorical-variables-and-logistic-regression.html#cb518-12" tabindex="-1"></a><span class="co">#when the condition is met (the subject is predicted to be released) and </span></span>
<span id="cb518-13"><a href="categorical-variables-and-logistic-regression.html#cb518-13" tabindex="-1"></a><span class="co">#FALSE when the condition is not met (the subject is not released)</span></span>
<span id="cb518-14"><a href="categorical-variables-and-logistic-regression.html#cb518-14" tabindex="-1"></a>logistic_reg_pred_class[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code>##    1    2    3    4    5    6    7    8    9   10 
## TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE</code></pre>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="categorical-variables-and-logistic-regression.html#cb520-1" tabindex="-1"></a><span class="co">#Let&#39;s make this into a factor with the same levels as the original variable</span></span>
<span id="cb520-2"><a href="categorical-variables-and-logistic-regression.html#cb520-2" tabindex="-1"></a>released_pred <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(logistic_reg_pred_class)</span>
<span id="cb520-3"><a href="categorical-variables-and-logistic-regression.html#cb520-3" tabindex="-1"></a><span class="fu">levels</span>(released_pred) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;No&quot;</span>,<span class="st">&quot;Yes&quot;</span>)</span>
<span id="cb520-4"><a href="categorical-variables-and-logistic-regression.html#cb520-4" tabindex="-1"></a><span class="fu">table</span>(released_pred)</span></code></pre></div>
<pre><code>## released_pred
##   No  Yes 
##  113 5113</code></pre>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="categorical-variables-and-logistic-regression.html#cb522-1" tabindex="-1"></a><span class="co">#Then we can produce the cross-tab</span></span>
<span id="cb522-2"><a href="categorical-variables-and-logistic-regression.html#cb522-2" tabindex="-1"></a>tab0 <span class="ot">&lt;-</span> <span class="fu">table</span>(released_pred, Arrests<span class="sc">$</span>released)</span>
<span id="cb522-3"><a href="categorical-variables-and-logistic-regression.html#cb522-3" tabindex="-1"></a>tab0</span></code></pre></div>
<pre><code>##              
## released_pred   No  Yes
##           No    57   56
##           Yes  835 4278</code></pre>
<p>We can derive various useful measures from classification tables. Two important ones are the <strong>sensitivity</strong> and the <strong>specificity</strong>. The model’s sensitivity is the rate at which the event of interest (e.g., being released) is predicted correctly for all cases having the event.</p>
<p>Sensitivity = number of cases with the event and predicted to have the event/number of samples actually presenting the event</p>
<p>In this case, this amounts to 4278 divided by 56 plus 4278. The sensitivity is sometimes also considered the <strong>true positive rate</strong> since it measures the accuracy in the event population. On the other hand, specificity is defined as:</p>
<p>Specificity = number of cases without the events and predicted as non-events/number of cases without the event</p>
<p>In this case, this amounts to 57 divided by 57 plus 835. The <strong>false positive rate</strong> is defined as one minus the specificity.</p>
<p>We can generate these measures automatically from the table we produced. However, for this sort of thing, I prefer to use the <code>confusionMatrix()</code> function from the <code>caret</code> package. It produces a very detailed set of calibration measures that help indicate how well the model is classifying.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="categorical-variables-and-logistic-regression.html#cb524-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb524-2"><a href="categorical-variables-and-logistic-regression.html#cb524-2" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data=</span>released_pred, </span>
<span id="cb524-3"><a href="categorical-variables-and-logistic-regression.html#cb524-3" tabindex="-1"></a>               <span class="at">reference=</span>Arrests<span class="sc">$</span>released, <span class="at">positive=</span><span class="st">&quot;Yes&quot;</span>) </span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No    57   56
##        Yes  835 4278
##                                          
##                Accuracy : 0.8295         
##                  95% CI : (0.819, 0.8396)
##     No Information Rate : 0.8293         
##     P-Value [Acc &gt; NIR] : 0.4943         
##                                          
##                   Kappa : 0.078          
##                                          
##  Mcnemar&#39;s Test P-Value : &lt;2e-16         
##                                          
##             Sensitivity : 0.9871         
##             Specificity : 0.0639         
##          Pos Pred Value : 0.8367         
##          Neg Pred Value : 0.5044         
##              Prevalence : 0.8293         
##          Detection Rate : 0.8186         
##    Detection Prevalence : 0.9784         
##       Balanced Accuracy : 0.5255         
##                                          
##        &#39;Positive&#39; Class : Yes            
## </code></pre>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="categorical-variables-and-logistic-regression.html#cb526-1" tabindex="-1"></a><span class="co">#The data argument specifies the vector </span></span>
<span id="cb526-2"><a href="categorical-variables-and-logistic-regression.html#cb526-2" tabindex="-1"></a><span class="co">#with the predictions and the reference </span></span>
<span id="cb526-3"><a href="categorical-variables-and-logistic-regression.html#cb526-3" tabindex="-1"></a><span class="co">#argument the vector with the observed </span></span>
<span id="cb526-4"><a href="categorical-variables-and-logistic-regression.html#cb526-4" tabindex="-1"></a><span class="co">#outcome or event. The positive argument </span></span>
<span id="cb526-5"><a href="categorical-variables-and-logistic-regression.html#cb526-5" tabindex="-1"></a><span class="co">#identifies the level of interest in the factor.</span></span></code></pre></div>
<p>We can see first the <strong>accuracy</strong>. The overall accuracy rate gives us the agreement between the observed and predicted classes. However, the overall accuracy is often not the most useful measure. <strong>Kappa</strong> is also a measure that is often used with values ranging between 0.30 and 0.50 considered to indicate reasonable agreement. However, for many applications, it will be of interest to focus on the sensitivity and the specificity as defined above. In this case, we can see that our sensitivity, or the true positive rate, is good. However, our Kappa is not. The model can predict those who are released with summons but is not doing so well predicting harsh treatment with the selected cut-off point.</p>
<p>One of the problems with taking this approach is that the choice of the cut-off point can be arbitrary, and yet this cut-off point will impact the sensitivity and specificity of the model. There is a trade-off between sensitivity and specificity. Given a fixed accuracy, more of one will result in less of the other.</p>
<p>So if we use a different cut-off point, say .75, the classification table would look like this:</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="categorical-variables-and-logistic-regression.html#cb527-1" tabindex="-1"></a>precision<span class="ot">&lt;-</span><span class="cf">function</span>(c) {</span>
<span id="cb527-2"><a href="categorical-variables-and-logistic-regression.html#cb527-2" tabindex="-1"></a>tab1 <span class="ot">&lt;-</span> <span class="fu">table</span>(logistic_reg_prob<span class="sc">&gt;</span>c, Arrests<span class="sc">$</span>released)</span>
<span id="cb527-3"><a href="categorical-variables-and-logistic-regression.html#cb527-3" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">diag</span>(tab1)<span class="sc">/</span><span class="fu">apply</span>(tab1, <span class="dv">2</span>, sum)</span>
<span id="cb527-4"><a href="categorical-variables-and-logistic-regression.html#cb527-4" tabindex="-1"></a><span class="fu">names</span>(out) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&#39;specificity&#39;</span>, <span class="st">&#39;sensitivity&#39;</span>)</span>
<span id="cb527-5"><a href="categorical-variables-and-logistic-regression.html#cb527-5" tabindex="-1"></a><span class="fu">list</span>(tab1, out)</span>
<span id="cb527-6"><a href="categorical-variables-and-logistic-regression.html#cb527-6" tabindex="-1"></a>}</span>
<span id="cb527-7"><a href="categorical-variables-and-logistic-regression.html#cb527-7" tabindex="-1"></a><span class="fu">precision</span>(.<span class="dv">75</span>)</span></code></pre></div>
<pre><code>## [[1]]
##        
##           No  Yes
##   FALSE  396  707
##   TRUE   496 3627
## 
## [[2]]
## specificity sensitivity 
##   0.4439462   0.8368713</code></pre>
<p>Here, we are predicting, according to our model, that anybody with a probability above .75 will be released with summons. Our sensitivity decreases, but our specificity goes up. You can see that the cut-off point will affect how many false positives and false negatives we have. The overall accuracy is still the same, but we have shifted the balance between sensitivity and specificity.</p>
<p>Potential trade-offs here may be appropriate when there are different penalties or costs associated with each type of error. For example, if you are trying to predict a homicide as part of an intervention or prevention program, you may give more importance to not making a false negative error. That is, you want to identify as many potential homicide victims as possible, even if that means that you will identify as victims individuals that, in the end, won’t be (false positives). On the other hand, if you have limited resources to attend to all the cases that you will predict as positives, you also need to factor this into the equation. You don’t want to use a cut-off point that will lead you to identify more cases as potential homicide victims than you can possibly work with.</p>
<p>Similarly, the criminal justice system is essentially built around the idea of avoiding false positives - that is, convicting people who are innocent. You will have heard many phrases like “innocent until proven guilty” or “It is far better that 10 guilty men go free than one innocent man is wrongfully convicted”. This approach would incline us to err on the side of false negatives and avoid false positives (higher sensitivity, lower specificity).</p>
<p>We may want to see what happens to sensitivity and specificity for different cut-off points. For this, we can look at <strong>receiver operating characteristics</strong> or simply <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curves</a>. This is essentially a tool for evaluating the sensitivity/specificity trade-off. The ROC curve can be used to investigate alternate cut-offs for class probabilities. <a href="https://uom-resquant.github.io/modelling_book/appendix.html#assessing-model-fit-roc-curves">See the appendix for more info on the ROC curve</a>. There are also other ways to assess a model’s fit, such as deviance and pseudo r squared. For those interested, <a href="https://uom-resquant.github.io/modelling_book/appendix.html#assessing-model-fit-deviance-and-pseudo-r-squared">you can read more in the appendix</a>.–&gt;</p>
</div>
<div id="lab-exercises-2" class="section level2 hasAnchor" number="8.7">
<h2><span class="header-section-number">8.7</span> Lab Exercises<a href="categorical-variables-and-logistic-regression.html#lab-exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Your turn!</strong> In the lab session, answer the following questions. Note that not all questions necessarily require analysing data in R. After you finish, click on ‘Reveal answer!’ to check your answers.</p>
<p>We start by revisiting the ‘Ban the Box: fair chance recruitment’ practices in the UK, a dataset from Week 2. We first load, select the needed variables, and carry out any data transformation required.</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="categorical-variables-and-logistic-regression.html#cb529-1" tabindex="-1"></a><span class="fu">library</span>(haven)</span>
<span id="cb529-2"><a href="categorical-variables-and-logistic-regression.html#cb529-2" tabindex="-1"></a>banbox <span class="ot">&lt;-</span> <span class="fu">read_dta</span>(<span class="st">&quot;https://dataverse.harvard.edu/api/access/datafile/3036350&quot;</span>)</span>
<span id="cb529-3"><a href="categorical-variables-and-logistic-regression.html#cb529-3" tabindex="-1"></a><span class="co">#if you have issues with loading the data</span></span>
<span id="cb529-4"><a href="categorical-variables-and-logistic-regression.html#cb529-4" tabindex="-1"></a><span class="co">#see section 2.2 on how to get the data</span></span>
<span id="cb529-5"><a href="categorical-variables-and-logistic-regression.html#cb529-5" tabindex="-1"></a></span>
<span id="cb529-6"><a href="categorical-variables-and-logistic-regression.html#cb529-6" tabindex="-1"></a><span class="co">#select needed variables</span></span>
<span id="cb529-7"><a href="categorical-variables-and-logistic-regression.html#cb529-7" tabindex="-1"></a>banbox <span class="ot">&lt;-</span> banbox <span class="sc">%&gt;%</span> <span class="fu">select</span> (response, black, crime, empgap)</span>
<span id="cb529-8"><a href="categorical-variables-and-logistic-regression.html#cb529-8" tabindex="-1"></a></span>
<span id="cb529-9"><a href="categorical-variables-and-logistic-regression.html#cb529-9" tabindex="-1"></a><span class="co">#data transformation</span></span>
<span id="cb529-10"><a href="categorical-variables-and-logistic-regression.html#cb529-10" tabindex="-1"></a>banbox<span class="sc">$</span>black_f <span class="ot">&lt;-</span> <span class="fu">factor</span>(banbox<span class="sc">$</span>black, <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;non-Black&quot;</span>, <span class="st">&quot;Black&quot;</span>))</span></code></pre></div>
<p>Using the above data, we want to assess whether receiving a positive answer to a job application is associated with having a criminal record. Your variables of interest are <code>response</code> - Application received Positive Response; <code>empgap</code> - Applicant has Employment Gap; <code>crime</code> - Applicant has Criminal Record; and <code>black</code> - whether someone is black or not.</p>
<ol style="list-style-type: decimal">
<li>Based on your research hypothesis, what are your dependent and independent variables?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>Dependent variable: Application received Positive Response, i.e., <code>response</code>.</p>
<p>Independent variable: Applicant has Criminal Record, i.e., <code>crime</code>.</p>
</details>
<ol start="2" style="list-style-type: decimal">
<li>Is your dependent variable numerical or categorical? If categorical, is it binary, ordinal, or multinomial? What about the independent variable?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>The dependent variable, Application received Positive Response (<code>responce</code>), is a binary variable.</p>
<p>The independent variable, Applicant has Criminal Record (<code>crime</code>), is also a binary variable.</p>
</details>
<ol start="3" style="list-style-type: decimal">
<li>Using your dependent and independent variables, what is your null hypothesis?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
Null hypothesis: There is no association between having a criminal record and receiving a callback (response). In other words, the odds of receiving a positive response are the same for applicants with criminal records and those without criminal records.
</details>
<ol start="4" style="list-style-type: decimal">
<li>Examine the relationship between your dependent and your independent variable using the appropriate visualisation strategies</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>We can produce a bar chart to assess the relationship</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="categorical-variables-and-logistic-regression.html#cb530-1" tabindex="-1"></a><span class="co"># produce barchart</span></span>
<span id="cb530-2"><a href="categorical-variables-and-logistic-regression.html#cb530-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb530-3"><a href="categorical-variables-and-logistic-regression.html#cb530-3" tabindex="-1"></a></span>
<span id="cb530-4"><a href="categorical-variables-and-logistic-regression.html#cb530-4" tabindex="-1"></a><span class="fu">ggplot</span>(banbox, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">as_factor</span>(crime), <span class="at">fill =</span> <span class="fu">factor</span>(response, <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>)))) <span class="sc">+</span></span>
<span id="cb530-5"><a href="categorical-variables-and-logistic-regression.html#cb530-5" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">position =</span> <span class="st">&quot;fill&quot;</span>) <span class="sc">+</span></span>
<span id="cb530-6"><a href="categorical-variables-and-logistic-regression.html#cb530-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Proportion&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;Response&quot;</span>, <span class="at">x =</span> <span class="st">&quot;criminal record&quot;</span>) <span class="sc">+</span></span>
<span id="cb530-7"><a href="categorical-variables-and-logistic-regression.html#cb530-7" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="08_logistic_regression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</details>
<ol start="5" style="list-style-type: decimal">
<li>Let’s build a logistic regression model. Replacing <span class="math inline">\(Y\)</span> with the name of your dependent variable and <span class="math inline">\(X\)</span> with the name of your independent variable, write down the equation with the unknown parameters (i.e, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) that you want to estimate. <em>Note: This question does not involve any data analysis</em>.</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p><span class="math display">\[
\log \left( \frac{P(\text{response} = 1)}{1 - P(\text{response} = 1)} \right) = \alpha + \beta \times \text{crime}
\]</span></p>
</details>
<ol start="6" style="list-style-type: decimal">
<li>Using the <code>glm()</code> function, estimate the parameters of your logistic regression model. Rewrite the equation above by replacing unknown parameters with the estimated parameters.</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="categorical-variables-and-logistic-regression.html#cb531-1" tabindex="-1"></a><span class="co"># fit logistic regression model</span></span>
<span id="cb531-2"><a href="categorical-variables-and-logistic-regression.html#cb531-2" tabindex="-1"></a>log_reg <span class="ot">&lt;-</span> <span class="fu">glm</span>(response <span class="sc">~</span> crime, <span class="at">data =</span> banbox, <span class="at">family =</span> binomial)</span>
<span id="cb531-3"><a href="categorical-variables-and-logistic-regression.html#cb531-3" tabindex="-1"></a>log_reg</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = response ~ crime, family = binomial, data = banbox)
## 
## Coefficients:
## (Intercept)        crime  
##      -1.939       -0.149  
## 
## Degrees of Freedom: 14812 Total (i.e. Null);  14811 Residual
## Null Deviance:       10750 
## Residual Deviance: 10740     AIC: 10740</code></pre>
<p>Based on the results of the linear regression model, we can rewrite the equation in the following way:</p>
<p><span class="math display">\[
\log \left( \frac{P(\text{response} = 1)}{1 - P(\text{response} = 1)} \right) = -1.939 +  -0.149 \times \text{crime}
\]</span></p>
<p>Applicants with a criminal record are less likely to receive a callback than applicants without a record.</p>
</details>
<ol start="7" style="list-style-type: decimal">
<li>After fitting your logistic regression model to predict whether applicants receive a positive response (response), create a confusion matrix comparing predicted responses to actual responses. Use 0.5 as the cut-off point.</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="categorical-variables-and-logistic-regression.html#cb533-1" tabindex="-1"></a><span class="co"># fit regression model</span></span>
<span id="cb533-2"><a href="categorical-variables-and-logistic-regression.html#cb533-2" tabindex="-1"></a>log_reg_prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(log_reg, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb533-3"><a href="categorical-variables-and-logistic-regression.html#cb533-3" tabindex="-1"></a></span>
<span id="cb533-4"><a href="categorical-variables-and-logistic-regression.html#cb533-4" tabindex="-1"></a>log_reg_pred_class <span class="ot">&lt;-</span> log_reg_prob <span class="sc">&gt;</span> .<span class="dv">5</span></span>
<span id="cb533-5"><a href="categorical-variables-and-logistic-regression.html#cb533-5" tabindex="-1"></a>response_pred <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(log_reg_pred_class)</span>
<span id="cb533-6"><a href="categorical-variables-and-logistic-regression.html#cb533-6" tabindex="-1"></a><span class="fu">levels</span>(response_pred) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;No&quot;</span>,<span class="st">&quot;Yes&quot;</span>)</span>
<span id="cb533-7"><a href="categorical-variables-and-logistic-regression.html#cb533-7" tabindex="-1"></a></span>
<span id="cb533-8"><a href="categorical-variables-and-logistic-regression.html#cb533-8" tabindex="-1"></a></span>
<span id="cb533-9"><a href="categorical-variables-and-logistic-regression.html#cb533-9" tabindex="-1"></a>banbox <span class="ot">&lt;-</span> <span class="fu">mutate</span>(banbox, </span>
<span id="cb533-10"><a href="categorical-variables-and-logistic-regression.html#cb533-10" tabindex="-1"></a>                  <span class="at">response =</span> <span class="fu">case_when</span>(</span>
<span id="cb533-11"><a href="categorical-variables-and-logistic-regression.html#cb533-11" tabindex="-1"></a>                    response <span class="sc">==</span> <span class="dv">1</span> <span class="sc">~</span> <span class="st">&quot;Yes&quot;</span>,</span>
<span id="cb533-12"><a href="categorical-variables-and-logistic-regression.html#cb533-12" tabindex="-1"></a>                    response <span class="sc">==</span> <span class="dv">0</span> <span class="sc">~</span> <span class="st">&quot;No&quot;</span></span>
<span id="cb533-13"><a href="categorical-variables-and-logistic-regression.html#cb533-13" tabindex="-1"></a>                  ),</span>
<span id="cb533-14"><a href="categorical-variables-and-logistic-regression.html#cb533-14" tabindex="-1"></a>                  <span class="at">response =</span> <span class="fu">as.factor</span>(response))</span>
<span id="cb533-15"><a href="categorical-variables-and-logistic-regression.html#cb533-15" tabindex="-1"></a></span>
<span id="cb533-16"><a href="categorical-variables-and-logistic-regression.html#cb533-16" tabindex="-1"></a></span>
<span id="cb533-17"><a href="categorical-variables-and-logistic-regression.html#cb533-17" tabindex="-1"></a><span class="co">#Then we can produce the cross-tab</span></span>
<span id="cb533-18"><a href="categorical-variables-and-logistic-regression.html#cb533-18" tabindex="-1"></a><span class="fu">table</span>(response_pred, banbox<span class="sc">$</span>response)</span></code></pre></div>
<pre><code>##              
## response_pred    No   Yes
##           No  13066  1747
##           Yes     0     0</code></pre>
</details>
<ol start="8" style="list-style-type: decimal">
<li>What is the specificity of your logistic regression model predictions?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="categorical-variables-and-logistic-regression.html#cb535-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb535-2"><a href="categorical-variables-and-logistic-regression.html#cb535-2" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data=</span>response_pred, </span>
<span id="cb535-3"><a href="categorical-variables-and-logistic-regression.html#cb535-3" tabindex="-1"></a>               <span class="at">reference=</span>banbox<span class="sc">$</span>response, <span class="at">positive=</span><span class="st">&quot;Yes&quot;</span>) </span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    No   Yes
##        No  13066  1747
##        Yes     0     0
##                                           
##                Accuracy : 0.8821          
##                  95% CI : (0.8768, 0.8872)
##     No Information Rate : 0.8821          
##     P-Value [Acc &gt; NIR] : 0.5064          
##                                           
##                   Kappa : 0               
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 0.0000          
##             Specificity : 1.0000          
##          Pos Pred Value :    NaN          
##          Neg Pred Value : 0.8821          
##              Prevalence : 0.1179          
##          Detection Rate : 0.0000          
##    Detection Prevalence : 0.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p>A specificity of 1 means the model makes zero false positive errors — it never says “Yes” when the correct answer is “No.” Does this mean the model is a good one? Depends on what we are interested in predicting. Could other independent variables be added to the model?</p>
</details>
</div>
<div id="further-resources-2" class="section level2 hasAnchor" number="8.8">
<h2><span class="header-section-number">8.8</span> Further resources<a href="categorical-variables-and-logistic-regression.html#further-resources-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>These are a set of useful external resources that may aid your comprehension (and I have partly relied upon myself, so credit to them!):</p>
<ul>
<li><p>The <a href="https://stats.oarc.ucla.edu/r/dae/logit-regression/">UCLA guide</a> to using logistic regression with R.</p></li>
<li><p>A <a href="http://www.r-bloggers.com/some-r-resources-for-glms/">helpful list of resources</a> for general linear models with R.</p></li>
</ul>
<!--
## Assessing model fit I: deviance and pseudo r squared

As you may remember, when looking at linear models, we could use the R squared to check the overall fit of the model. When running logistic regression, we cannot obtain the R squared (although there is a collection of pseudo-R^2 measures that have been produced). In linear regression, things are a bit simpler. As Menard (2010: 43) explains:

> "there is only one reasonable residual variation criterion for quantitative variables in OLS, the familiar error sum of squares... but there are several possible residual variation criteria (entropy, squared error, qualitative difference) for binary variables. Another hindrance is the existence of numerous mathematical equivalents to R^2 in OLS, which are not necessarily mathematically (same formula) or conceptually (same meaning in the context of the model) equivalent to R^2 in logistic regression... Moreover, in logistic regression, we must choose whether we are more interested in qualitative prediction (whether predicitons are correct or incorrect), quantitative prediction (how close predictions are to being correct), or both, because different measures of explained variation are appropriate for these two different types of prediction"

A common starting point for assessing model fit is to look at the **log-likelihood statistic** and the **deviance** (also referred to as -2LL). 

The log-likelihood aims to provide a measure of how much-unexplained variation there is after you fit the mode. Large values indicate a poor fit. 

The deviance, on the other hand, is simply the log-likelihood multiplied by -2 and is generally abbreviated as -2LL. The deviance will be a positive value, and *larger values indicate worse prediction* of the response variable. It is analogous to the error sum of squares in linear regression. In the same way that OLS linear regression tries to minimise the error sum of squares, maximum likelihood logistic regression tries to minimise the -2LL.

The difference between the -2LL for the model with no predictors and the -2LL for the model with all the predictors is the closer we get in logistic regression to the regression sum of squares. This difference is often called **model chi-squared**, and it provides a test of the null hypothesis that all the regression coefficients equal zero. <!--It is, thus, equivalent to the F test in OLS regression.


```
## 
## Call:  glm(formula = released ~ colour + sex + checks + employed, family = binomial, 
##     data = Arrests)
## 
## Coefficients:
## (Intercept)  colourWhite      sexMale       checks  employedYes  
##     1.40739      0.49608     -0.04215     -0.35796      0.77973  
## 
## Degrees of Freedom: 5225 Total (i.e. Null);  5221 Residual
## Null Deviance:       4776 
## Residual Deviance: 4331  AIC: 4341
```

In our example, we saw that some measures of fit were printed below the table with the coefficients. The **null deviance** is the deviance of the model with no predictors, and the **residual deviance** is simply the deviance of this model. You clearly want the residual deviance to be smaller than the null deviance. The difference between the null and the residual deviance is what we call the model chi-squared. In this case, this is 4776 minus 4275. We can ask R to do this for us.

First, notice that the object we created has all the information we need already stored.


```r
names(logistic_reg)
```

```
##  [1] "coefficients"      "residuals"         "fitted.values"    
##  [4] "effects"           "R"                 "rank"             
##  [7] "qr"                "family"            "linear.predictors"
## [10] "deviance"          "aic"               "null.deviance"    
## [13] "iter"              "weights"           "prior.weights"    
## [16] "df.residual"       "df.null"           "y"                
## [19] "converged"         "boundary"          "model"            
## [22] "call"              "formula"           "terms"            
## [25] "data"              "offset"            "control"          
## [28] "method"            "contrasts"         "xlevels"
```

So we can use this stored information in our calculations.


```r
with(logistic_reg, null.deviance - deviance)
```

```
## [1] 445.5594
```

Is 501 small? How much smaller is enough? This value has a chi-square distribution, and its significance can be easily computed. For this computation, we need to know the degrees of freedom for the model (which equal the number of predictors in the model) and can be obtained like this:


```r
with(logistic_reg, df.null - df.residual)
```

```
## [1] 4
```

<!--Finally, the p-value can be obtained using the following code to invoke the Chi-Square distribution:


```r
#When doing it yourself, this is all you really need 
#(we present the code in a separate fashion above so that you understand better what the one here does)
#with(logistic_reg, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

We can see that the model chi-square is highly significant. Our model as a whole fits significantly better than a model with no predictors.

Menard (2010) recommends also looking at the likelihood ratio R^2, which can be calculated as the difference between the null deviance and the residual deviance divided by the null deviance.


```r
#Likelihood ratio R2
#with(logistic_reg, (null.deviance - deviance)/null.deviance)
```

Some authors refer to this as the Hosmer/Lemeshow R^2. It indicates how much the inclusion of the independent variables in the model reduces variation, as measured by the null deviance. It varies between 0 (when our prediction is catastrophically useless) and 1 (when we predict with total accuracy). There are many other pseudo R^2 measures that have been proposed, but Menard [based on research](http://www.tandfonline.com/doi/pdf/10.1080/00031305.2000.10474502) on the properties of various of these measures recommends the likelihood ratio R^2 because:

+ It is the one with a closer conceptual link to R^2 in OLS regression.

+ It does not appear to be sensitive to the base rate (the proportion of cases that have the attribute of interest) of the phenomenon being studied and, therefore, will work even in cases with unbalanced probabilities.

+ It varies between 0 and 1

+ And it can be used in other generalised linear models (models for categorical outcomes with more than two levels, which we don't cover here)-->
<!--## Summary: exercise for this week
Once you finish your lab session, don't forget to do this [Exercise](https://eonk.shinyapps.io/MCD_ex) and have a chance to sum-up this week's R codes.

Once you finish your lab session, don't forget to do this [Exercise](https://eonk.shinyapps.io/MCD_ex) and have a chance to sum up this week's R codes.-->

</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>If values were between 0 and 1, that wouldn’t be so problematic. Scores between 0 and 1 would simply be treated as estimated probabilities, which is what the linear probability model does. The problem is that the model can also yield scores greater than 1 and lower than 0, which are no longer interpretable as probabilities.<a href="categorical-variables-and-logistic-regression.html#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="statistical-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

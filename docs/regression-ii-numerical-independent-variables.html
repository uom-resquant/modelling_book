<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Regression II: numerical independent variables | Modelling Criminological Data CRIM20452</title>
  <meta name="description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Regression II: numerical independent variables | Modelling Criminological Data CRIM20452" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Regression II: numerical independent variables | Modelling Criminological Data CRIM20452" />
  
  <meta name="twitter:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

<meta name="author" content="" />


<meta name="date" content="2025-03-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-i-mean-differences.html"/>
<link rel="next" href="regression-iii.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelling Criminological Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html"><i class="fa fa-check"></i><b>1</b> A first lesson about R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#install-r-rstudio"><i class="fa fa-check"></i><b>1.1</b> Install R &amp; RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#open-up-and-explore-rstudio"><i class="fa fa-check"></i><b>1.2</b> Open up and explore RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#customising-the-rstudio-look"><i class="fa fa-check"></i><b>1.3</b> Customising the RStudio look</a></li>
<li class="chapter" data-level="1.4" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#getting-organised-r-projects"><i class="fa fa-check"></i><b>1.4</b> Getting organised: R Projects</a></li>
<li class="chapter" data-level="1.5" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#functions-talk-to-your-computer"><i class="fa fa-check"></i><b>1.5</b> Functions: Talk to your computer</a></li>
<li class="chapter" data-level="1.6" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-packages"><i class="fa fa-check"></i><b>1.6</b> More on packages</a></li>
<li class="chapter" data-level="1.7" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#objects-creating-an-object"><i class="fa fa-check"></i><b>1.7</b> Objects: creating an object</a></li>
<li class="chapter" data-level="1.8" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-objects"><i class="fa fa-check"></i><b>1.8</b> More on objects</a></li>
<li class="chapter" data-level="1.9" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#naming-conventions-for-objects-in-r"><i class="fa fa-check"></i><b>1.9</b> Naming conventions for objects in R</a></li>
<li class="chapter" data-level="1.10" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-vectors"><i class="fa fa-check"></i><b>1.10</b> R object types: vectors</a></li>
<li class="chapter" data-level="1.11" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-data-frame"><i class="fa fa-check"></i><b>1.11</b> R object types: Data frame</a></li>
<li class="chapter" data-level="1.12" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#exploring-data"><i class="fa fa-check"></i><b>1.12</b> Exploring data</a></li>
<li class="chapter" data-level="1.13" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-data-types-factors"><i class="fa fa-check"></i><b>1.13</b> R data types: Factors</a></li>
<li class="chapter" data-level="1.14" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-import-data"><i class="fa fa-check"></i><b>1.14</b> How to import data</a></li>
<li class="chapter" data-level="1.15" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-use-comment"><i class="fa fa-check"></i><b>1.15</b> How to use ‘comment’</a></li>
<li class="chapter" data-level="1.16" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-quit-rstudio"><i class="fa fa-check"></i><b>1.16</b> How to Quit RStudio</a></li>
<li class="chapter" data-level="1.17" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#summary"><i class="fa fa-check"></i><b>1.17</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html"><i class="fa fa-check"></i><b>2</b> Getting to know your data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#causality-in-social-sciences"><i class="fa fa-check"></i><b>2.1</b> Causality in Social Sciences</a></li>
<li class="chapter" data-level="2.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-data-thanks-to-reproducibility"><i class="fa fa-check"></i><b>2.2</b> Getting data thanks to reproducibility</a></li>
<li class="chapter" data-level="2.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-a-sense-of-your-data"><i class="fa fa-check"></i><b>2.3</b> Getting a sense of your data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#first-steps"><i class="fa fa-check"></i><b>2.3.1</b> First steps</a></li>
<li class="chapter" data-level="2.3.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#on-tibbles-and-labelled-vectors"><i class="fa fa-check"></i><b>2.3.2</b> On tibbles and labelled vectors</a></li>
<li class="chapter" data-level="2.3.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#turning-variables-into-factors-and-changing-the-labels"><i class="fa fa-check"></i><b>2.3.3</b> Turning variables into factors and changing the labels</a></li>
<li class="chapter" data-level="2.3.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#looking-for-missing-data-and-other-anomalies"><i class="fa fa-check"></i><b>2.3.4</b> Looking for missing data and other anomalies</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#data-wrangling-with-dplyr"><i class="fa fa-check"></i><b>2.4</b> Data wrangling with dplyr</a></li>
<li class="chapter" data-level="2.5" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-single-verbs"><i class="fa fa-check"></i><b>2.5</b> Using dplyr single verbs</a></li>
<li class="chapter" data-level="2.6" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-for-grouped-operations"><i class="fa fa-check"></i><b>2.6</b> Using dplyr for grouped operations</a></li>
<li class="chapter" data-level="2.7" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#making-comparisons-with-numerical-outcomes"><i class="fa fa-check"></i><b>2.7</b> Making comparisons with numerical outcomes</a></li>
<li class="chapter" data-level="2.8" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html"><i class="fa fa-check"></i><b>3</b> Data visualisation with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#anatomy-of-a-plot"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a plot</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#what-graph-should-i-use"><i class="fa fa-check"></i><b>3.3</b> What graph should I use?</a></li>
<li class="chapter" data-level="3.4" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-histograms"><i class="fa fa-check"></i><b>3.4</b> Visualising numerical variables: Histograms</a></li>
<li class="chapter" data-level="3.5" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-density-plots"><i class="fa fa-check"></i><b>3.5</b> Visualising numerical variables: Density plots</a></li>
<li class="chapter" data-level="3.6" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-box-plots"><i class="fa fa-check"></i><b>3.6</b> Visualising numerical variables: Box plots</a></li>
<li class="chapter" data-level="3.7" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#exploring-relationships-between-two-quantitative-variables-scatterplots"><i class="fa fa-check"></i><b>3.7</b> Exploring relationships between two quantitative variables: scatterplots</a></li>
<li class="chapter" data-level="3.8" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplots-conditioning-in-a-third-variable"><i class="fa fa-check"></i><b>3.8</b> Scatterplots conditioning in a third variable</a></li>
<li class="chapter" data-level="3.9" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplot-matrix"><i class="fa fa-check"></i><b>3.9</b> Scatterplot matrix</a></li>
<li class="chapter" data-level="3.10" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#titles-legends-and-themes-in-ggplot2"><i class="fa fa-check"></i><b>3.10</b> Titles, legends, and themes in ggplot2</a></li>
<li class="chapter" data-level="3.11" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#plotting-categorical-data-bar-charts"><i class="fa fa-check"></i><b>3.11</b> Plotting categorical data: bar charts</a></li>
<li class="chapter" data-level="3.12" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#further-resources"><i class="fa fa-check"></i><b>3.12</b> Further resources</a></li>
<li class="chapter" data-level="3.13" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#summary-2"><i class="fa fa-check"></i><b>3.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Refresher on descriptive statistics &amp; data carpentry</a>
<ul>
<li class="chapter" data-level="4.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#getting-some-data-from-eurobarometer"><i class="fa fa-check"></i><b>4.2</b> Getting some data from Eurobarometer</a></li>
<li class="chapter" data-level="4.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#thinking-about-your-data-filtering-cases"><i class="fa fa-check"></i><b>4.3</b> Thinking about your data: filtering cases</a></li>
<li class="chapter" data-level="4.4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#selecting-variables-using-dplyrselect"><i class="fa fa-check"></i><b>4.4</b> Selecting variables: using <code>dplyr::select</code></a></li>
<li class="chapter" data-level="4.5" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#creating-summated-scales"><i class="fa fa-check"></i><b>4.5</b> Creating summated scales</a></li>
<li class="chapter" data-level="4.6" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#collapsing-categories-in-character-variables"><i class="fa fa-check"></i><b>4.6</b> Collapsing categories in character variables</a></li>
<li class="chapter" data-level="4.7" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#working-with-apparently-cryptic-variable-names-and-levels"><i class="fa fa-check"></i><b>4.7</b> Working with apparently cryptic variable names and levels</a></li>
<li class="chapter" data-level="4.8" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#recoding-factors"><i class="fa fa-check"></i><b>4.8</b> Recoding factors</a></li>
<li class="chapter" data-level="4.9" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#understanding-missing-data"><i class="fa fa-check"></i><b>4.9</b> Understanding missing data</a></li>
<li class="chapter" data-level="4.10" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#exploring-data-frames-visually"><i class="fa fa-check"></i><b>4.10</b> Exploring data frames visually</a></li>
<li class="chapter" data-level="4.11" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#a-quick-recap-on-descriptive-statistics"><i class="fa fa-check"></i><b>4.11</b> A quick recap on descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#central-tendency"><i class="fa fa-check"></i><b>4.11.1</b> Central Tendency</a></li>
<li class="chapter" data-level="4.11.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#dispersion"><i class="fa fa-check"></i><b>4.11.2</b> Dispersion</a></li>
<li class="chapter" data-level="4.11.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#bivariate-analysis"><i class="fa fa-check"></i><b>4.11.3</b> Bivariate analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#further-resources-1"><i class="fa fa-check"></i><b>4.12</b> Further resources</a></li>
<li class="chapter" data-level="4.13" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#summary-3"><i class="fa fa-check"></i><b>4.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html"><i class="fa fa-check"></i><b>5</b> Regression I: Mean differences</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#dependent-variable-numerical-independent-variable-binary"><i class="fa fa-check"></i><b>5.2</b> Dependent variable: numerical | Independent variable: binary</a></li>
<li class="chapter" data-level="5.3" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#calculating-mean-differences-in-r"><i class="fa fa-check"></i><b>5.3</b> Calculating mean differences in <code>R</code></a></li>
<li class="chapter" data-level="5.4" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#visual-exploration"><i class="fa fa-check"></i><b>5.4</b> Visual exploration</a></li>
<li class="chapter" data-level="5.5" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#using-linear-regression-to-calculate-mean-differences"><i class="fa fa-check"></i><b>5.5</b> Using linear regression to calculate mean differences</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#linear-regression"><i class="fa fa-check"></i><b>5.5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.5.2" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#linear-regression-as-a-difference-in-means-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Linear Regression as a Difference-in-Means Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size</a></li>
<li class="chapter" data-level="5.7" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#lab-exercises"><i class="fa fa-check"></i><b>5.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html"><i class="fa fa-check"></i><b>6</b> Regression II: numerical independent variables</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#motivating-regression"><i class="fa fa-check"></i><b>6.2</b> Motivating regression</a></li>
<li class="chapter" data-level="6.3" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>6.3</b> Fitting a simple regression model</a></li>
<li class="chapter" data-level="6.4" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#residuals-r-squared"><i class="fa fa-check"></i><b>6.4</b> Residuals: R squared</a></li>
<li class="chapter" data-level="6.5" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#is-this-the-same-as-last-week"><i class="fa fa-check"></i><b>6.5</b> Is this the same as last week?</a></li>
<li class="chapter" data-level="6.6" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#regression-assumptions"><i class="fa fa-check"></i><b>6.6</b> Regression assumptions</a></li>
<li class="chapter" data-level="6.7" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#lab-exercises-1"><i class="fa fa-check"></i><b>6.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-iii.html"><a href="regression-iii.html"><i class="fa fa-check"></i><b>7</b> Regression III</a>
<ul>
<li class="chapter" data-level="7.1" data-path="regression-iii.html"><a href="regression-iii.html#fitting-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>7.1</b> Fitting regression with categorical predictors</a></li>
<li class="chapter" data-level="7.2" data-path="regression-iii.html"><a href="regression-iii.html#motivating-multiple-regression"><i class="fa fa-check"></i><b>7.2</b> Motivating multiple regression</a></li>
<li class="chapter" data-level="7.3" data-path="regression-iii.html"><a href="regression-iii.html#fitting-and-interpreting-a-multiple-regression-model"><i class="fa fa-check"></i><b>7.3</b> Fitting and interpreting a multiple regression model</a></li>
<li class="chapter" data-level="7.4" data-path="regression-iii.html"><a href="regression-iii.html#presenting-your-regression-results."><i class="fa fa-check"></i><b>7.4</b> Presenting your regression results.</a></li>
<li class="chapter" data-level="7.5" data-path="regression-iii.html"><a href="regression-iii.html#rescaling-input-variables-to-assist-interpretation"><i class="fa fa-check"></i><b>7.5</b> Rescaling input variables to assist interpretation</a></li>
<li class="chapter" data-level="7.6" data-path="regression-iii.html"><a href="regression-iii.html#testing-conditional-hypothesis-interactions"><i class="fa fa-check"></i><b>7.6</b> Testing conditional hypothesis: interactions</a></li>
<li class="chapter" data-level="7.7" data-path="regression-iii.html"><a href="regression-iii.html#multicollinearity"><i class="fa fa-check"></i><b>7.7</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html"><i class="fa fa-check"></i><b>8</b> Studying relationships between two factors</a>
<ul>
<li class="chapter" data-level="8.1" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#cross-tabulations"><i class="fa fa-check"></i><b>8.1</b> Cross-tabulations</a></li>
<li class="chapter" data-level="8.2" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#expected-frequencies-and-chi-square"><i class="fa fa-check"></i><b>8.2</b> Expected frequencies and Chi-Square</a></li>
<li class="chapter" data-level="8.3" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#odds-and-odd-ratios"><i class="fa fa-check"></i><b>8.3</b> Odds and odd ratios</a></li>
<li class="chapter" data-level="8.4" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Logistic regression</a></li>
<li class="chapter" data-level="8.5" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#fitting-logistic-regression"><i class="fa fa-check"></i><b>8.5</b> Fitting logistic regression</a></li>
<li class="chapter" data-level="8.6" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#assessing-model-fit-confusion-matrix"><i class="fa fa-check"></i><b>8.6</b> Assessing model fit: confusion matrix</a></li>
<li class="chapter" data-level="8.7" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#interactions"><i class="fa fa-check"></i><b>8.7</b> Interactions</a></li>
<li class="chapter" data-level="8.8" data-path="studying-relationships-between-two-factors.html"><a href="studying-relationships-between-two-factors.html#further-resources-2"><i class="fa fa-check"></i><b>8.8</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference</a></li>
<li class="chapter" data-level="10" data-path="wrapping-up.html"><a href="wrapping-up.html"><i class="fa fa-check"></i><b>10</b> Wrapping up</a></li>
<li class="chapter" data-level="11" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>11</b> Appendix</a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="appendix.html"><a href="appendix.html#expected-frequencies"><i class="fa fa-check"></i><b>11.0.1</b> Expected frequencies</a></li>
<li class="chapter" data-level="11.1" data-path="appendix.html"><a href="appendix.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.1</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="appendix.html"><a href="appendix.html#fitting-logistic-regression-alternative"><i class="fa fa-check"></i><b>11.1.1</b> Fitting logistic regression: alternative</a></li>
<li class="chapter" data-level="11.1.2" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-deviance-and-pseudo-r-squared"><i class="fa fa-check"></i><b>11.1.2</b> Assessing model fit: deviance and pseudo r squared</a></li>
<li class="chapter" data-level="11.1.3" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-roc-curves"><i class="fa fa-check"></i><b>11.1.3</b> Assessing model fit: ROC curves</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelling Criminological Data CRIM20452</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-ii-numerical-independent-variables" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Regression II: numerical independent variables<a href="regression-ii-numerical-independent-variables.html#regression-ii-numerical-independent-variables" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-3" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Introduction<a href="regression-ii-numerical-independent-variables.html#introduction-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Last week, we began studying linear regression modelling as a powerful tool to assess the relationship between two variables. We learned that, in bivariate analysis, we always start with a research hypothesis derived from theory, and from that, we identify a <em>dependent</em> variable (also known as an <em>outcome</em> or <em>response</em> variable) and an <em>independent</em> variable (also known as an <em>explanatory</em> or <em>predictor</em> variable). We then learned that when the dependent variable is numerical and the independent variable is binary, we examine their association by calculating the difference between the average scores of the dependent variable across the two groups of the binary independent variable; that is, the mean difference represents the association in this scenario. Finally, we learned that the linear regression framework is a powerful method that allows us to efficiently estimate the mean difference. By defining a linear model given by <span class="math inline">\(Y = \alpha + \beta \cdot X\)</span>, where <span class="math inline">\(Y\)</span> represents the numerical dependent variable and <span class="math inline">\(X\)</span> represents the binary independent variable, we can use the linear regression estimator to estimate values for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. In this model, <span class="math inline">\(\widehat{\alpha}\)</span> represents the average score of the dependent variable among observations in the reference group, and <span class="math inline">\(\widehat{\beta}\)</span> represents the mean difference.</p>
<p>In this session, we will continue our journey with linear regression models. This form of analysis has been one of the primary techniques of data analysis in the social sciences for many years, and it belongs to a family of techniques known as generalised linear models. Regression is a flexible method that allows us to “explain” or “predict” a given outcome (<span class="math inline">\(Y\)</span>) as a function of an independent variable (<span class="math inline">\(X\)</span>). Building on last week’s material, this week we will study the scenario in which both the dependent variable <em>and</em> the independent variable are numerical. It is the same linear model as last week; in fact, we can always use linear regression models when the dependent variable is numerical. For example, next week, we will expand it even further and learn about models with categorical independent variables as well as models with multiple explanatory variables simultaneously—what we call <em>multiple</em> regression models.</p>
<p>We will use a new dataset today, specifically the data used by Patrick Sharkey and his colleagues, to study the association between non-profit organisations and crime levels. In <a href="https://books.wwnorton.com/books/Uneasy-Peace/"><em>“Uneasy Peace”</em></a>, Prof Sharkey argues that one factor contributing to the decline of crime from the 1990s onwards was the role played by non-profit community organisations in bringing peace and services to deteriorated neighbourhoods. Watch this video to gain a more theoretical background and learn about the research.</p>
<iframe src="https://www.youtube.com/embed/47IISvRXmpA" width="672" height="400px" data-external="1">
</iframe>
<p>In this session, we will use the replication data from one of the papers that Prof Sharkey published to study this question. This data is found in the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/46WIH0">Harvard Dataverse</a>. If you are interested in the specific study analysing this data, you can find it <a href="https://journals.sagepub.com/doi/abs/10.1177/0003122417736289">here</a>.</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="regression-ii-numerical-independent-variables.html#cb373-1" tabindex="-1"></a><span class="co"># create an object with the URL address of the dataset</span></span>
<span id="cb373-2"><a href="regression-ii-numerical-independent-variables.html#cb373-2" tabindex="-1"></a>urlfile <span class="ot">&lt;-</span> <span class="st">&quot;https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/46WIH0/ARS2VS&quot;</span></span>
<span id="cb373-3"><a href="regression-ii-numerical-independent-variables.html#cb373-3" tabindex="-1"></a></span>
<span id="cb373-4"><a href="regression-ii-numerical-independent-variables.html#cb373-4" tabindex="-1"></a><span class="co"># import the dataset into R</span></span>
<span id="cb373-5"><a href="regression-ii-numerical-independent-variables.html#cb373-5" tabindex="-1"></a>communitycrime <span class="ot">&lt;-</span> <span class="fu">read.table</span>(urlfile, <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\t</span><span class="st">&#39;</span>,<span class="at">header =</span> T)</span></code></pre></div>
<p>As before, we create an object with the permanent <code>URL</code> address, and then we use a function to read the data into R. The data can be saved using an <code>api</code>, and it is in tab-separated format. We use the <code>read.table</code> function from base R for this. We pass two arguments to the function <code>sep= '\t'</code>, telling R this file is tab separated. The <code>header = T</code> function tells R that it is TRUE (T) that this file has a first row that acts as a header (this row has the name of the variables).</p>
<p>There are many more variables here that we are going to need, so let’s do some filtering and selection. We will focus on 2012, the most recent year in the dataset, and just a few select variables.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="regression-ii-numerical-independent-variables.html#cb374-1" tabindex="-1"></a><span class="co"># load the dplyr package for data cleaning</span></span>
<span id="cb374-2"><a href="regression-ii-numerical-independent-variables.html#cb374-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb374-3"><a href="regression-ii-numerical-independent-variables.html#cb374-3" tabindex="-1"></a></span>
<span id="cb374-4"><a href="regression-ii-numerical-independent-variables.html#cb374-4" tabindex="-1"></a><span class="co"># create a new object, &#39;df&#39;, that only includes</span></span>
<span id="cb374-5"><a href="regression-ii-numerical-independent-variables.html#cb374-5" tabindex="-1"></a><span class="co"># observations from 2012</span></span>
<span id="cb374-6"><a href="regression-ii-numerical-independent-variables.html#cb374-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">filter</span>(communitycrime, year <span class="sc">==</span> <span class="st">&quot;2012&quot;</span>)</span>
<span id="cb374-7"><a href="regression-ii-numerical-independent-variables.html#cb374-7" tabindex="-1"></a></span>
<span id="cb374-8"><a href="regression-ii-numerical-independent-variables.html#cb374-8" tabindex="-1"></a><span class="co"># select just some variables from the dataset</span></span>
<span id="cb374-9"><a href="regression-ii-numerical-independent-variables.html#cb374-9" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">select</span>(df, place_name, state_name, viol_r, </span>
<span id="cb374-10"><a href="regression-ii-numerical-independent-variables.html#cb374-10" tabindex="-1"></a>             black, lesshs, unemployed, fborn,</span>
<span id="cb374-11"><a href="regression-ii-numerical-independent-variables.html#cb374-11" tabindex="-1"></a>             incarceration, log_incarceraton,</span>
<span id="cb374-12"><a href="regression-ii-numerical-independent-variables.html#cb374-12" tabindex="-1"></a>             swornftime_r, log_viol_r, largest50)</span></code></pre></div>
<p>So now we have a more manageable data set that we can use for this session. The file includes a sample of 264 US cities (see variable <code>place_name</code>) across 44 of states (variable <code>state_name</code>). As ever, we always start understanding what our unit of analysis is (i.e., what each row represents). In this case, our unit of analysis are cities in the US: each row of the data frame <code>df</code> represents a different city. Then we have information (<em>variables</em>) on those cities, such as their level of violence and their unemployment rate. You can check the names of each column (i.e., each variable) included in the dataset by using the <code>names()</code> function.</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="regression-ii-numerical-independent-variables.html#cb375-1" tabindex="-1"></a><span class="co"># print the names of all columns in the dataset &#39;df&#39;</span></span>
<span id="cb375-2"><a href="regression-ii-numerical-independent-variables.html#cb375-2" tabindex="-1"></a><span class="fu">names</span>(df)</span></code></pre></div>
<pre><code>##  [1] &quot;place_name&quot;       &quot;state_name&quot;       &quot;viol_r&quot;           &quot;black&quot;           
##  [5] &quot;lesshs&quot;           &quot;unemployed&quot;       &quot;fborn&quot;            &quot;incarceration&quot;   
##  [9] &quot;log_incarceraton&quot; &quot;swornftime_r&quot;     &quot;log_viol_r&quot;       &quot;largest50&quot;</code></pre>
<p>The variables we have extracted contain information on the demographic composition of those cities (per cent black population, per cent without a high school degree, per cent unemployed, per cent foreign-born), and some criminal justice ones (incarceration rate and the rate of sworn full-time police officers). We also have measures of the violence rate and a binary indicator that tells us if the city is one of the 50 largest in the country.</p>
<p>We will examine the relationship between unemployment and violence. Based on Sharkey’s book, we would expect that cities with a larger percentage of unemployed residents would also have larger violence rates. Therefore, <em>violence rate</em> is our dependent variable, and <em>unemployment percentage</em> is our independent variable. They are both numerical variables. Let’s start examining the following scatterplot. On the Y-axis, we have the distribution of our dependent variable, violence rate, ranging from 0 to just over 2000. On the X-axis, we have the distribution of our independent variable, unemployment percentage, ranging from 0 to 14. Each dot in the scatterplot represents one of the 264 cities in our dataset.</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="regression-ii-numerical-independent-variables.html#cb377-1" tabindex="-1"></a><span class="co"># load the &#39;ggplot2&#39; package</span></span>
<span id="cb377-2"><a href="regression-ii-numerical-independent-variables.html#cb377-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb377-3"><a href="regression-ii-numerical-independent-variables.html#cb377-3" tabindex="-1"></a></span>
<span id="cb377-4"><a href="regression-ii-numerical-independent-variables.html#cb377-4" tabindex="-1"></a><span class="co"># plot a scatterplot</span></span>
<span id="cb377-5"><a href="regression-ii-numerical-independent-variables.html#cb377-5" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> unemployed, <span class="at">y =</span> viol_r)) <span class="sc">+</span></span>
<span id="cb377-6"><a href="regression-ii-numerical-independent-variables.html#cb377-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() </span></code></pre></div>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>What do you think when looking at this scatterplot? Is there a relationship between violence and unemployment? Does it look like cities with a high score on the X-axis (unemployment) also have a high score on the Y-axis (violent crime)? It may be a bit hard to see, but we think there is certainly a trend.</p>
</div>
<div id="motivating-regression" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Motivating regression<a href="regression-ii-numerical-independent-variables.html#motivating-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now, imagine that we play a game. Imagine we have all the names of the cities in a hat, and we randomly take one of the names from the hat. You’re sitting in the audience, and you have to guess the level of violence (<code>viol_r</code>) for that city. Imagine paying £150 to the student who gets the closest to the right value. What would you guess if you only had one guess and knew (as we do) how violent crime is distributed across all cities?</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="regression-ii-numerical-independent-variables.html#cb378-1" tabindex="-1"></a><span class="co"># plot a density plot with the distribution of violence rate</span></span>
<span id="cb378-2"><a href="regression-ii-numerical-independent-variables.html#cb378-2" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> viol_r)) <span class="sc">+</span> </span>
<span id="cb378-3"><a href="regression-ii-numerical-independent-variables.html#cb378-3" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb378-4"><a href="regression-ii-numerical-independent-variables.html#cb378-4" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(df<span class="sc">$</span>viol_r), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb378-5"><a href="regression-ii-numerical-independent-variables.html#cb378-5" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Density estimate and mean of violent crime rate&quot;</span>)</span></code></pre></div>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="regression-ii-numerical-independent-variables.html#cb379-1" tabindex="-1"></a><span class="fu">summary</span>(df<span class="sc">$</span>log_viol_r)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   3.831   5.532   6.149   6.061   6.595   7.634</code></pre>
<p>If we only had one shot, we would go for the mean (or perhaps the median, given the skew). Most cities have values clustered around those values, which is another way of saying they are bound to be not too far from them. It would be silly to say 15, for example, or 1500, since very few cities have such low or high levels of violence (as measured by <code>viol_r</code>).</p>
<p>Imagine, however, that now, when we take the name of the city from the hat, you are also told how much unemployment there is in that city—so the value of the <code>unemployed</code> variable for the city that has been selected (for example, 11). Imagine that you also have the scatterplot we produced earlier in front of you. Would you still go for a value close to 500 (the mean) as your best guess for the value of the selected city?</p>
<p>We certainly would not go with the overall mean or median as our prediction any more. If somebody told us that 10% of the population residing in the city was unemployed, we would be more inclined to guess the average level of violence for the cities <em>with that level of unemployment</em> (the conditional mean) rather than the overall mean across all the cities. Wouldn’t you?</p>
<p>If we plot the conditional means, we can see that the mean <code>viol_r</code> for cities that report an unemployment rate of 11% is around 1000. So you may be better off guessing that.</p>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Linear regression tackles this problem using a slightly different approach. Rather than focusing on the conditional mean (smoothed or not), it draws a straight line that tries to capture the trend in the data. If we focus on the regions of the scatterplot that are less sparse, we see that this is an upward trend, suggesting that as the level of unemployment increases, so does the level of violent crime.</p>
<p>Simple linear regression draws a single straight line of predicted values <strong>as the model for the data</strong>. This line would be a <strong>model</strong>, a <em>simplification</em> of the real world like any other model (e.g., a toy pistol, an architectural drawing, a subway map) that assumes that there is approximately a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Let’s draw the regression line:</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="regression-ii-numerical-independent-variables.html#cb381-1" tabindex="-1"></a><span class="co"># produce a scatterplot and a regression line</span></span>
<span id="cb381-2"><a href="regression-ii-numerical-independent-variables.html#cb381-2" tabindex="-1"></a><span class="co"># summarising the relationship between unemployment</span></span>
<span id="cb381-3"><a href="regression-ii-numerical-independent-variables.html#cb381-3" tabindex="-1"></a><span class="co"># and violence rate</span></span>
<span id="cb381-4"><a href="regression-ii-numerical-independent-variables.html#cb381-4" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> unemployed, <span class="at">y =</span> viol_r)) <span class="sc">+</span></span>
<span id="cb381-5"><a href="regression-ii-numerical-independent-variables.html#cb381-5" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb381-6"><a href="regression-ii-numerical-independent-variables.html#cb381-6" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) </span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The <code>geom_smooth</code> function asks for a geom with the regression line, <code>method = lm</code> asks for the linear regression line, <code>se = FALSE</code> asks for just the line to be printed, and the other arguments specify the colour and thickness of the line.</p>
<p>What that line is doing is giving you guesses (predictions) for the values of violent crime based on the information we have about the level of unemployment. It gives you one possible guess for the value of violence for every possible value of unemployment and links them all together in a straight line.</p>
<p>Another way to think about this line is as the best possible summary of the cloud of points represented in the scatterplot (if we can assume that a straight line would do a good job of this). If we were to tell you to draw a straight line that best represents this pattern of points, the regression line would be the one that best does it (if certain assumptions are met).</p>
<p>The linear model is then a model that takes the form of the equation of a straight line through the data. The line does not go through all the points. In fact, you can see it is a slightly less accurate representation than the (smoothed) conditional means:</p>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>As De Veaux et al. (2012: 179) highlight: “like all models of the real world, the line will be wrong, wrong in the sense that it can’t match reality exactly. But it can help us understand how the variables are associated”. A map is never a perfect representation of the world; the same happens with statistical models. Yet, as with maps, models can be helpful.</p>
</div>
<div id="fitting-a-simple-regression-model" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Fitting a simple regression model<a href="regression-ii-numerical-independent-variables.html#fitting-a-simple-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to draw a regression line, we need to know two things:
(1) We need to know where the line begins: what is the value of <span class="math inline">\(Y\)</span> (our dependent variable) when <span class="math inline">\(X\)</span> (our independent variable) is 0 so that we have a point from which to start drawing the line. The technical name for this point is the <strong>intercept</strong>.
(2) And we need to know what is the <strong>slope</strong> of that line, that is, how inclined the line is, the angle of the line.</p>
<p>If you recall from elementary algebra (and you may not), the equation for any straight line is:
<span class="math inline">\(Y = m \cdot X + c\)</span>
In statistics, we use a slightly different notation, although the equation remains the same:
<span class="math inline">\(Y = \alpha + \beta \cdot X\)</span></p>
<p>We need the origin of the line (<span class="math inline">\(\alpha\)</span>) and the slope of the line (<span class="math inline">\(\beta\)</span>). How does <code>R</code> get the intercept and the slope for the red line? How does <code>R</code> know where to draw this line? We need to estimate these <strong>parameters</strong> (or <strong>regression coefficients</strong>) from the data. How? We don’t have time to get into these more mathematical details now. You should study the <a href="http://link.springer.com/chapter/10.1007/978-1-4614-7138-7_3">required reading</a> to understand this (<em>required means it is required, it is not optional</em>)<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>. For now, suffice to say that for linear regression models like the one we cover here when drawing the line, <code>R</code> tries to minimise the distance from every point in the scatterplot to the regression line using a method called <strong>least squares estimation</strong>.</p>
<p>In order to fit the model, we use the <code>lm()</code> function using the formula specification <code>(Y ~ X)</code>—exactly as we did last week. Typically, you want to store your regression model into an object. For example, let’s fit a linear model regression violence rates on percentage of unemployment and store the results under an object named <code>fit_1</code>:</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="regression-ii-numerical-independent-variables.html#cb383-1" tabindex="-1"></a><span class="co"># run a regression model where violence rate is</span></span>
<span id="cb383-2"><a href="regression-ii-numerical-independent-variables.html#cb383-2" tabindex="-1"></a><span class="co"># the dependent variable and per cent unemployed</span></span>
<span id="cb383-3"><a href="regression-ii-numerical-independent-variables.html#cb383-3" tabindex="-1"></a><span class="co"># is the independent variable</span></span>
<span id="cb383-4"><a href="regression-ii-numerical-independent-variables.html#cb383-4" tabindex="-1"></a>fit_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(viol_r <span class="sc">~</span> unemployed, <span class="at">data =</span> df)</span></code></pre></div>
<p>In your <code>R</code> Studio global environment space, you will see a new object called <code>fit_1</code> with 12 elements on it. We can get a sense for what this object is and includes using the functions we introduced in previous weeks:</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="regression-ii-numerical-independent-variables.html#cb384-1" tabindex="-1"></a><span class="co"># check the class of the object &quot;fit_1&quot;</span></span>
<span id="cb384-2"><a href="regression-ii-numerical-independent-variables.html#cb384-2" tabindex="-1"></a><span class="fu">class</span>(fit_1)</span></code></pre></div>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="regression-ii-numerical-independent-variables.html#cb386-1" tabindex="-1"></a><span class="co"># check the attributes of the object &quot;fit_1&quot;</span></span>
<span id="cb386-2"><a href="regression-ii-numerical-independent-variables.html#cb386-2" tabindex="-1"></a><span class="fu">attributes</span>(fit_1)</span></code></pre></div>
<pre><code>## $names
##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;        
## 
## $class
## [1] &quot;lm&quot;</code></pre>
<p><code>R</code> is telling us that this is an object of class <code>lm</code> and includes several attributes. One of the beauties of <code>R</code> is that you are producing all the results from running the model, putting them in an object, and then giving you the opportunity to use them later on. If you want to see the basic results from running the model, you can simply print the name of the object.</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="regression-ii-numerical-independent-variables.html#cb388-1" tabindex="-1"></a><span class="co"># print results of the regression model &quot;fit_1&quot;</span></span>
<span id="cb388-2"><a href="regression-ii-numerical-independent-variables.html#cb388-2" tabindex="-1"></a>fit_1</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = viol_r ~ unemployed, data = df)
## 
## Coefficients:
## (Intercept)   unemployed  
##      -213.7        121.7</code></pre>
<p>This is exactly the same as we did last week. The output of the <code>lm()</code> function in <code>R</code> provides us with two numbers under “Coefficients:”. The value of -213.7 is the estimated value for the <strong>Intercept</strong> (i.e., <span class="math inline">\(\widehat{\alpha}= -213.7\)</span>). This indicates the predicted score of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> equals zero. In this case, it indicates that a hypothetical city with 0% of its population under unemployment would have an expected violence rate of -213.7.</p>
<p>We then need the <span class="math inline">\(\widehat{\beta}\)</span> regression coefficient for our independent variable, the value that will shape the <strong>slope</strong> in this scenario. This value is 121.7. This estimated regression coefficient for our independent variable has a convenient interpretation. When the value is positive, it tells us that <em>every one unit increase in</em> <span class="math inline">\(X\)</span> <em>is associated with a</em> <span class="math inline">\(\beta\)</span> <em>increase on</em> <span class="math inline">\(Y\)</span>. If the coefficient is negative, then it represents a decrease in <span class="math inline">\(Y\)</span>. Here, we can read it as “every one unit increase in the percentage of people unemployed is associated with a 121.7 unit increase in violence rate.”</p>
<p>Knowing these two parameters not only allows us to draw the line, but we can also solve for any given value of <span class="math inline">\(X\)</span>. Let’s go back to our guess-the-violence game. Imagine if we tell you the unemployment percentage is 4. What would be your best bet now? We can go back to our regression line equation and insert the estimated parameters:</p>
<p><span class="math inline">\(Y = \alpha + \beta \cdot X\)</span><br />
<span class="math inline">\(Y = -213.7 + 121.7 \cdot 4\)</span><br />
<span class="math inline">\(Y = 273.1\)</span></p>
<p>Or, if you don’t want to do the calculation yourself, you can use the <code>predict</code> function (differences are due to rounding error):</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="regression-ii-numerical-independent-variables.html#cb390-1" tabindex="-1"></a><span class="co"># First, you name your stored model, and then you identify the new data </span></span>
<span id="cb390-2"><a href="regression-ii-numerical-independent-variables.html#cb390-2" tabindex="-1"></a><span class="co"># (which has to be in a data frame format and with a variable name matching </span></span>
<span id="cb390-3"><a href="regression-ii-numerical-independent-variables.html#cb390-3" tabindex="-1"></a><span class="co"># the one in the original data set)</span></span>
<span id="cb390-4"><a href="regression-ii-numerical-independent-variables.html#cb390-4" tabindex="-1"></a><span class="fu">predict</span>(fit_1, <span class="fu">data.frame</span>(<span class="at">unemployed =</span> <span class="fu">c</span>(<span class="dv">4</span>))) </span></code></pre></div>
<pre><code>##        1 
## 272.9868</code></pre>
<p>This is the expected value of <span class="math inline">\(Y\)</span>, <em>violence rate</em>, when <span class="math inline">\(X\)</span>, <em>percentage of unemployment</em>, is 4% of the population <em>according to our model</em> (according to our simplification of the real world, our simplification of the whole cloud of points into just one straight line). Look back at the scatterplot we produced earlier with the green line. Does it look like the red line when X is 4 corresponds to a value of Y of 273.1?</p>
</div>
<div id="residuals-r-squared" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Residuals: R squared<a href="regression-ii-numerical-independent-variables.html#residuals-r-squared" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the output, when we ran the model above, we saw something called the residuals. The residuals (in regression) are the differences between the observed values of Y for each case minus the predicted or expected value of Y, in other words, the distances between each point in the dataset and the regression line (see the visual example below).</p>
<div class="float">
<img src="imgs/residual_01.png" alt="@http://www.shodor.org/" />
<div class="figcaption"><span class="citation">@http://www.shodor.org</span>/</div>
</div>
<p>You see, we have our line, our predicted value, and then we have the black dots, which are our observed values. The distance between them is essentially the amount by which we were wrong, and all these distances between observed and predicted values are our residuals. <strong>Least square estimation</strong>, the “machine” we use to build the regression line, essentially aims to reduce the squared average of all these distances: that’s how it draws the line.</p>
<p>Why do we have residuals? Well, think about it. The fact that the line is not a perfect representation of the cloud of points makes sense, doesn’t it? You cannot predict perfectly what the value of Y is for every city just by looking ONLY at unemployment! This line only uses information regarding unemployment. This means there’s bound to be some difference between our predicted level of violence, given our knowledge of unemployment (the regression line) and the actual level of violence (the actual location of the points in the scatterplot). Other things that matter are not being taken into account by our model to predict the values of Y. There are other things that surely matter regarding understanding violence. And then, of course, we have measurement errors and other forms of noise.</p>
<p>We can re-write our equation like this if we want to represent each value of Y (rather than the predicted value of Y) then:
<span class="math inline">\(y = b_0 + b_1x + e(residuals)\)</span></p>
<p>The residuals capture how much variation is unexplained - how much we still have to learn if we want to understand variation in Y. A good model tries to maximise explained variation and reduce the magnitude of the residuals.</p>
<p>We can use information from the residuals to produce a measure of effect size - how good our model is in predicting variation in our dependent variables. Remember our game where we try to guess violence (Y)? If we had no information about X, our best bet for Y would be the mean of Y. The regression line aims to improve that prediction. By knowing X’s values, we can build a regression line that aims to get us closer to the actual values of Y.</p>
<p><img src="https://people.richland.edu/james/ictcm/2004/weight2.png" /></p>
<p>The distance between the mean (our best guess without any other information) and the observed value of Y is the <strong>total variation</strong>. The residual is the difference between our predicted value of Y and the observed value of Y. This is what we cannot explain (i.e., variation in Y that is <em>unexplained</em>). The difference between the mean value of Y and the expected value of Y (the value given by our regression line) is how much better we are doing with our prediction by using information about X (i.e., in our previous example, it would be variation in Y that can be <em>explained</em> by knowing about unemployment). How much closer the regression line gets us to the observed values? We can then contrast these two different sources of variation (explained and unexplained) to produce a single measure of how good our model is. The formula is as follows:</p>
<p><span class="math display">\[
R^2 = \frac{\text{Explained Variation}}{\text{Total Variation}}
\]</span></p>
<p>This can also be written as:</p>
<p><span class="math display">\[
R^2 = \frac{\sum (\hat{Y}_i - \bar{Y})^2}{\sum (Y_i - \bar{Y})^2}
\]</span></p>
<p>Where:
- <span class="math inline">\(Y_i\)</span>: observed values of the dependent variable.
- <span class="math inline">\(\hat{Y}_i\)</span>: predicted values from the regression line.
- <span class="math inline">\(\bar{Y}\)</span>: mean of the observed <span class="math inline">\(Y\)</span> values.
- <span class="math inline">\(\sum (\hat{Y}_i - \bar{Y})^2\)</span>: explained variation (the squared differences between the predicted values and the mean of <span class="math inline">\(Y\)</span>).
- <span class="math inline">\(\sum (Y_i - \bar{Y})^2\)</span>: total variation (the squared differences between the observed values and the mean of <span class="math inline">\(Y\)</span>).</p>
<!--![formula](http://docs.oracle.com/cd/E40248_01/epm.1112/cb_statistical_11123500/images/graphics/r_squared_constant.gif)-->
<p>All this formula is doing is taking a ratio of the explained variation (the squared differences between the regression line and the mean of Y for each observation) by the total variation (the squared differences of the observed values of Y for each observation from the mean of Y). This gives us a measure of the <strong>percentage of variation in Y that is “explained” by X</strong>.</p>
<p>Then, we can take this value as a measure of the strength of our model. If you look at the R output, you will see that the <span class="math inline">\(R^2\)</span> for our model was .29 (look at the multiple R square value in the output). We can say that our model explains 29% of the variance in the fear of violent crime measure.</p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="regression-ii-numerical-independent-variables.html#cb392-1" tabindex="-1"></a><span class="co">#As an aside, and to continue emphasising your appreciation of the object-oriented nature of R,</span></span>
<span id="cb392-2"><a href="regression-ii-numerical-independent-variables.html#cb392-2" tabindex="-1"></a><span class="co">#when we run the summary() function, we are simply generating a list object of the class summary.lm.</span></span>
<span id="cb392-3"><a href="regression-ii-numerical-independent-variables.html#cb392-3" tabindex="-1"></a><span class="fu">attributes</span>(<span class="fu">summary</span>(fit_1))</span></code></pre></div>
<pre><code>## $names
##  [1] &quot;call&quot;          &quot;terms&quot;         &quot;residuals&quot;     &quot;coefficients&quot; 
##  [5] &quot;aliased&quot;       &quot;sigma&quot;         &quot;df&quot;            &quot;r.squared&quot;    
##  [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot;    &quot;cov.unscaled&quot; 
## 
## $class
## [1] &quot;summary.lm&quot;</code></pre>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="regression-ii-numerical-independent-variables.html#cb394-1" tabindex="-1"></a><span class="co">#This means that we can access its elements if so we wish. </span></span>
<span id="cb394-2"><a href="regression-ii-numerical-independent-variables.html#cb394-2" tabindex="-1"></a><span class="co">#So, for example, to obtain just the R Squared, we could ask for:</span></span>
<span id="cb394-3"><a href="regression-ii-numerical-independent-variables.html#cb394-3" tabindex="-1"></a><span class="fu">summary</span>(fit_1)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.2991119</code></pre>
<p>Knowing how to interpret this is important. <span class="math inline">\(R^2\)</span> ranges from 0 to 1. The greater it is, the more powerful our model is; the more explaining we are doing, the better we can account for variation in our outcome <span class="math inline">\(Y\)</span> with our input. In other words, the stronger the relationship is between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. As with all the other measures of effect size, interpretation is a matter of judgement. You are advised to see what other researchers report in relation to the particular outcome that you may be exploring. <a href="http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit">This</a> is a reasonable explanation of how to interpret R-Squared.</p>
<p>Weisburd and Britt (2009: 437) suggest that in criminal justice, you rarely see values for <span class="math inline">\(R^2\)</span> greater than .40. Thus if your <span class="math inline">\(R^2\)</span> is larger than .40, you can assume you have a powerful model. When, on the other hand, <span class="math inline">\(R^2\)</span> is lower than .15 or .2, the model is likely to be viewed as relatively weak. Our observed r squared here is not too bad. There is considerable room for improvement to develop a better model to explain violence<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>. In any case, many people would argue that <span class="math inline">\(R^2\)</span> is a bit overrated. You need to be aware of what it measures and the context in which you use it. Read <a href="http://blog.minitab.com/blog/adventures-in-statistics/how-high-should-r-squared-be-in-regression-analysis">here</a> for some additional detail.</p>
</div>
<div id="is-this-the-same-as-last-week" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Is this the same as last week?<a href="regression-ii-numerical-independent-variables.html#is-this-the-same-as-last-week" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Yes! Last week, we used linear regression modelling to assess the association between a binary independent variable and a numerical dependent variable. We learned that the estimated intercept (<span class="math inline">\(\alpha\)</span>) represents the average score of the dependent variable in the reference group (<span class="math inline">\(\bar{Y} \mid x=0\)</span>), and that the slope coefficient (<span class="math inline">\(\beta\)</span>) represents the mean difference (<span class="math inline">\((\bar{Y} \mid x=1) - (\bar{Y} \mid x=0)\)</span>). Now, we’re taking a deeper dive into linear regression modelling and understanding that fitting a regression model is essentially drawing a line—the line that best represents the linear relationship between an independent variable <span class="math inline">\(X\)</span> and a dependent variable <span class="math inline">\(Y\)</span>.</p>
<p>In fact, everything we learned today applies for what we studied last week. A binary independent variable is just a special case of linear regression models. Let’s have a look at another independent variable: whether the city is one of the largest 50 cities in the United States or not. The name of the variable is <code>largest50</code></p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="regression-ii-numerical-independent-variables.html#cb396-1" tabindex="-1"></a><span class="co"># print frequency table of variable largest50</span></span>
<span id="cb396-2"><a href="regression-ii-numerical-independent-variables.html#cb396-2" tabindex="-1"></a><span class="fu">table</span>(df<span class="sc">$</span>largest50)</span></code></pre></div>
<pre><code>## 
##   0   1 
## 216  48</code></pre>
<p>48 observations have a score of 1, indicating that they are one of the 50 largest cities in the United States, whereas 216 observations have a score of 0, indicating that they not. Sharkey also hypothesised that violent crime is more common in large cities. So, we can treat violence rate (<code>viol_r</code>) as our numerical dependent variable and whether they are one of the largest 50 cities in the country (<code>largest50</code>) as our new binary independent variable. We can test this association by fitting a linear regression model that estimates parameters <span class="math inline">\(\alpha\)</span> (the intercept) and <span class="math inline">\(\beta\)</span> (the slope coefficient, in this case representing the mean difference). But, if today we learned that linear regression models are essentially all about drawing a line, what if we start producing a scatterplot?</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="regression-ii-numerical-independent-variables.html#cb398-1" tabindex="-1"></a><span class="co"># produce a scatterplot between violence rate and largest 50</span></span>
<span id="cb398-2"><a href="regression-ii-numerical-independent-variables.html#cb398-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> largest50, <span class="at">y =</span> viol_r)) <span class="sc">+</span></span>
<span id="cb398-3"><a href="regression-ii-numerical-independent-variables.html#cb398-3" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>This scatterplot does not seem very useful… This is because, as we already knew, our independent variable in this example is binary, which means it can only take two values: 0 or 1. Therefore, all the dots in the scatterplot are stacked on 0 or on 1.</p>
<p>But this scatterplot is not entirely useless! If we quickly check, for instance, what the average violence rate among smaller cities is and what the average violence rate among larger cities is, we can try to eyeball those means in the plot</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="regression-ii-numerical-independent-variables.html#cb399-1" tabindex="-1"></a><span class="co"># load the dplyr package</span></span>
<span id="cb399-2"><a href="regression-ii-numerical-independent-variables.html#cb399-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb399-3"><a href="regression-ii-numerical-independent-variables.html#cb399-3" tabindex="-1"></a></span>
<span id="cb399-4"><a href="regression-ii-numerical-independent-variables.html#cb399-4" tabindex="-1"></a><span class="co"># produce grouped means using group_by</span></span>
<span id="cb399-5"><a href="regression-ii-numerical-independent-variables.html#cb399-5" tabindex="-1"></a>df <span class="sc">%&gt;%</span></span>
<span id="cb399-6"><a href="regression-ii-numerical-independent-variables.html#cb399-6" tabindex="-1"></a>  <span class="fu">group_by</span>(largest50) <span class="sc">%&gt;%</span></span>
<span id="cb399-7"><a href="regression-ii-numerical-independent-variables.html#cb399-7" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">average_violence =</span> <span class="fu">mean</span>(viol_r))</span></code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   largest50 average_violence
##       &lt;dbl&gt;            &lt;dbl&gt;
## 1         0             486.
## 2         1             824.</code></pre>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The average violence rate in smaller cities is 486, whereas the average violence rate is larger cities is 824. Although this scatterplot does not look particularly useful at first, we can spot those two means in each distribution of “stacked dots”. But isn’t linear regression modelling essentially just about drawing a line? What if simply draw a line connecting the two means?</p>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>That’s exactly what linear regression does! It draws a line. This line, because the independent variable is binary, only goes from 0 (a group representing smaller cities) to 1 (a group representing larger cities). If we fit a linear regression model:</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="regression-ii-numerical-independent-variables.html#cb401-1" tabindex="-1"></a><span class="co"># fit a linear regression model</span></span>
<span id="cb401-2"><a href="regression-ii-numerical-independent-variables.html#cb401-2" tabindex="-1"></a><span class="fu">lm</span>(viol_r <span class="sc">~</span> largest50, <span class="at">data =</span> df)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = viol_r ~ largest50, data = df)
## 
## Coefficients:
## (Intercept)    largest50  
##       485.6        338.0</code></pre>
<p>We can see that the estimated Intercept is <span class="math inline">\(\widehat{\alpha}=485.6\)</span>. This is the expected score of the dependent variable <span class="math inline">\(Y\)</span> (i.e., violence rates) in the hypothetical scenario of <span class="math inline">\(x=0\)</span>—but it just so happens that, in this case, <span class="math inline">\(x=0\)</span> represents the group of smaller cities, so the intercept has a much more meaningful interpretation!</p>
<p>Similarly, we can see that the slope coefficient is <span class="math inline">\(\widehat{\beta} = 338\)</span>. As we learned today, we could simply interpret this number by stating that every unit increase in the variable <code>largest50</code> is associated with a 338-unit increase in violence rates. However, the variable <code>largest50</code> cannot increase by one unit in the conventional sense, as it is binary; or rather, it can <em>only</em> increase by one unit. The only possible change in this variable is between <span class="math inline">\(x = 0\)</span> (representing the group of smaller cities) and <span class="math inline">\(x = 1\)</span> (representing the group of larger cities)—it’s the line going from the mean when X equals 0 all the way to the mean when X equals 1. Therefore, the idea of a “one-unit increase” for a binary variable necessarily implies the <strong>difference</strong> between the estimated scores of the two groups. This is why we interpret the slope coefficient (<span class="math inline">\(\beta\)</span>) for binary variables as the mean difference—this is a special case of linear regression models!</p>
</div>
<div id="regression-assumptions" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Regression assumptions<a href="regression-ii-numerical-independent-variables.html#regression-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Like other models and statistical tests, the regression model also makes assumptions of its own. In fact, there are so many that we could spend an entire class discussing them. Gelman and Hill (2007) point out that the most important regression assumptions by decreasing order of importance are:</p>
<ul>
<li><strong>Validity</strong>. The data should be appropriate for the question that you are trying to answer:</li>
</ul>
<blockquote>
<p>“Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to all cases to which it will be applied… Data used in empirical research rarely meet all (if any) of these criteria precisely. However, keeping these goals in mind can help you be precise about the types of questions you can and cannot answer reliably.”</p>
</blockquote>
<ul>
<li><p><strong>Additivity and linearity</strong>. These are the most important mathematical assumptions of the model. You can include interaction effects in your models if the additivity assumption is violated. This is something we will discuss in more detail in future sessions. Similarly, we will discuss problems with non-linearities. If the relationship is non-linear (e.g., it is curvilinear), predicted values will be wrong in a biased manner, meaning that predicted values will systematically miss the true pattern of the mean of y (as related to the x-variables).</p></li>
<li><p><strong>Independence of errors</strong>. Regression assumes that the errors from the prediction line (or hyperplane for multiple regression) are independent. Suppose there is a dependency between the observations (you are assessing change across the same units, working with spatial units, or with units that are somehow grouped, such as students from the same class). In that case, you may have to use more appropriate models (e.g., multilevel models, spatial regression, etc.).</p></li>
<li><p><strong>Equal variances of errors</strong>. When the variance of the residuals is unequal, you may need different estimation methods. This is, nonetheless, considered a minor issue. There is a small effect on the validity of t-test and F-test results, but generally, regression inferences are robust with regard to the variance issue.</p></li>
<li><p><strong>Normality of errors</strong>. The residuals should be normally distributed. Gelman and Hill (2007: 46) discuss this as the least important of the assumptions and, in fact, “do <em>not</em> recommend diagnostics of the normality of the regression residuals”. If the errors do not have a normal distribution, it usually is not particularly serious. Regression inferences tend to be robust with respect to normality (or nonnormality of the errors). In practice, the residuals may appear to be nonnormal when the wrong regression equation has been used. So, we will show you how to inspect the normality of the residuals, not because this is a problem in itself, but because it may give you further evidence that there is some other problem with the model you are applying to your data.</p></li>
</ul>
<p>So, these are the assumptions of linear regression.</p>
<p>In this section, we can go through very quickly how to test some of them using visuals. While finding that some of the assumptions are violated does not necessarily mean that you have to scrap your model, it is important to use these diagnostics to illustrate that you have considered what the possible issues with your model are and if you find any serious issues that you address them.</p>
<p>In <code>R</code>, we can use the <code>plot()</code> function on our output <code>lm</code> object to look through some diagnostics. This gives us 4 plots, so to show them all, we’ll use the code <code>par(mfrow = c(2, 2))</code> to split our plot window into 4 panes (remember to set back, run <code>par(mfrow = c(1, 1))</code>). Let’s return to <code>fit_1</code>, our model.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="regression-ii-numerical-independent-variables.html#cb403-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb403-2"><a href="regression-ii-numerical-independent-variables.html#cb403-2" tabindex="-1"></a><span class="fu">plot</span>(fit_1)</span></code></pre></div>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>The 4 plots we get are:</p>
<ul>
<li><strong>Residuals vs Fitted</strong>. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns, is an indication of a linear relationship, which is good.</li>
<li><strong>Normal Q-Q</strong>. Used to examine whether the residuals are normally distributed. It’s good if residual points follow the straight dashed line.</li>
<li><strong>Scale-Location (or Spread-Location)</strong>. Used to check the homogeneity of variance of the residuals (homoscedasticity). A horizontal line with equally spread points is a good indication of homoscedasticity.</li>
<li><strong>Residuals vs Leverage</strong>. Used to identify influential cases, that is, extreme values that might influence the regression results when included or excluded from the analysis.</li>
</ul>
<p>In our example, we have issues with all four of them. This is probably due to the fact that our dependent variable is not normally distributed—check the histogram in the beginning of the section again—indicating that we probably have a non-linear relationship between unemployment and violence rates. We will revisit this in future sessions.</p>
<p>For the purposes of this module, it is enough that you understand that these assumptions of regression exist, what they mean, and how you might test for them. For some of them, for example, additivity, we will discuss some ways to address this in future weeks in sections focused on interaction effects. For others, it is just important to keep in mind when these might be violated and raise these as possible limitations in your ability to rely on the conclusions you draw from your results.</p>
</div>
<div id="lab-exercises-1" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Lab Exercises<a href="regression-ii-numerical-independent-variables.html#lab-exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Your turn!</strong> In the lab session, using the same data set, answer the following questions. Note that not all questions necessarily require analysing data in <code>R</code>. After you finish, click on ‘<em>Reveal answer!</em>’ to check your answers.</p>
<p>We start revisiting the <code>communitycrime</code> dataset (which we loaded in the beginning of the section) to select new variables:</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="regression-ii-numerical-independent-variables.html#cb404-1" tabindex="-1"></a><span class="co"># select new variables from the raw dataset</span></span>
<span id="cb404-2"><a href="regression-ii-numerical-independent-variables.html#cb404-2" tabindex="-1"></a>df_homework <span class="ot">&lt;-</span> communitycrime <span class="sc">%&gt;%</span></span>
<span id="cb404-3"><a href="regression-ii-numerical-independent-variables.html#cb404-3" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">2012</span>) <span class="sc">%&gt;%</span></span>
<span id="cb404-4"><a href="regression-ii-numerical-independent-variables.html#cb404-4" tabindex="-1"></a>  <span class="fu">select</span>(place_name, state_name, burglary_r, fborn, largest100)</span></code></pre></div>
<p>Using data from 264 cities in the United States in 2012, you want to assess whether the percentage or residents who are foreign born is associated with burglary rates. Your variables of interest are <code>burglary_r</code>, reflecting the burglary rate of the cities in the United States; <code>fborn</code>, indicating the percentage of foreign-born residents; and <code>largest100</code>, indicating whether the city is one of the 100 largest cities in the US.</p>
<ol style="list-style-type: decimal">
<li>Based on your research hypothesis, what are your dependent and independent variables?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>Dependent variable: burglary rates, i.e., <code>burglary_r</code>.</p>
<p>Independent variable: percentage of foreign-born population, i.e., <code>fborn</code>.</p>
</details>
<ol start="2" style="list-style-type: decimal">
<li>Is your dependent variable numerical or categorical? If categorical, is it binary, ordinal, or multinomial? What about the independent variable?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>The dependent variable, burglary rates (<code>burglary_r</code>), is a numerical variable.</p>
<p>The independent variable, percentage of foreign born (<code>fborn</code>), is also a numerical variable.</p>
</details>
<ol start="3" style="list-style-type: decimal">
<li>Using your dependent and independent variables, what is your null hypothesis?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>Null hypothesis: there is no association between the percentage of foreign-born residents and the burglary rate in cities in the United States.</p>
</details>
<ol start="4" style="list-style-type: decimal">
<li>Examine the relationship between your dependent and your independent variable using the appropriate visualisation strategies</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>We can produce a scatterplot to assess the relationship</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="regression-ii-numerical-independent-variables.html#cb405-1" tabindex="-1"></a><span class="co"># produce scatterplot</span></span>
<span id="cb405-2"><a href="regression-ii-numerical-independent-variables.html#cb405-2" tabindex="-1"></a><span class="fu">ggplot</span>(df_homework, <span class="fu">aes</span>(<span class="at">x =</span> fborn, <span class="at">y =</span> burglary_r)) <span class="sc">+</span> </span>
<span id="cb405-3"><a href="regression-ii-numerical-independent-variables.html#cb405-3" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Based on the scatterplot, we could expect a negative relationship: a higher concentration of foreign-born residents appears to be associated with fewer burglaries. We can check the regression line.</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="regression-ii-numerical-independent-variables.html#cb406-1" tabindex="-1"></a><span class="co"># produce scatterplot</span></span>
<span id="cb406-2"><a href="regression-ii-numerical-independent-variables.html#cb406-2" tabindex="-1"></a><span class="fu">ggplot</span>(df_homework, <span class="fu">aes</span>(<span class="at">x =</span> fborn, <span class="at">y =</span> burglary_r)) <span class="sc">+</span> </span>
<span id="cb406-3"><a href="regression-ii-numerical-independent-variables.html#cb406-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb406-4"><a href="regression-ii-numerical-independent-variables.html#cb406-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) </span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="06_regression_II_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</details>
<ol start="5" style="list-style-type: decimal">
<li>Let’s build a regression model. Replacing <span class="math inline">\(Y\)</span> with the name of your dependent variable, and <span class="math inline">\(X\)</span> with the name of your independent variable, write down the equation with the unknown parameters (i.e, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) that you want to estimate. <em>Note: this question does not involve any data analysis</em>.</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p><span class="math display">\[
burglary\_r = \alpha + \beta \cdot fborn
\]</span></p>
</details>
<ol start="6" style="list-style-type: decimal">
<li>Using the <code>lm()</code> function, estimate the parameters of your linear regression model. Rewrite the equation above replacing unknown parameters with the estimated parameters.</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="regression-ii-numerical-independent-variables.html#cb408-1" tabindex="-1"></a><span class="co"># fit linear regression model</span></span>
<span id="cb408-2"><a href="regression-ii-numerical-independent-variables.html#cb408-2" tabindex="-1"></a><span class="fu">lm</span>(burglary_r <span class="sc">~</span> fborn, <span class="at">data =</span> df_homework)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = burglary_r ~ fborn, data = df_homework)
## 
## Coefficients:
## (Intercept)        fborn  
##     1148.12       -14.67</code></pre>
<p>Based on the results of the linear regression model, we can rewrite the equation in the following way:</p>
<p><span class="math display">\[
burglary\_r = 1148.12 - 14.67 \cdot fborn
\]</span></p>
<p>This indicates that the expected level of burglary rates in a hypothetical city with 0% foreign-born residents would be 1148.12. Every one-unit increase in the percentage of foreign-born residents is associated with a decrease of 14.67 units of burglary rates.</p>
</details>
<ol start="7" style="list-style-type: decimal">
<li>Based on your regression model, what is the expected burglary rate in a city with 1% foreign-born residents? What about a city with 10%? And 50%?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<p>We can use the regression model to make those predictions. We can calculate manually or use <code>R</code> as a calculator.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="regression-ii-numerical-independent-variables.html#cb410-1" tabindex="-1"></a><span class="co"># fit regression model</span></span>
<span id="cb410-2"><a href="regression-ii-numerical-independent-variables.html#cb410-2" tabindex="-1"></a>regression_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(burglary_r <span class="sc">~</span> fborn, <span class="at">data =</span> df_homework)</span>
<span id="cb410-3"><a href="regression-ii-numerical-independent-variables.html#cb410-3" tabindex="-1"></a></span>
<span id="cb410-4"><a href="regression-ii-numerical-independent-variables.html#cb410-4" tabindex="-1"></a><span class="co"># predict burglary scores when foreign born is 1</span></span>
<span id="cb410-5"><a href="regression-ii-numerical-independent-variables.html#cb410-5" tabindex="-1"></a><span class="fu">predict</span>(regression_model, <span class="fu">data.frame</span>(<span class="at">fborn =</span> <span class="fu">c</span>(<span class="dv">1</span>))) </span></code></pre></div>
<pre><code>##        1 
## 1133.445</code></pre>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="regression-ii-numerical-independent-variables.html#cb412-1" tabindex="-1"></a><span class="co"># predict burglary scores when foreign born is 10</span></span>
<span id="cb412-2"><a href="regression-ii-numerical-independent-variables.html#cb412-2" tabindex="-1"></a><span class="fu">predict</span>(regression_model, <span class="fu">data.frame</span>(<span class="at">fborn =</span> <span class="fu">c</span>(<span class="dv">10</span>))) </span></code></pre></div>
<pre><code>##       1 
## 1001.37</code></pre>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="regression-ii-numerical-independent-variables.html#cb414-1" tabindex="-1"></a><span class="co"># predict burglary scores when foreign born is 50</span></span>
<span id="cb414-2"><a href="regression-ii-numerical-independent-variables.html#cb414-2" tabindex="-1"></a><span class="fu">predict</span>(regression_model, <span class="fu">data.frame</span>(<span class="at">fborn =</span> <span class="fu">c</span>(<span class="dv">50</span>))) </span></code></pre></div>
<pre><code>##        1 
## 414.3705</code></pre>
<p>The predicted burglary rate in a city with 1% of foreign-born residents is 1133.45. In a city with 10% of foreign-born residents, it is 1101.37. In a city with 50% foreign-born residents, it is 414.37.</p>
</details>
<ol start="8" style="list-style-type: decimal">
<li>What is the <span class="math inline">\(R^2\)</span> of your regression model?</li>
</ol>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="regression-ii-numerical-independent-variables.html#cb416-1" tabindex="-1"></a><span class="co"># extract R Squared</span></span>
<span id="cb416-2"><a href="regression-ii-numerical-independent-variables.html#cb416-2" tabindex="-1"></a><span class="fu">summary</span>(regression_model)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.1319607</code></pre>
<p>The <span class="math inline">\(R^2\)</span> is 0.1319, suggesting that 13.19% of the variance in burglary rates is explained by this regression model.</p>
</details


</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p><a href="http://link.springer.com/chapter/10.1007/978-1-4614-9170-5_15">This</a> is a fine chapter too if you struggle with the explanations in the required reading. Many universities, like the University of Manchester, have full access to Springer ebooks. You can also have a look at <a href="http://people.stern.nyu.edu/wgreene/Statistics/MultipleRegressionBasicsCollection.pdf">these notes</a>.<a href="regression-ii-numerical-independent-variables.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p><a href="http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit">This</a> is a reasonable explanation of how to interpret R-Squared.<a href="regression-ii-numerical-independent-variables.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-i-mean-differences.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-iii.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

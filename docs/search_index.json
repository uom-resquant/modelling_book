[["index.html", "Modelling Criminological Data CRIM20452 Preface", " Modelling Criminological Data CRIM20452 2025-03-02 Preface This study material is designed to introduce Criminology students at the University of Manchester to the use of data science in crime research and practice. This is an improved version of the material originally developed by Juanjo Medina and Reka Solymosi, and is currently being updated and maintained by Thiago Oliveira - thiago.oliveira@manchester.ac.uk and Ana Nicoriciu - anamaria.nicoriciu@manchester.ac.uk. This lab note is a work in process. If you have any suggestions or find any errors, please don’t hesitate to contact us by submitting an issue to the GitHub repository. We appreciate your feedback and will use it to improve the material for future students. Note for students * While this material briefly covers some concepts, students are expected to engage in weekly reading, attend lab sessions, and participate in lectures for a comprehensive course experience. These notes are not intended to be a stand-alone reference or textbook but rather a set of exercises to gain hands-on practice with the concepts introduced during the course. * This material is designed for Criminology students at the University of Manchester. They are meant to introduce students to the concept of descriptive statistics and the key concepts required to build an understanding of quantitative data analysis in crime research. * The handouts utilise various datasets, including data from the UK data service, such as the Crime Survey for England and Wales, which is available under an Open Government Licence. These datasets are designed to be a learning resource and should not be used for research purposes or the production of summary statistics. "],["a-first-lesson-about-r.html", "Chapter 1 A first lesson about R 1.1 Install R &amp; RStudio 1.2 Open up and explore RStudio 1.3 Customising the RStudio look 1.4 Getting organised: R Projects 1.5 Functions: Talk to your computer 1.6 More on packages 1.7 Objects: creating an object 1.8 More on objects 1.9 Naming conventions for objects in R 1.10 R object types: vectors 1.11 R object types: Data frame 1.12 Exploring data 1.13 R data types: Factors 1.14 How to import data 1.15 How to use ‘comment’ 1.16 How to Quit RStudio 1.17 Summary", " Chapter 1 A first lesson about R 1.1 Install R &amp; RStudio We recommend using your laptops for this course. If you have not already, then please download and install R and RStudio onto your laptops. - click here for instructions using Windows or - here for instructions using a Mac. If you are using a Mac it would be convenient that you use the most up-to-date version of OS or at least one compatible with the most recent version of R. Read this if you want to check how to do that. If you prefer, you can always use any of the PCs in the computer cluster. All of them already have the software installed. 1.2 Open up and explore RStudio In this session, we will focus on developing basic familiarity with R Studio. You can use R without using R Studio, but R Studio is an app that makes it easier to work with R. R Studio automatically runs R in the background. We will be interacting with R via R Studio in this course unit. When you first open R Studio, you will see (as in the image above) that there are 3 main panes. The bigger one to your left is the console. If you read the text in the console, you will see that R Studio is opening R and can see what version of R you are running. Depending on whether you are using the cluster machines or your own installation, this may vary, but don’t worry too much about it. The view in R Studio is structured so that you have 4 open panes in a regular session. Click on the File drop-down menu, select New File, then R Script. You will now see the 4 window areas on display. You can shift between different views and panels in each of these areas. You can also use your mouse to re-size the different windows if that is convenient. Look, for example, at the bottom right area. Within this area you can see that there are different tabs, which are associated with different views. You can see in the tabs in this section that there are different views available: Files, Plots, Packages, Help, and Viewer. The Files allow you to see the files in the physical directory that is currently set up as your working environment. You can think of it like a window in Windows Explorer that lets you see the content of a folder. In the plots panel, you will see any data visualisations or graphical displays of data that you produce. We haven’t yet produced any, so it is empty at the moment. If you click on packages, you will see the packages that are currently available in your installation. What is a “package” in this context? Packages are modules that expand what R can do. There are thousands of them. A few come pre-installed when you do a basic R installation. Others you pick and install yourself. This term, we will introduce some important packages we recommend you install when prompted. The other really useful panel in this part of the screen is the Help viewer. Here, you can access the documentation for the various packages that make up R. Learning how to use this documentation will be essential if you want to get the most from R. In the diagonally opposite corner, the top left, you should now have an open script window. The script is where you write your programming code - the instructions you send to your computer. A script is nothing but a text file that you can write in. Unlike other programs for data analysis you may have used in the past (Excel, SPSS), you need to interact with R by writing down instructions and asking R to evaluate those instructions. R is an interpreted programming language: you write instructions (code) that the R engine has to interpret in order to do something. All the instructions we write can and should be saved in a script, which you can later return to and continue working on. One of the key advantages of doing data analysis this way is that you are producing a written record of every step you take in the analysis. The challenge, though, is that you need to learn this language in order to be able to use it. That will be the main focus of this course, teaching you to write R code for data analysis purposes. As with any language, the more you practice it, the easier it will become. More often than not, you will be cutting and pasting chunks of code we will give you. But we will also expect you to develop a basic understanding of what these bits of code do. It is a bit like cooking. At first, you will just follow recipes as they are given to you, but as you become more comfortable in your “kitchen”, you will feel more comfortable experimenting. The advantage of doing analysis this way is that once you have written your instructions and saved them in a file, you will be able to share them with others and run them every time you want in a matter of seconds. This creates a reproducible record of your analysis: something that your collaborators or someone else anywhere (including your future self, the one that will have forgotten how to do the stuff) could run and get the same results as you did at some point earlier. This makes science more transparent, and transparency brings many advantages. For example, it makes your research more trustworthy. Don’t underestimate how critical this is. Reproducibility is becoming a key criterion for assessing good quality research. And tools like R allow us to enhance it. You may want to read more about reproducible research here. 1.3 Customising the RStudio look RStudio allows you to customise the way it looks. For example, working with white backgrounds is not generally a good idea if you care about your eyesight. If you don’t want to end up with dry eyes, not only is it good for you to follow the 20-20-20 rule (every 20 minutes, look for 20 seconds to an object located 20 feet away from you), but it may also be a good idea to use more eye-friendly screen displays. Click in the Tools menu and select Global options. This will open up a pop-up window with various options. Select Appearance. In this section, you can change the font type and size, as well as the kind of theme background that R will use in the various windows. I suffer from poor sight, so I often increase the font type. I also use the Tomorrow Night Bright theme to prevent my eyes from going too dry from the effort of reading a lightened screen, but you may prefer a different one. You can preview them and then click apply to select the one you like. This will not change your results or analysis. This is just something you may want to do in order to make things look better and healthier for you. 1.4 Getting organised: R Projects Whenever you do analysis, you will be working with a variety of files. You may have an Excel data set (or some other type of data set file, like CSV, for example), a Microsoft Word file where you are writing down the essay with your results, but also a script with all the programming code you have been using. R needs to know where all these files are on your computer. Often you will get error messages because you are expecting R to find one of these files in the wrong location. It is absolutely critical that you understand how your computer organises and stores files. Please watch the video below to understand the basics of file management and file paths: Windows users MAC users The best way to avoid problems with file management in R is using what RStudio calls R Projects. Technically, a RStudio project is just a directory (a folder) with the name of the project and a few files and folders created by R Studio for internal purposes. This is where you should hold your scripts, your data, and your reports. You can manage this folder with your own operating system manager (e.g., Windows Explorer) or through the R Studio file manager (which you access in the bottom right corner of the Windows set in R Studio). When a project is reopened, R Studio opens every file and data view that was open when the project was closed last time around. Let’s learn how to create a project. Go to the File drown menu and select New Project. That will open a dialogue box where you ask to specify what kind of directory you want to create. Select a new working directory in this dialogue box. Now, you get another dialogue box where you have to specify what kind of project you want to create. Select the first option New Project. Finally, you get to select a name for your project (in the image below, I use the code for this course unit, but you can use any sensible name you prefer), and you will need to specify the folder/directory in which to place this directory. If you are using a cluster machine, use the P: drive; otherwise, select what you prefer on your laptop (preferably a folder that you created specifically for this course and to avoid problems later, not your desktop). With simple projects, a single script file and a data file are all you may have. But with more complex projects, things can rapidly become messy. So, you may want to create subdirectories within this project folder. I typically use the following structure in my own work to put all files of a certain type in the same subdirectory: Scripts and code: Here, I put all the text files with my analytic code, including Rmarkdown files, which is something we will introduce much later in the semester. Source data: Here, I put the original data. I tend not to touch this once I have obtained the original data. Documentation: This is the subdirectory where I place all the data documentation (e.g., codebooks, questionnaires, etc.) Modified data: All analyses involve transformations and changes to the original data files. You don’t want to mess up the original data files, so you should create new data files as soon as you start changing your source data. I go so far as to place them in a different subdirectory. Literature: Analysis is all about answering research questions. There is always a literature about these questions. I place the relevant literature for the analytic project I am conducting in this subdirectory. Reports and write-up: Here is where I file all the reports and data visualisations that are associated with my analysis. You can create these subdirectories using Windows Explorer or the Files window in R Studio. 1.5 Functions: Talk to your computer So far, we have covered an introduction to the main interface you will be using and talked about RStudio projects. In this unit, you will be using this interface and creating files within your RStudio projects to produce analysis based on programming code that you will need to write using the R language. Let’s write some very simple code using R to talk to your computer. First, open a new SCRIPT within the project you just created. Type the following instructions in the script window. After you are done, click in the top right corner where it says Run (if you prefer quick shortcuts, you can select the text and then press Ctrl + Enter): print(&quot;I love stats&quot;) ## [1] &quot;I love stats&quot; Congratulations!!! You just run your first line of R code! 👏👏 In these handouts, you will see grey boxes with bits of code. You can cut and paste this code into your script window and run the code from it to reproduce our results. As we go along, we will be covering new bits of code. Sometimes, in these lab notes, you will see the results of running the code, which is what you see printed in your console or in your plot viewer. The results will appear enclosed in a box as above. The R language uses functions to tell the computer what to do. In the R language functions are the verbs. You can think of functions as predefined commands that somebody has already programmed into R and tell R what to do. Here, you learnt your first R function: print. All this function does is ask R to print whatever you want in the main console (see the window in the bottom left corner). In R, you can pass a number of arguments to any function. These arguments control what the function will do in each case. The arguments appear between brackets. Here, we passed the text “I love stats” as an argument. Once you execute the program by clicking on Run, the R engine sends this to your machine’s CPU in the form of binary code, and this produces a result. In this case, we see that the result is printed on the main console. Every R function admits different kinds of arguments. Learning R involves not only learning different functions but also learning the valid arguments you can pass to each function. As indicated above, the window in the bottom left corner is the main console. You will see that the words “I love stats” appear printed there. If, rather than using R Studio, you were working directly from R, that’s all you would get: the main console where you can write code interactively (rather than all the different windows you see in R Studio). You can write your code directly in the main console and execute it line by line in an interactive fashion. However, we will be running code from scripts so that you get used to the idea of properly documenting all the steps you take. 1.6 More on packages Before, we described packages as elements that add the functionality of R. Most packages introduce new functions that allow you to ask R to do different things. Anybody can write a package, so consequently, R packages vary in quality and complexity. You can find packages in different places, as well, from official repositories (which means they have passed a minimum of quality control), something called GitHub (a webpage where software developers post work in progress), to personal webpages (danger, danger!). In early 2017 we passed the 10,000 mark just in the main official repository, so the number of things that can be done with R grows exponentially every day as people keep adding new packages. When you install R, you only install a set of basic packages, not the full 10,000 plus. So, if you want to use any of these added packages that are not part of the basic installation, you first need to install them. You can see what packages are available for your local installation by looking at the packages tab in the bottom right corner panel. Click there and check. We are going to install a package that is not there so that you can see how the installation is done. If you just installed R on your laptop, you will see a shortish list of packages that constitute the basic installation of R. If you are using one of the machines in the computer cluster, this list is a bit longer because we asked IT to install some of the most commonly used packages. But knowing how to install packages is essential since you will want to do it very often. We will install a package called “cowsay” to demonstrate the process. In the Packages panel, there is an Install menu that opens a dialogue box and allows you to install packages. Instead, we are going to use code to do this. Just cut and paste the code below into your script and then run it: install.packages(&quot;cowsay&quot;) Here, we are introducing a new function, “install.packages” and what we have passed as an argument is the name of the package that we want to install. This is how we install a package that is available in the official CRAN repository. Given that you are connecting to an online repository, you will need an internet connection every time you want to install a package. CRAN is an official repository that has a collection of R packages that meet a minimum set of quality criteria. It’s a fairly safe place to get packages from. If we wanted to install a package from somewhere else, we would have to adapt the code. Later this semester, you will see how we install packages from GitHub. This line of code (as it is currently written) will install this package in a personal library that will be located in your P: drive if you are using a cluster machine. If you are using a Windows machine this code will place this package within a personal library in your Documents folder. Once you install a package, it will remain in the machine/location where you installed it until you physically delete it. How do you find out what a package does? You look at the relevant documentation. In the Packages window, scroll down until you find the new package we installed listed. Here, you will see the name of the package (cowsay), a brief description of what the program is about, and the version you have installed (an indication that a package is a good package is that it has gone through several versions, that means that someone is making sure the package gets regular updates and improvements). The version I have for cowsay is 0.7.0. Yours may be older or newer. It doesn’t matter much at this point. Click in the name cowsay. You will see that R Studio has now brought you to the Help tab. Here is where you find the help files for this package, including all the available documentation. Every beginner in R will find these help files a bit confusing. But after a while, their format and structure will begin to make sense to you. Click where it says User guides, package vignettes, and other documentation. Documentation in R has become much better since people started to write vignettes for their packages. They are little tutorials that explain with examples what each package does. Click on the cowsay::cowsay_tutorial that you see listed here. You will find an HTML file that gives you a detailed tutorial on this package. You don’t need to read it now, but remember that this is one way to find help when using R. Let’s try to use some of the functions of this package. We will use the “say” function: say(&quot;I love stats&quot;) You will get an error message telling you that this function could not be found. What happened?? This will be the first of many error messages you will get. An error message is the computer’s way of telling you that your instructions are somehow incomplete or problematic and, thus, are unable to do what you ask. It is frustrating to get these messages, and a critical skill for you this semester will be to overcome that frustration and try to understand why the computer cannot do what you ask. These labs are all about finding out the source of the error and solving it. There is nothing wrong with getting errors. The problem is if you give up and let your frustration get the best of you. So why are we getting this error? Installing a package is only the first step. The next step, when you want to use it in a given session, is to load it. Think of it as a pair of shoes. You buy them once, but you have to take them from your closet and put them on when you want to use them. Same with packages, you only install once, but need to load it from your library every time you want to use it -within a given session (once loaded, it will remain loaded until you finish your session). To see what packages you currently have loaded in your session, you use the search() function (you do not need to pass it any arguments in this case). search() ## [1] &quot;.GlobalEnv&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [4] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ## [7] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; If you run this code, you will see that cowsay is not in the list of loaded packages. Therefore, your computer cannot use any of the functions associated with it until you load it. To load a package, we use the library function. So, if we want to load the new package we installed on our machine, we would need to use the following code: library(&quot;cowsay&quot;) Run the search function again. You will see this package is listed now. So now we can try using the function “say” again. say(&quot;I love stats&quot;) ## ## -------------- ## I love stats ## -------------- ## \\ ## \\ ## \\ ## |\\___/| ## ==) ^Y^ (== ## \\ ^ / ## )=*=( ## / \\ ## | | ## /| | | |\\ ## \\| | |_|/\\ ## jgs //_// ___/ ## \\_) ## You get a random animal in the console repeating the text we passed as an argument. If we like a different animal, we could pass a new argument on to the “say” function. So, if we want to have a cow rather than a random animal, then we would pass the following arguments on to our function. say(&quot;I love stats&quot;, &quot;cow&quot;) ## ## ----- ## I love stats ## ------ ## \\ ^__^ ## \\ (oo)\\ ________ ## (__)\\ )\\ /\\ ## ||------w| ## || || This is an important feature of arguments in functions. We said how different functions admit different arguments. Here, by specifying cow, the function prints that particular animal. But why is it that when we didn’t specify a particular kind of animal, we still got a result? That happened because functions always have default arguments that are necessary for them to run and that you do not have to make explicit. Default arguments are implicit and do not have to be typed. The say function has a default argument, random, which will print a random character or animal. It is only when you want to change the default that you need to make an alternative animal explicit. Remember, you only have to install a package that has not been installed ONCE. But if you want to use it in a given session, you will have to load it within that session using the library function. Once you load it within a session, the package will remain loaded until you terminate your session (for example, by closing R Studio). Do not forget this. 1.7 Objects: creating an object We have seen how the first argument that the “say” function takes is the text that we want to convert into speech for our given animal. We could write the text directly into the function (as we did above), but now we are going to do something different. We are going to create an object to store the text. An object? What do I mean? In the same way that everything you do in R you do with functions (your verbs), everything that exists in R is an object. You can think of objects as boxes where you put stuff. In this case, we are going to create an object called my_text and inside this object, we are going to store the text “I love stats”. How do you do this? We will use the code below: my_text &lt;- &quot;I love stats.&quot; This bit of code is simply telling R we are creating a new object with the assigned name (“my_text”). We are creating a box with such a name, and inside this box, we are placing a bit of text (“I love stats”). The arrow &lt;- you see is the assignment operator. This is an important part of the R language that tells R what we are including inside the object in question. Run the code. Look now at the Environment window in the top right corner. We see that this object is now listed there. You can think of the Environment as a warehouse where you put stuff - your different objects. Is there a limit to this environment? Yes, your RAM. R works on your RAM, so you need to be aware that if you use very large objects, you will need loads of RAM. But that won’t be a problem you will encounter in this course unit. Once we put things into these boxes or objects, we can use them as arguments for our functions. See the example below: say(my_text, &quot;cow&quot;) ## ## ----- ## I love stats. ## ------ ## \\ ^__^ ## \\ (oo)\\ ________ ## (__)\\ )\\ /\\ ## ||------w| ## || || 1.8 More on objects Now that we have covered some of the preliminaries, we can move on to the data. In Excel, you are used to seeing your data in spreadsheet format. If you need some recap, you should review some of the materials from previous modules. This chapter will be helpful to have a better understanding of the notion of a data set, levels of measurement, and tidy data. R is considerably more flexible than Excel. Most of the work we do here will use data sets or data frames as they are called in R. But as you have seen earlier, you can have objects other than data frames in R. These objects can relate to external files or simple textual information (“I love stats”). This flexibility is a big asset because, among other things, it allows us to break down data frames or the results from doing analysis on them to their constitutive parts (this will become clearer as we go along). Technically, R is an Object Oriented language. Object-oriented programming (OOP) is a programming language model organized around objects rather than “actions” and data rather than logic. As we have seen earlier, to create an object, you have to give it a name and then use the assignment operator (the &lt;- symbol) to assign it some value. For example, if we want to create an object that we name “x”, and we want it to represent the value of 5, we write: x &lt;- 5 We are simply telling R to create a numeric object, called x, with one element (5) or of length 1. It is numeric because we are putting a number inside this object. The length is 1 because it only has one element on it, the number 5. You can see the content of the object x in the main console either by using the print function we used earlier or by auto-printing, that is, just typing the name of the object and running that as code: x ## [1] 5 When writing expressions in R, you must understand that R is case sensitive. This could drive you nuts if you are not careful. More often than not, if you write an expression asking R to do something and R returns an error message, chances are that you used lowercase when uppercase was needed (or vice versa). So, always check for the right spelling. For example, see what happens if I use a capital ‘X’: X ## Error in eval(expr, envir, enclos): object &#39;X&#39; not found You will get the following message: \"Error in eval(expr, envir, enclos): object 'X' not found\". R is telling us that X does not exist. There isn’t an object X (upper case), but there is an object x (lower case). Error messages in R are pretty good at telling you exactly what went wrong. Remember, computers are very literal. They are like dogs. You can tell a dog to “sit”, and if it has been trained, it will sit. But if you tell a dog, “Would you be so kind as to relax a bit and lay down on the sofa?” it won’t have a clue what you are saying and will stare at you like you have gone mad. Error messages are computers’ ways of telling us, “I really want to help you, but I don’t really understand what you mean” (never take them personally; computers don’t hate you). When you get an error message or implausible results, you want to look back at your code to figure out what the problem is. This process is called debugging. There are some proper systematic ways to write code that facilitate debugging, but we won’t get into that here. R is very good with automatic error handling at the levels we’ll be using it at. Very often, the solution will simply involve correcting the spelling. A handy tip is to cut and paste the error message into Google and find a solution. If anybody had given me a penny for every time I had to do that myself, I would be Bill Gates by now. You’re probably not the first person to make your mistake, after all, and someone on the internet has surely already found a solution to your issue. People make mistakes all the time. It’s how we learn. Don’t get frustrated, don’t get stuck. Instead, look for a solution. These days, we have Google. We didn’t back in the day. Now, you have the answer to your frustration within quick reach. Use it to your advantage. 1.9 Naming conventions for objects in R You may have noticed the various names I have used to designate objects (my_1st_vector, the_smiths, etc.). You can use almost any names you want for your objects. Objects in R can have names of any length consisting of letters, numbers, underscores (“_“) or the period (”.”) and should begin with a letter. In addition, when naming objects, you need to remember: Some names are forbidden. These include words such as FALSE and TRUE, logical operators, and programming words like Inf, for, else, break, function, and words for special entities like NA and NaN. You want to use names that do not correspond to a specific function. We have seen, for example, that there is a function called print(); you don’t want to call an object “print” to avoid conflicts. To avoid this, use nouns instead of verbs when naming your variables and data. You don’t want them to be too long (or you will regret it every time you need to use that object in your analysis: your fingers will bleed from typing). You want to make them as intuitive to interpret as possible. You want to follow consistent naming conventions. R users are terrible about this. However, we could make it better if we all aim to follow similar conventions. In these handouts, you will see that I follow the underscore_separated convention. See here for details. It is also important to remember that R will always treat numbers as numbers. This sounds straightforward, but actually, it is important to note. We can name our variables almost anything. EXCEPT they cannot be numbers. Numbers are protected by R. 1 will always mean 1. If you want, give it a try. Try to create a variable called 12 and assign it the value “twelve”. As we did in the sections above, we can assign something meaning by using the “&lt;-” characters. 12 &lt;- &quot;twelve&quot; ## Error in 12 &lt;- &quot;twelve&quot;: invalid (do_set) left-hand side to assignment You get an error! 1.10 R object types: vectors In R, there are different kinds of objects. We will start with vectors. What is a vector? A vector is simply a set of elements of the same class (typically, these classes are character, numeric, integer, or logical -as in True/False). Vectors are the basic data structure in R. Typically, you will use the c() function (c stands for concatenate) to create vectors. The code below exemplifies how to create vectors of different classes (numeric, logical, character, etc.). Notice how the listed elements (to simplify, there are two elements in each vector below) are separated by commas ,: my_1st_vector &lt;- c(0.5, 0.6) #creates a numeric vector with two elements my_2nd_vector &lt;- c(1L, 2L) #creates an integer vector (&quot;L&quot; suffix specifies an integer type) my_3rd_vector &lt;- c(TRUE, FALSE) #creates a logical vector my_4th_vector &lt;- c(T, F) #creates a logical vector using abbreviations of True and False, #but you should avoid this formulation and instead use the full word. my_5th_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) #creates a character vector my_6th_vector &lt;- c(1+0i, 2+4i) #creates a complex vector (we won&#39;t really use in this class) Cut and paste this code into your script and run it. You will see how all these vectors are added to your global environment and stored there. The beauty of an object-oriented statistical language like R is that once you have these objects, you can use them as inputs in functions, use them in operations, or create other objects. This makes R very flexible. See some examples below: class(my_1st_vector) #a function to figure out the class of the vector ## [1] &quot;numeric&quot; length(my_1st_vector) #a function to figure out the length of the vector ## [1] 2 my_1st_vector + 2 #Add a constant to each element of the vector ## [1] 2.5 2.6 my_7th_vector &lt;- my_1st_vector + 1 #Create a new vector that contains #the elements of my1stvector plus a constant of 1 my_1st_vector + my_7th_vector #Adds the two vectors and Auto-print ## [1] 2.0 2.2 #the results (note how the sum was done) As indicated earlier, when you create objects, you place them in your working memory or workspace. Each R session will be associated with a workspace (called “global environment” in R Studio). In R Studio you can visualise the objects you have created during a session in the Global Environment screen. But if you want to produce a list of what’s there, you can use the ls() function (the results you get may differ from the ones below depending on what you actually have in your global environment). ls() #list all objects in your global environment ## [1] &quot;my_1st_vector&quot; &quot;my_2nd_vector&quot; &quot;my_3rd_vector&quot; &quot;my_4th_vector&quot; ## [5] &quot;my_5th_vector&quot; &quot;my_6th_vector&quot; &quot;my_7th_vector&quot; &quot;my_text&quot; ## [9] &quot;x&quot; If you want to delete a particular object, you can do so using the rm() function. rm(x) #remove x from your global environment It is also possible to remove all objects at once: rm(list = ls()) #remove all objects from your global environment If you mix in vector elements of a different class (for example, numerical and logical), R will coerce to the minimum common denominator so that every element in the vector is of the same class. So, for example, if you input a number and a character, it will coerce the vector to be a character vector - see the example below and notice the use of the class() function to identify the class of an object. my_8th_vector &lt;- c(0.5, &quot;a&quot;) class(my_8th_vector) #The class() function will tell us the class of the vector ## [1] &quot;character&quot; 1.11 R object types: Data frame Ok, so now that you understand some of the basic types of objects you can use in R, let’s start talking about data frames. One of the most common objects you will work with, in this course, are data frames. Data frames can be created with the data.frame() function. Data frames are multiple vectors of possibly different classes (e.g., numeric, factors, character) but of the same length (e.g., all vectors or variables have the same number of rows). This may sound a bit too technical, but it is simply a way of saying that a data frame is what in other programs for data analysis gets represented as data sets, the tabular spreadsheets you have seen when using Excel. Let’s create a data frame with two variables: #We create a data frame called mydata_1 with two variables, #an integer vector called foo and a logical vector called bar mydata_1 &lt;- data.frame(foo = 1:4, bar = c(T,T,F,F)) mydata_1 ## foo bar ## 1 1 TRUE ## 2 2 TRUE ## 3 3 FALSE ## 4 4 FALSE Or alternatively, for the same result: x &lt;- 1:4 y &lt;- c(T, T, F, F) mydata_2 &lt;- data.frame (foo = x, bar = y) mydata_2 ## foo bar ## 1 1 TRUE ## 2 2 TRUE ## 3 3 FALSE ## 4 4 FALSE As you can see in R, as in any other language, there are multiple ways of saying the same thing. Programmers aim to produce code that has been optimised: it is short and quick. It is likely that as you develop your R skills, you find increasingly more efficient ways of asking R how to do things. What this means, too, is that when you go for help from your peers or us, we may teach you slightly different ways of getting the right result. As long as you get the right result, that’s what matters at this point. These are silly toy examples of data frames. In this course, we will use real data. Next week, we will learn in greater detail how to read data into R. But you should also know that R comes with pre-installed data sets. Some packages, in fact, are nothing but collections of data frames. Let’s have a look at some of them. We are going to look at some data that are part of the fivethirtyeight package. This package contains data sets and codes behind the stories from various online news articles. This package is not part of the base installation of R, so you will need to install it first. I won’t give you the code for it. See if you can figure it out by looking at previous examples. Done? Ok, now we are going to look at the data sets that are included in this package. Remember, first, we have to install and load the package if we want to use it: library(fivethirtyeight) #This function will return all the data frames that #are available in the named package. data(package=&quot;fivethirtyeight&quot;) Notice that this package has some data sets that relate to stories covered in this journal that had a criminological angle. Let’s look, for example, at the hate_crimes data set. How do you do that? First, we have to load the data frame into our global environment. To do so, use the following code: data(&quot;hate_crimes&quot;) This function will search among all the loaded packages and locate the “hate_crimes” data set. Every object in R can have attributes. These are names; dimensions (for matrices and arrays: number of rows and columns) and dimensions names; class of object (numeric, character, etc.); length (for a vector, this will be the number of elements in the vector); and other user-defined. You can access the attributes of an object using the attributes() function. Let’s query R for the attributes of this data frame. attributes(hate_crimes) ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## [51] 51 ## ## $class ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## ## $names ## [1] &quot;state&quot; &quot;state_abbrev&quot; ## [3] &quot;median_house_inc&quot; &quot;share_unemp_seas&quot; ## [5] &quot;share_pop_metro&quot; &quot;share_pop_hs&quot; ## [7] &quot;share_non_citizen&quot; &quot;share_white_poverty&quot; ## [9] &quot;gini_index&quot; &quot;share_non_white&quot; ## [11] &quot;share_vote_trump&quot; &quot;hate_crimes_per_100k_splc&quot; ## [13] &quot;avg_hatecrimes_per_100k_fbi&quot; These results printed in my console may not make too much sense to you at this point. We will return to this next week, so do not worry. Go now to the global environment panel and left-click on the data frame “hate_crimes”. This will open the data viewer in the top left section of R Studio. What you get there is a spreadsheet with 12 variables and 51 observations. Each variable, in this case, provides you with information (demographics, voting patterns, and hate crime) about each of the US states. 1.12 Exploring data Ok, let’s now have a quick look at the data. There are so many different ways of producing summary stats for data stored in R that it is impossible to cover them all! We will just introduce a few functions that you may find useful for summarising data. Before we do any of that, it is important you get a sense of what is available in this data set. Go to the help tab, and in the search box, input the name of the data frame; this will take you to the documentation for this data frame. Here, you can see a list of the available variables. Let’s start with the mean. This function takes as an argument the numeric variable for which you want to obtain the mean. Because of the way that R works, you cannot simply put the name of the variable; you have to tell R as well which data frame that variable is located in. To do that, you write the name of the data frame, the dollar sign($), and then the name of the variable you want to summarise. If you want to obtain the mean of the variable that gives us the proportion of people who voted for Donald Trump, you can use the following expression: mean(hate_crimes$share_vote_trump) ## [1] 0.49 This code is saying to look inside the “hate_crimes” dataset object and find the “share_vote_trump” variable, then print the mean. The $ is used when you want to find a particular component of an object. In the case of data frames, that component will typically be one of the vectors (variables). However, we will see other uses for other kinds of objects as we move through the course. Another function you may want to use with numeric variables is summary(): summary(hate_crimes$share_vote_trump) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.040 0.415 0.490 0.490 0.575 0.700 This gives you the five-number summary (minimum, first quartile, median, third quartile, and maximum, plus the mean and the count of missing values if there are any). You don’t have to specify a variable; you can ask for these summaries from the whole data frame: summary(hate_crimes) ## state state_abbrev median_house_inc share_unemp_seas ## Length:51 Length:51 Min. :35521 Min. :0.02800 ## Class :character Class :character 1st Qu.:48657 1st Qu.:0.04200 ## Mode :character Mode :character Median :54916 Median :0.05100 ## Mean :55224 Mean :0.04957 ## 3rd Qu.:60719 3rd Qu.:0.05750 ## Max. :76165 Max. :0.07300 ## ## share_pop_metro share_pop_hs share_non_citizen share_white_poverty ## Min. :0.3100 Min. :0.7990 Min. :0.01000 Min. :0.04000 ## 1st Qu.:0.6300 1st Qu.:0.8405 1st Qu.:0.03000 1st Qu.:0.07500 ## Median :0.7900 Median :0.8740 Median :0.04500 Median :0.09000 ## Mean :0.7502 Mean :0.8691 Mean :0.05458 Mean :0.09176 ## 3rd Qu.:0.8950 3rd Qu.:0.8980 3rd Qu.:0.08000 3rd Qu.:0.10000 ## Max. :1.0000 Max. :0.9180 Max. :0.13000 Max. :0.17000 ## NA&#39;s :3 ## gini_index share_non_white share_vote_trump hate_crimes_per_100k_splc ## Min. :0.4190 Min. :0.0600 Min. :0.040 Min. :0.06745 ## 1st Qu.:0.4400 1st Qu.:0.1950 1st Qu.:0.415 1st Qu.:0.14271 ## Median :0.4540 Median :0.2800 Median :0.490 Median :0.22620 ## Mean :0.4538 Mean :0.3157 Mean :0.490 Mean :0.30409 ## 3rd Qu.:0.4665 3rd Qu.:0.4200 3rd Qu.:0.575 3rd Qu.:0.35694 ## Max. :0.5320 Max. :0.8100 Max. :0.700 Max. :1.52230 ## NA&#39;s :4 ## avg_hatecrimes_per_100k_fbi ## Min. : 0.2669 ## 1st Qu.: 1.2931 ## Median : 1.9871 ## Mean : 2.3676 ## 3rd Qu.: 3.1843 ## Max. :10.9535 ## NA&#39;s :1 So you see how now we are getting this info for all variables in one go. There are multiple ways of getting results in R. Particularly for basic and intermediate-level statistical analysis, many core functions and packages can give you the answer that you are looking for. For example, there are a variety of packages that allow you to look at summary statistics using functions defined within those packages. You will need to install these packages before you can use them. I am only going to introduce one of them here skimr. It is neat and is maintained by the criminologist Elin Waring, an example of kindness and dedication to her students. You will need to install it before anything else. Use the code you have learnt to do so and then load it. I won’t be providing you the code for it; by now you should know how to do this. Once you have loaded the skimr package, you can use it. Its main function is skim. Like summary for data frames, skim presents results for all the columns, and the statistics will depend on the class of the variable. However, the results are displayed and stored in a nicer way - though we won’t get into the details right now. skim(hate_crimes) skim_type skim_variable n_missing complete_rate character.min character.max character.empty character.n_unique character.whitespace numeric.mean numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 numeric.hist character state 0 1.0000000 4 20 0 51 0 NA NA NA NA NA NA NA NA character state_abbrev 0 1.0000000 2 2 0 51 0 NA NA NA NA NA NA NA NA numeric median_house_inc 0 1.0000000 NA NA NA NA NA 5.522361e+04 9208.4781698 3.552100e+04 4.865700e+04 5.491600e+04 6.071900e+04 76165.000000 ▂▆▇▅▂ numeric share_unemp_seas 0 1.0000000 NA NA NA NA NA 4.956860e-02 0.0106981 2.800000e-02 4.200000e-02 5.100000e-02 5.750000e-02 0.073000 ▅▇▇▇▂ numeric share_pop_metro 0 1.0000000 NA NA NA NA NA 7.501961e-01 0.1815873 3.100000e-01 6.300000e-01 7.900000e-01 8.950000e-01 1.000000 ▁▂▅▆▇ numeric share_pop_hs 0 1.0000000 NA NA NA NA NA 8.691176e-01 0.0340732 7.990000e-01 8.405000e-01 8.740000e-01 8.980000e-01 0.918000 ▃▅▃▆▇ numeric share_non_citizen 3 0.9411765 NA NA NA NA NA 5.458330e-02 0.0310770 1.000000e-02 3.000000e-02 4.500000e-02 8.000000e-02 0.130000 ▇▆▆▂▂ numeric share_white_poverty 0 1.0000000 NA NA NA NA NA 9.176470e-02 0.0247148 4.000000e-02 7.500000e-02 9.000000e-02 1.000000e-01 0.170000 ▂▇▃▂▁ numeric gini_index 0 1.0000000 NA NA NA NA NA 4.537647e-01 0.0208908 4.190000e-01 4.400000e-01 4.540000e-01 4.665000e-01 0.532000 ▆▇▅▁▁ numeric share_non_white 0 1.0000000 NA NA NA NA NA 3.156863e-01 0.1649152 6.000000e-02 1.950000e-01 2.800000e-01 4.200000e-01 0.810000 ▇▇▆▂▁ numeric share_vote_trump 0 1.0000000 NA NA NA NA NA 4.900000e-01 0.1187097 4.000000e-02 4.150000e-01 4.900000e-01 5.750000e-01 0.700000 ▁▁▆▇▇ numeric hate_crimes_per_100k_splc 4 0.9215686 NA NA NA NA NA 3.040930e-01 0.2527086 6.744680e-02 1.427066e-01 2.261971e-01 3.569347e-01 1.522302 ▇▂▁▁▁ numeric avg_hatecrimes_per_100k_fbi 1 0.9803922 NA NA NA NA NA 2.367613e+00 1.7142450 2.669408e-01 1.293139e+00 1.987068e+00 3.184344e+00 10.953480 ▇▅▁▁▁ Apart from summary statistics, last semester, we discussed a variety of ways to graphically display variables. Week 3 of ‘Making Sense of Criminological Data’ covered scatterplots, a graphical device to show the relationship between two quantitative variables. I don’t know if you remember the number of points and clicks you had to make in Excel to get this done. 1.13 R data types: Factors An important thing to understand in R is that categorical (ordered, also called ordinal, or unordered, also called nominal) data are typically encoded as factors, which are just a special type of vector. A factor is simply an integer vector that can contain only predefined values (this bit is very important) and is used to store categorical data. Factors are treated specially by many data analytic and visualisation functions. This makes sense because they are essentially different from quantitative variables. Although you can use numbers to represent categories, using factors with labels is better than using integers to represent categories because factors are self-describing (having a variable that has values “Male” and “Female” is better than a variable that has values “1” and “2” to represent male and female). When R reads data in other formats (e.g., comma-separated), it will automatically convert all character variables into factors by default. If you would rather keep these variables as simple character vectors, you need to explicitly ask R to do so. We will come back to this next week with some examples. Factors can also be created with the factor() function concatenating a series of character elements. You will notice that it is printed differently from a simple character vector and that it tells us the levels of the factor (look at the second printed line). the_smiths &lt;- factor(c(&quot;Morrisey&quot;, &quot;Marr&quot;, &quot;Rourke&quot;, &quot;Joyce&quot;)) #create a new factor the_smiths #auto-print the factor ## [1] Morrisey Marr Rourke Joyce ## Levels: Joyce Marr Morrisey Rourke Alternatively, for similar results, use the as.factor() function. Here, you will create the_smiths_char object and then transform it to a factor variable, the_smiths_f. the_smiths_char &lt;- c(&quot;Morrisey&quot;, &quot;Marr&quot;, &quot;Rourke&quot;, &quot;Joyce&quot;) #create a character vector the_smiths_f &lt;- as.factor(the_smiths_char) #create a factor using a character vector the_smiths_f #auto-print factor ## [1] Morrisey Marr Rourke Joyce ## Levels: Joyce Marr Morrisey Rourke Factors in R can be seen as vectors with more information added. This extra information consists of a record of the distinct values in that vector, called levels. If you want to know the levels in a given factor, you can use the levels() function: levels(the_smiths_f) ## [1] &quot;Joyce&quot; &quot;Marr&quot; &quot;Morrisey&quot; &quot;Rourke&quot; Notice that the levels appear printed in alphabetical order (Try levels(the_smiths_char) and see what R says. Yes, the_smiths_char and the_smiths_f are different!). There will be situations when this is not the most convenient order (e.g., Dec, Jan, Mar (Alphabetical order) instead of Jan, Mar, Dec). Later on, we will discuss in these tutorials how to reorder your factor levels when you need to. Let’s look at one more example here. Let’s say we are making data about Hogwarts Houses, which are divided into four houses: Gryffindor, Hufflepuff, Ravenclaw and Slytherin. #We create a data frame called HarryPotter with two variables: #a character vector called name and a character vector called house HarryPotter &lt;- data.frame(name = c(&quot;Potter&quot;, &quot;Malfoy&quot;, &quot;Lovegood&quot;, &quot;Chang&quot;, &quot;Hagrid&quot;, &quot;Diggory&quot;), house = c(&quot;Gryffindor&quot;, &quot;Slytherin&quot;, &quot;Ravenclaw&quot;, &quot;Ravenclaw&quot;, &quot;Gryffindor&quot;, &quot;Hufflepuff&quot;)) HarryPotter ## name house ## 1 Potter Gryffindor ## 2 Malfoy Slytherin ## 3 Lovegood Ravenclaw ## 4 Chang Ravenclaw ## 5 Hagrid Gryffindor ## 6 Diggory Hufflepuff Use str(HarryPotter$house) and see what R says. R will list all observations in the variable and say it’s a character variable, right? Now, we are going to convert house, a character variable, into a factor variable house_f, meaning that R will categorise the variable. HarryPotter$house_f &lt;- as.factor(HarryPotter$house) str(HarryPotter$house_f) ## Factor w/ 4 levels &quot;Gryffindor&quot;,&quot;Hufflepuff&quot;,..: 1 4 3 3 1 2 levels(HarryPotter$house_f) ## [1] &quot;Gryffindor&quot; &quot;Hufflepuff&quot; &quot;Ravenclaw&quot; &quot;Slytherin&quot; #try &#39;levels(HarryPotter$house)&#39; and find the difference Now, can you clearly understand what factor means in R? Factors are used to represent categorical data. Once created, factors can contain pre-defined set values, known as levels. Like we just converted 6-character data (house) into 4-factor data! (house_f). 1.14 How to import data Programmers are lazy, and the whole point of using code-based interfaces is that we get to avoid doing unnecessary work, like point-and-click downloading of files. When data exists online in a suitable format, we can tell R to read the data from the web directly and cut out the middleman (that being ourselves in our pointing-and-clicking activity). How can we do this? Well, think about what we do when we read in a file. We say, “Dear R, I would like to create a new object, please, and I will call this new object my_dataframe”. We do this by typing the name we are giving the object and the assignment function &lt;-(assignment operator). Then, on the right-hand side of the assignment function, there is the value to which we are assigning the variable. So it could be a bit of text (such as when you’re creating a my_text object and you pass it the string “I love stats”), or it could be some function, for example, when you read a CSV file with the read_csv() function. So, if we’re reading a CSV, we also need to specify where to read the CSV from. Where should R look to find this data? This is where normally you are putting in the path to your file, right? Something like: my_dataframe &lt;- read.csv(&#39;PATH_OR_URL_TO_CSV_FILE&#39;) Well, what if your data does not live on your laptop or PC? Well, if there is a way that R can still access this data just by following a path, then this approach will still work! Tips! Please keep your folders simple. Just use one data folder for this module and do not create multiple folders such as ‘data for week 1’, ‘data for week 2’, or something like this. All data we will use in the module are on the blackboard, but we will also provide the links to where we saved the data for you. You know when you right-click on the link, and select “Save As…” or whatever you click on to save? You could also select “Copy Link Address”. This just copies the webpage where this data is stored. Give it a go! Copy the address, and then paste it into your browser. It will take you to a blank page where a forced download of the data will begin. So what if you pasted this into the read.csv() function? #example 1: when you download data directly from the webpage, #you will use this code. my_dataframe &lt;- read.csv(&quot;www.data.com/data you want to import.csv&quot;) #example 2: when you download data from BB and save it to your #computer, you can use file.choose() #file.choose() brings up a file explorer window that allows #you to interactively choose a file path to work with. my_dataframe &lt;- read.csv(file.choose()) In the first example, the my_dataframe object would be assigned the value returned from the read.csv() function reading in the file from the ‘URL’ link you provided. File path is no mysterious thing, file path is simply the path to the file you want to read. If this is a website, then so be it. R also can read Stata (.dta) files using the read_dta() function in the Haven package and SPSS (.sav) files using the read_spss() function also in the Haven Package. There are so many different codes we can use to import data into R. In this course, you will learn one by one! 1.15 How to use ‘comment’ In the bits of code above, you will have noticed parts that were greyed out, for instance, in the last example provided. You can see that after the hashtag, all the text is being greyed out. What is this? What’s going on? These are comments. Comments are simply annotations that R will know are not code (and therefore don’t attempt to understand and execute). We use the hash-tag symbol to specify to R that what comes after is not programming code but simply bits of notes that we write to remind ourselves what the code is actually doing. Including these comments will help you to understand your code when you come back to it. To create a comment, you use the hashtag/ sign # followed by some text. Whenever the R engine sees the hashtag (#), it knows that what follows is not code to be executed. You can use this sign to include annotations when you are coding. These annotations are a helpful reminder to yourself (and others reading your code) of what the code is doing and (even more important) why you are doing it. It is good practice to often use annotations. You can use these annotations in your code to explain your reasoning and to create “scannable” headings in your code. That way, after you save your script, you will be able to share it with others or return to it at a later point and understand what you were doing when you first created it -see here for further details on annotations and how to save a script when working with the basic R interface. Just keep in mind: You need one # per line, and anything after that is a comment that is not executed by R. You can use spaces after. 1.16 How to Quit RStudio At some point, you will quit your R/R Studio session. I know it’s hard to visualise, right? Why would you want to do that? Anyhow, when that happens, R Studio will ask you a hard question: “Save workspace image to bla bla bla/.RData?” What to do? What does that even mean? If you say “yes”, what will happen is that all the objects you have in your environment will be preserved, alongside the History (which you can access in the top right set of windows) listing all the functions you have run within your session. So, next time you open this project, everything will be there. If you think that what is real is those objects and that history, well, then you may think that’s what you want to do. The truth is what is real is your scripts and the data that your scripts use as inputs. You don’t need anything in your environment because you can recreate those things by re-running your scripts. I like keeping things tidy, so when I am asked whether I want to save the image, my answer is always no. Most long-time users of R never save the workspace or care about saving the history. Remember, what is real is your scripts and the data. Just so you know, though, you should not then panic if you open your next R Studio session and you don’t see any objects in your environment. The good news is you can generate them quickly enough (if you really need them) by re-running your scripts. I would suggest that, at this point, it may be helpful for you to get into this habit as well. I suspect otherwise you will be in week 9 of the semester and have an environment full of garbage you don’t really need. What is more. I would suggest you go to the Tools drop-down menu, select Global Options, and make sure you select “Never” where it says “Save workspace”. Then click “Apply”. This way, you will never be asked to save what is in your global environment when you terminate a session. 1.17 Summary This week, we used the following R functions: install and load a package install.packages() library() generate and print data my_text &lt;- “I love stats.” print() say() explore data search() skim() attribute() mean() summary() str() transform variables into factor variables as_factor() "],["getting-to-know-your-data.html", "Chapter 2 Getting to know your data 2.1 Causality in Social Sciences 2.2 Getting data thanks to reproducibility 2.3 Getting a sense of your data 2.4 Data wrangling with dplyr 2.5 Using dplyr single verbs 2.6 Using dplyr for grouped operations 2.7 Making comparisons with numerical outcomes 2.8 Summary", " Chapter 2 Getting to know your data 2.1 Causality in Social Sciences In today’s session, we will refresh themes you have explored in previous research methods courses, specifically causality. This is a central concept in empirical research. We often do research because we want to make causal inferences. We want to be in a position where we establish whether an intervention or a social process is causally linked to crime or some other relevant criminological outcome. Making causal inferences often involves making comparisons. For example, between cases that have been subject to an intervention and cases that have not been subject to the causal process we are trying to investigate. But you should already know that not all kinds of research comparisons are the same. In previous methods courses, you must have discussed the differences between experimental and observational studies. These different kinds of research designs have a bearing on your ability to make causal inferences. Let’s think about a specific case so that this makes more sense. Is there discrimination against former offenders in the labour market? In other words, are offenders less likely to find employment after release because of prejudice among employers? Or can we say that the fact that former offenders are less likely to be in employment may be due to other factors? Perhaps they are less skilled. Perhaps they have less social capital, people they know that can help them to get jobs or to learn about job opportunities. Perhaps they are less interested in finding employment? Only in comparisons, when other things are equal, can you make causal inferences. It would only be fair to compare John, an ex-offender with a high school degree and X number of personal connections and Y numbers of professional experience, with Peter, a non-ex-offender, with the same educational and professional credentials as John (and everything else that matters when getting jobs also being equal between Peter and John). How can you do that? How can you create situations when other things are equal? Well, that’s what courses in research design are oriented to teach you. What is important for you to remember is that the way data are generated and the way you do your study will, of course, affect the kind of interpretations that you make from your statistical comparisons. And not all studies are created equal. Some research designs put you in a better position than others to make causal inferences. You should have learnt by now that the “bronze” standard for establishing causality in the social sciences is the randomised experiment. In a randomised trial, the researchers change the causal variable of interest for a group using something like a coin toss. As Angrist and Pischke (2015: xiii) highlight: “By changing circumstances randomly, we make it highly likely that the variable of interest is unrelated to the many other factors determining the outcomes we mean to study… Random manipulation makes other things equal hold on average across the groups that did and did not experience manipulation” So, say you want to establish whether arresting a perpetrator may have a deterrent effect on subsequent domestic abuse. You could randomise, basically using the equivalent of a lottery, to decide whether the police officer is going to arrest the perpetrator or not and then compare those you arrest with those you don’t arrest. Because you are randomising your treatment (the arrest), on average, the treatment and the control group, in the long run, should be fairly similar, and any differences you observe between them in the outcome of interest (domestic abuse recidivism) you could link it to your intervention -if you are interested in the answer to this you can read about it here. In this session, we will look at data from a randomised trial that tried to establish whether discrimination against former offenders exists in the labour market. In doing so, we will also learn various functions used in R to read data, transform data, and obtain summary statistics for groups. We will also very quickly introduce a plotting function used in R to generate graphics. 2.2 Getting data thanks to reproducibility Last week we introduced the notion of reproducible research and said that using and publishing code (particularly if using open-source tools like R) is the way that many researchers around the world think that science ought to be done. This way of operating makes research more open, more credible, and more legitimate. It also means that we can more easily access the data used in published research. For this session, we are going to use the data from this and this paper study. In this research project, the authors tried to answer the question of whether criminal antecedents and other personal characteristics have an impact on access to employment. You can find more details about this work in episode 8 of Probable Causation, the criminology podcast. If you want to know more about ‘Ban the Box: fair chance recruitment’ practices in the UK, you can find more information here and also can watch this short video from Leo Burnett who helped promote giving people a chance to explain their past. This could give you a better understanding of this issue. Amanda Agan and Sonja Starr developed a randomised experiment in which they created 15,220 fake resumes randomly generating these critical characteristics (such as having a criminal record) and used these resumes to send online job applications to low-skill, entry-level job openings in New Jersey and New York City. All the fictitious applicants were male and about 21 to 22 years old. These kinds of experiments are very common among researchers who want to explore through these “audits” whether some personal characteristics are discriminated against in the labour market. Because Amanda Agan and Sonja Starr conformed to reproducible standards when doing their research, we can access this data from the Harvard Dataverse (a repository for open research data). Click here to locate the data. On this page, you can see a download section and some files that can be accessed. One of them contains analytic code pertaining to the study, and the other contains the data. You also see a link called metadata. Metadata is data about data, it simply provides you with some information about the data. If you click on metadata, you will see a reference to the software the authors used (STATA) at the bottom. So we know these files are in STATA proprietary format. Let’s download the data file and then read the data into R. You could just click “download” and then place the file in your project directory. Alternatively, and preferably, you may want to use code to make your whole work more reproducible. Think of it this way: every time you click or use drop-down menus, you are doing things that others cannot reproduce because there won’t be a written record of your steps. You will need to do some clicking to find the required URL for writing your code. The file we want is the AganStarrQJEData.dta. Click on the name of this file. You will be taken to another webpage. You will see the download URL on it. Copy and paste this URL into your code below. #First, let&#39;s import the data using an URL address: library(haven) banbox &lt;- read_dta(&quot;https://dataverse.harvard.edu/api/access/datafile/3036350&quot;) ##Window users! R in Windows has some problems with HTTPS addresses; #that&#39;s why we need to do this first: This data file is a STATA.dta file in our working directory. To read STATA files, we will need the haven package. This is a package developed to import different kinds of data files into R. If you don’t have it, you will need to install it. And then load it. ##IF THE CODE ABOVE DOES NOT WORK, USE THIS CODE. ##Window users! R in Windows have some problems with https addresses, #in that case, try to use this code #First, let&#39;s create an object with the link. Paste the copied address here: urlfile &lt;- &quot;https://dataverse.harvard.edu/api/access/datafile/3036350&quot; #Now we can use the &#39;read_dta&#39; and &#39;url&#39; functions and import the data in the urlfile link library(haven) banbox &lt;- read_dta(url(urlfile)) You will need to pay attention to the file extension to find the appropriate function to read in your file. For example, if something has the extension .sav, it is a file used by the software SPSS. To read this, you would use the read_spss() function in the haven package. Some other file types need other packages. For example, to read in comma-separated values or .csv files, you can use the read_csv() function from the readr package. To read in Excel files, you would use the appropriate function from the readxl package, which might be read_xls() or read_xlsx(), depending on the file extension. 2.3 Getting a sense of your data 2.3.1 First steps What is the first thing you need to ask yourself when you look at a dataset? Data are often too big to look at the whole thing. It is almost always impossible to eyeball the entire dataset and see what you have in terms of interesting patterns or potential problems. It is often a case of information overload, and we want to be able to extract what is relevant and important about it. But where do you start? You can find a brief but very useful overview put together by Steph de Silva in the image below. Read it before we carry on. As Mara Averick suggests, this also makes for good relationship advice! Here, we will introduce a few functions that will help you make sense of what you have just downloaded. Summarising the data is the first step in any analysis, and it is also used to find potential problems with the data. Regarding the latter, you want to look out for missing values, values outside the expected range (e.g., someone aged 200 years), values that seem to be in the wrong units, mislabeled variables, or variables that seem to be the wrong class (e.g., a quantitative variable encoded as a factor). Let’s start with the basic things you always look at first in a dataset. You can see in the Environment window that banbox has 14813 observations (rows) of 62 variables (columns). You can also obtain this information using code. Here, you want the DIMensions of the data frame (the number of rows and columns), so you use the dim() function: dim(banbox) ## [1] 14813 62 Looking at this information will help you to diagnose whether there was any trouble getting your data into R (e.g., imagine you know there should be more cases or more variables). You may also want to have a look at the names of the columns using the names() function. We will see the names of the variables. names(banbox) ## [1] &quot;nj&quot; &quot;nyc&quot; &quot;app_date_d&quot; ## [4] &quot;pre&quot; &quot;post&quot; &quot;storeid&quot; ## [7] &quot;chain_id&quot; &quot;center&quot; &quot;crimbox&quot; ## [10] &quot;crime&quot; &quot;drugcrime&quot; &quot;propertycrime&quot; ## [13] &quot;ged&quot; &quot;empgap&quot; &quot;white&quot; ## [16] &quot;black&quot; &quot;remover&quot; &quot;noncomplier_store&quot; ## [19] &quot;balanced&quot; &quot;response&quot; &quot;response_date_d&quot; ## [22] &quot;daystoresponse&quot; &quot;interview&quot; &quot;cogroup_comb&quot; ## [25] &quot;cogroup_njnyc&quot; &quot;post_cogroup_njnyc&quot; &quot;white_cogroup_njnyc&quot; ## [28] &quot;ged_cogroup_njnyc&quot; &quot;empgap_cogroup_njnyc&quot; &quot;box_white&quot; ## [31] &quot;pre_white&quot; &quot;post_white&quot; &quot;white_nj&quot; ## [34] &quot;post_remover_ged&quot; &quot;post_ged&quot; &quot;remover_ged&quot; ## [37] &quot;post_remover_empgap&quot; &quot;post_empgap&quot; &quot;remover_empgap&quot; ## [40] &quot;post_remover_white&quot; &quot;post_remover&quot; &quot;remover_white&quot; ## [43] &quot;raerror&quot; &quot;retail&quot; &quot;num_stores&quot; ## [46] &quot;avg_salesvolume&quot; &quot;avg_num_employees&quot; &quot;retail_white&quot; ## [49] &quot;retail_post&quot; &quot;retail_post_white&quot; &quot;percblack&quot; ## [52] &quot;percwhite&quot; &quot;tot_crime_rate&quot; &quot;nocrimbox&quot; ## [55] &quot;nocrime_box&quot; &quot;nocrime_pre&quot; &quot;response_white&quot; ## [58] &quot;response_black&quot; &quot;response_ged&quot; &quot;response_hsd&quot; ## [61] &quot;response_empgap&quot; &quot;response_noempgap&quot; As you may notice, these names may be hard to interpret. If you open the dataset in the data viewer of RStudio (using View), you will see that each column has a variable name and underneath a longer and more meaningful variable label that tells you what each variable means. View(banbox) 2.3.2 On tibbles and labelled vectors You also want to understand what the banbox object actually is. You can do that using the class() function: class(banbox) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; What does tbl stand for? It refers to tibbles. This is essentially a new type of data structure introduced into R. Tibbles are data frames, but a particular type. Not all data frames we encounter in R are tibbles. The R language has been around for a while, and sometimes, things that made sense a couple of decades ago make less sense now. A number of programmers are trying to create code that is more modern and more useful today. They are doing this by introducing a set of packages that speak to each other in order to modernise R without breaking existing code. You can think of it as an easier and more efficient modern dialect of R. This set of packages is called the tidyverse. Tibbles are data frames that have been optimised for use with this new set of packages. You can read a bit more about tibbles here. You can also look at the class of each individual column. As discussed, the class of the variable lets us know, for example, if it is an integer (number), character, or factor. To get the class of one variable, you pass it to the class() function. For example: class(banbox$crime) ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; class(banbox$num_stores) ## [1] &quot;numeric&quot; We talked about numeric vectors in week one. It is simply a collection of numbers. But what is a labelled vector? This is a new type of vector introduced by the haven package. Labelled vectors are categorical variables that have labels. Go to the Environment panel and left-click in the banbox object. This should open the data browser in the top left quadrant of RStudio. If you look carefully, you will see that the various columns that include categorical variables only contain numbers. In many statistical environments, such as STATA or SPSS, this is a common standard. The variables have a numeric value for each observation, and each of these numeric values is associated with a label. This made sense when computer memory was an issue - for this was an efficient way of saving resources. It also made manual data input quicker. These days, it makes perhaps less sense. However, labelled vectors give you a chance to reproduce data from other statistical environments without losing any fidelity in the import process. See what happens if we try to summarise this labelled vector. We will use the table() to provide a count of observations on the various valid values of the crime variable. It is a function that obtains your frequency distribution. table(banbox$crime) ## ## 0 1 ## 7323 7490 So, we see that we have 7490 observations classed as 1 and 7323 classed as 2. If only we knew what those numbers represent! Well, we actually do. We will use the attributes() function to see the different “compartments” within your “box”, your object. attributes(banbox$crime) ## $label ## [1] &quot;Applicant has Criminal Record&quot; ## ## $format.stata ## [1] &quot;%9.0g&quot; ## ## $class ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; ## ## $labels ## No Crime Crime ## 0 1 So, this object has different compartments. The first one is called a label, and it provides a description of what the variable measures. This is what you saw in the RStudio data viewer earlier. The second compartment explains the original format. The third one identifies the class of the vector. Whereas, the final one, labels, provides the labels that allow us to identify what the meaning of 0 and 1 mean in this context. 2.3.3 Turning variables into factors and changing the labels Last week, we said that many R functions expect factors when you have categorical data, so typically, after you import data into R you may want to coerce your labelled vectors into factors. To do that, you need to use the as_factor() function of the haven package. Let’s see how we do that. #This code asks R to create a new column in your banbox tibble #that is going to be called crime_f. Typically, when you alter #variables, you can to create a new one so that the original gets #preserved in case you do something wrong. Then we use the #as_factor() function to explain to R that what we want to do #is to get the original crime variable and mutate it into #a factor, this resulting factor is what will be stored in #the new column. banbox$crime_f &lt;- as_factor(banbox$crime) You will see now that you have 63 variables in your dataset; look at the environment to check. Let’s explore the new variable we have created (you can also look for the new variable in the data browser and see how it looks different to the original crime variable): class(banbox$crime_f) ## [1] &quot;factor&quot; table(banbox$crime_f) ## ## No Crime Crime ## 7323 7490 attributes(banbox$crime_f) ## $levels ## [1] &quot;No Crime&quot; &quot;Crime&quot; ## ## $class ## [1] &quot;factor&quot; ## ## $label ## [1] &quot;Applicant has Criminal Record&quot; So far, we have looked at single columns in your data frame one at a time. But there is a way that you can apply a function to all elements of a vector (list or data frame). You can use the functions sapply(), lapply(), and mapply() . To find out more about when to use each one see here. For example, we can use the lapply() function to look at each column and get its class. To do so, we have to pass two arguments to the lapply() function, the first is the name of the data frame to tell it what to look through, and the second is the function we want it to apply to every column of that function. So we want to type lapply('name of dataframe', 'name of function') Which is: lapply(banbox, class) As you can see, many variables are classed as ‘labelled’. This is common with survey data. Many of the questions in social surveys measure the answers as categorical variables (e.g., these are nominal or ordinal-level measures). In fact, with this dataset there are many variables that are encoded as numeric that really aren’t. Welcome to real-world data, where things can be a bit messy and need tidying! See, for example, the variable black: class(banbox$black) ## [1] &quot;numeric&quot; table(banbox$black) ## ## 0 1 ## 7406 7407 We know that this variable measures whether someone is black or not. When people use 0 and 1 to code binary responses, typically, they use a 1 to denote a positive response, a yes. So, I think it is fair to assume that a 1 here means the respondent is black. Because this variable is of class numeric, we cannot simply use as_factor() to assign the pre-existing labels and create a new factor. In this case, we don’t have preexisting labels since this is not a labelled vector. So what can we do to tidy this variable? We’ll need to do some further work. #We will use a slightly different function as.factor() banbox$black_f &lt;- as.factor(banbox$black) #You can check that the resulting column is a factor class(banbox$black_f) ## [1] &quot;factor&quot; #But if you print the frequency distribution #you will see the data are still presented #in relation to 0 and 1 table(banbox$black_f) ## ## 0 1 ## 7406 7407 #You can use the levels function to see the levels of the categories in your factor levels(banbox$black_f) ## [1] &quot;0&quot; &quot;1&quot; So, all we have done is create a new column that is a factor but still refers to 0 and 1. If we assume (rightly) that 1 means black, we have 7407 black applicants. Of course, it makes sense we only get 0 and 1 here. What else could R do? This is not a labelled vector, so there is no way for R to know that 0 and 1 mean anything other than 0 and 1, which is why those are the levels is using. But now that we have the factor, we can rename those levels. We can use the following code to do just that: #We are using the levels function to access them and change #them to the levels we specify with the c() function. Be #careful here because the order we specify here will map #out to the order of the existing levels. #So given that 1 is black and black is the second level #(as shown when printing the results above) you want to make sure that in the c() #you write black as the second level. levels(banbox$black_f) &lt;- c(&quot;non-Black&quot;, &quot;Black&quot;) table(banbox$black_f) ## ## non-Black Black ## 7406 7407 This gives you an idea of the kind of transformations you often want to perform to make your data more useful for your purposes. But let’s keep looking at functions you can use to explore your dataset. 2.3.4 Looking for missing data and other anomalies You can, for example, use the head() function if you just want to visualise the values for the first few cases in your dataset. The next code, for example, asks for the values for the first two cases. If you want a different number to be shown, you just need to change the number you are passing as an argument. head(banbox, 2) In the same way, you could look at the last two cases in your dataset using tail(): tail(banbox, 2) It is good practice to do this to ensure R has read the data correctly and there’s nothing terribly wrong with your dataset. If you have access to STATA you can open the original file in STATA and check if there are any discrepancies, for example. Glimpsing your data in this way can also give you a first impression of what the data looks like. One thing you may also want to do is to see if there are any missing values. For that, we can use the is.na() function. Missing values in R are coded as NA. The code below, for example, asks for NA values for the variable response_black in the banbox object for observations 1 to 10: is.na(banbox$response_black[1:10]) ## [1] FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE TRUE The result is a logical vector that tells us if it is true that there is missing (NA) data for each of those first ten observations. You can see that there are 6 observations out of those 10 that have missing values for this variable. sum(is.na(banbox$response_black)) ## [1] 7406 This is asking R to sum up how many cases are TRUE NA in this variable. When reading a logical vector like the one we are creating, R will treat the FALSE elements as 0s and the TRUE elements as 1s. So basically, the sum() function will count the number of TRUE cases returned by the is.na() function. You can use a bit of a hack to get the proportion of missing cases instead of the count: mean(is.na(banbox$response_black)) ## [1] 0.4999662 This code exploits the mathematical fact that the mean of binary outcomes (0 or 1) gives you the proportion of ones in your data. As a rule of thumb, if you see more than 5% of the cases declared as NA, you need to start thinking about the implications of this. Beware of formulaic application of rules of thumb such as this, though! In this case, we know that 49%% of the observations have missing values in this variable. When you see things like this the first thing to do is to look at the codebook or documentation to try to get some clues as to why there are so many missing cases. With survey data, you often have questions that are simply not asked to everybody, so it’s not necessarily that something went very wrong with the data collection but simply that the variable in question was only used with a subset of the sample. Therefore, any analysis you do using this question will only relate to that particular subset of cases. There is a whole field of statistics devoted to doing analysis when missing data is a problem. R has extensive capabilities for dealing with missing data -see, for example here. For the purpose of this introductory course, however, we only explain how to do analysis that ignores missing data. This is often referred to as a full/complete case analysis because you only use observations for which you have full information in all the variables you employ. You would cover techniques for dealing with this sort of issue in more advanced courses. 2.4 Data wrangling with dplyr The data analysis workflow has a number of stages. The diagram below (produced by Hadley Wickham) is a nice illustration of this process: We have started to see different ways of bringing data into R. And we have also started to see how we can explore our data. It is now time we start discussing one of the following stages, transform. A good deal of time and effort, in data analysis, is devoted to this. You get your data, and then you have to transform it so that you can answer the questions you want to address in your research. We have already seen, for example, how to turn variables into factors, but there are other things you may want to do. R offers a great deal of flexibility in how to transform your data; here, we are going to illustrate some of the functionality of the dplyr package for data carpentry (a term people use to refer to this kind of operation). This package is part of the tydiverse and it aims to provide a friendly and modern take on how to work with data frames (or tibbles) in R. It offers, as the authors of the package put it, “a flexible grammar of data manipulation”. Dplyr aims to provide a function for each basic verb of data manipulation: filter() to select cases based on their values. arrange() to reorder the cases. select() and rename() to select variables based on their names. mutate() and transmute() to add new variables that are functions of existing variables. summarise() to condense multiple values to a single value. sample_n() and sample_frac() to take random samples. In this session, we will introduce and practice some of these. But we won’t have time to cover everything. There is, however, a very nice set of vignettes for this package in the help files, so you can try to go through those if you want a greater degree of detail or more practice. Now, let’s load the package: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Notice that when you run this package you get a series of warnings in the console. It is telling us that some functions from certain packages are being “masked”. One of the things with a language like R is that sometimes packages introduce functions that have the same name as others that are already loaded into your session. When that happens, the newly loaded ones will override the previous ones. You can still use them, but you will have to refer to them explicitly. Otherwise, R will assume you are using the function most recently loaded: #Example: #If you use load dplyr and then invoke the *filter()* function #R will assume you are using the filter function from dplyr #rather than the *filter()* function that exist in the *stats* #package, which is part of the basic installation of R. If #after loading dplyr you want to use the filter function from #the stats package, you will have to invoke it like this: stats::filter() #Notice the grammar, first you write the name of the package, #then colon twice, and then the name of the function. Don&#39;t #run this code. You would need to pass some valid arguments #for this to produce meaningful results. 2.5 Using dplyr single verbs One of the first operations you may want to carry out when working with data frames is to subset them based on the values of particular variables. Say we want to replicate the results reported by Agan and Starr in 2017. In this earlier paper, these researchers only used data from the period prior to the introduction of Ban the Box legislation and only used data from businesses that asked about criminal records in their online applications. How can we recreate this dataset? For this kind of operation, we use the filter() function. Like all single verbs in dplyr, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame, selecting rows where the expression is TRUE. Ok, let’s filter out some information we are interested in from bandbox. If we look at the dataset, we can see that there is a variable called “crimbox” that identifies ‘applications that require information about criminal antecedents’ and there is a variable called “pre” that identifies ‘whether the application was sent before the legislation was introduced’. In this dataset, the value 1 is being used to denote positive responses. Therefore, if we want to create the 2017 dataset, we would start by selecting only data where the value in these two variables equals 1 as shown below. #We will store the results of filtering the data in a new object that I am calling aer #(short for the name of the journal in which the paper was published) aer2017&lt;- filter(banbox, crimbox == 1, pre == 1) Notice that the number of cases equals the number of cases reported by the authors in their 2017 paper. That’s cool! So far, we have replicated the results. You may have noticed in the code above that I wrote “==” instead of “=”. Logical operators in R are not written exactly the same way as in normal practice. Keep this in mind when you get error messages when running your code. Often, the source of your error may be that you are writing the logical operators the wrong way (as far as R is concerned). Look here for valid logic operators in R. Earlier, we said that real-life data may have hundreds of variables, and only a few of them may be relevant to your analysis. For this week’s analysis, we want to select only a few variables that might be highly related to ‘Discrimination in Employment’. Say you only want “crime”, “ged” (a ged is a high school equivalence diploma rather than a proper high school diploma and is sometimes seen as inferior), “empgap” (a gap year on employment), “black_f”, “response”, and “daystoresponse” from this dataset. For this kind of operation, you use the select() function. The syntax of this function is easy. First, we name the data frame object (“aer2017”), and then we list the variables. The order in which we list the variables within the select function will determine the order in which those columns appear in the new data frame we are creating. So, this is a handy function to use if you want to change the order of your columns for some reason. Since I am pretty confident I am not making a mistake, I will transform the original “aer2017” tibble rather than creating an entirely new object. aer2017 &lt;- select(aer2017, crime, ged, empgap, black_f, response, daystoresponse) If you now look at the global environment, you will see that the “aer2017” tibble has reduced in size and now only has 6 columns. If you view the data, you will see the 6 variables we selected. 2.6 Using dplyr for grouped operations So far, we have used dplyr single verbs for ungrouped operations. However, we can also use some of the functionality of dplyr to obtain answers to questions that relate to groups of cases within our data frame. Imagine that you want to know if applicants with a criminal record are less likely to receive a positive response from employers. How could you figure that one out? To answer this kind of question, we can use the group_by() function in conjunction with other dplyr functions. In particular, we are going to look at the summarise function. First, we group the observations by criminal record in a new object, “by_antecedents”, by using as_factor() in the call to the crime variable the results will be labelled later on (even though we are not changing the crime variable in the aer2017 data frame). Keep in mind we are using as_factor() because the column crime is a labelled vector rather than a factor or a character vector, and we do this to aid interpretation (it is easier to interpret labels than 0 and 1). aer2017_by_antecedent &lt;- group_by(aer2017, as_factor(crime)) #Then we run the summarise function to provide some useful #summaries of the groups we are using: the number of cases #and the mean of the response variable results_1 &lt;- summarise(aer2017_by_antecedent, count = n(), outcome = mean(response, na.rm = TRUE)) results_1 #auto-print the results stored in the newly created object ## # A tibble: 2 × 3 ## `as_factor(crime)` count outcome ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 No Crime 1319 0.136 ## 2 Crime 1336 0.0846 Let’s look at the code in the summarise function above. First, we are asking R to place the results in an object we are calling “results”. Then, we specify that we want to group the data in the way we specified in our group_by() function before, that is, by criminal record. Then, we pass two arguments. Each of these arguments is creating a new variable in the resulting object called “results”. The first variable we are creating is called “count” by saying this equals “n()”, we are specifying to R that this new variable simply counts the number of cases in each of the grouping categories. The second variable we are creating is called “outcome”, and to compute this variable, we are asking R to compute the mean of the variable response for each of the two groups of applicants defined in “by_antecedents” (those with records, those without). Remember that the variable response in the “aer2017” data frame was coded as a numeric variable, even though, in truth, it is categorical in nature (there was a response, or not, from the employers). It doesn’t really matter. Taking the mean of a binary variable, in this case, is mathematically equivalent to computing a proportion, as we discussed earlier. So, what we see here is that about 13.6% of applicants with no criminal record received a positive response from their employers, whereas only 8% of those with criminal records did receive such a response. Given that the assignation of a criminal record was randomised to the applicants, there’s a pretty good chance that no other confounders are influencing this outcome. And that is the beauty of randomised experiments. You may be in a position to make stronger claims about your results. 2.7 Making comparisons with numerical outcomes We have been looking at relationships so far between categorical variables, specifically between having a criminal record (yes or no), race (black or white), and receiving a positive response from employers (yes or no). Often, we may be interested in looking at the impact of a factor on a numerical outcome. The researchers measured such an outcome in the banbox object. The variable “daystoresponse” tells us how long it took the employers to provide a positive response. Let’s look at this variable: summary(banbox$daystoresponse) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 3.00 10.00 19.48 28.00 153.00 14361 The summary() function provides some useful stats for numerical variables. We obtain the minimum and maximum values, the 25th percentile, the median, the mean, the 75th percentile, and the number of missing data (NA). You can see the massive amount of missing data here. Most cases have missing data on this variable. Clearly, this is a function of, first and foremost, the fact that the number of days to receive a positive response will only be collected in cases where there was a positive response! However, even accounting for that, it is clear that this information is also missing in many cases that received a positive response. So, given all of this, we need to be very careful when interpreting this variable. However, because it is the only numeric variable here, we will use it to illustrate some handy functions. We could do as before and get results by groups. Let’s look at the impact of race on days to response: banbox_by_race &lt;- group_by(banbox, black_f) results_2 &lt;- summarise(banbox_by_race, avg_delay = mean(daystoresponse, na.rm = TRUE)) results_2 ## # A tibble: 2 × 2 ## black_f avg_delay ## &lt;fct&gt; &lt;dbl&gt; ## 1 non-Black 18.7 ## 2 Black 20.4 We can see that the average delay seems to be longer for ‘Black’ applicants than ‘White’ applicants. However, we could also try to represent these differences graphically. The problem with comparing groups on quantitative variables using numerical summaries, such as the mean, is that these comparisons hide more than they show. We want to see the full distribution, not just the mean. For this, we are going to use ggplot2, the main graphical package we will use this semester. We won’t get into the details of this package or what the code below means, but just try to run it. We will cover graphics in R in the next section. This is just a taster for it. library(ggplot2) ggplot(banbox, aes(y = daystoresponse, x = black_f)) + geom_boxplot() Watch this video and see if you can interpret the results portrayed here. What do you think? Overall, the outcomes are worse for ‘Black’ applicants, and in fact, the authors find that employers substantially increase discrimination on the basis of race after ban the box goes into effect. You can now replicate these findings with the data provided. by applying the new skills you’ve learned this week! 2.8 Summary This week, we used the following R functions: read data: ‘haven’ package read_dta() explore data dim() names() class() attribute() head() tail() table() lapply() sum() mean() transform variables into factor variables as_factor() missing variable is.na() ‘dplyr’ package filter() select() group_by() summarise() "],["data-visualisation-with-r.html", "Chapter 3 Data visualisation with R 3.1 Introduction 3.2 Anatomy of a plot 3.3 What graph should I use? 3.4 Visualising numerical variables: Histograms 3.5 Visualising numerical variables: Density plots 3.6 Visualising numerical variables: Box plots 3.7 Exploring relationships between two quantitative variables: scatterplots 3.8 Scatterplots conditioning in a third variable 3.9 Scatterplot matrix 3.10 Titles, legends, and themes in ggplot2 3.11 Plotting categorical data: bar charts 3.12 Further resources 3.13 Summary", " Chapter 3 Data visualisation with R 3.1 Introduction A picture is worth a thousand words; when presenting and interpreting data, this basic idea also applies. There has been, indeed, a growing shift in data analysis toward more visual approaches to both interpretation and dissemination of numerical analysis. Part of the new data revolution consists of the mixing of ideas from visualisation of statistical analysis and visual design. Indeed, data visualisation is one of the most interesting areas of development in the field. Good graphics not only help researchers make their data easier to understand by the general public, but they are also a useful way to understand the data ourselves. In many ways, it is very often a more intuitive way to understand patterns in our data than trying to look at numerical results presented in a tabular form. Recent research has revealed that papers which have good graphics are perceived as overall clearer and more interesting, and their authors are perceived as smarter (see this presentation) The preparation for this session includes many great resources on visualising quantitative information, and if you have not had time to go through them, we recommend that you take some time to do so. As with other aspects of R, there are a number of core functions that can be used to produce graphics. However, these offer limited possibilities for building graphs. The package we will be using throughout this tutorial is ggplot2. The aim of ggplot2 is to implement the grammar of graphics. The ggplot2 package has excellent online documentation and is becoming an industry-standard in some sectors. Here for example, you can read about how the BBC uses as part of their News service. If you don’t already have the package installed (check you do), you will need to do so using the install.packages() function. You will then need to load up the package library(ggplot2) The grammar of graphics upon which this package is based defines various components of a graphic. Some of the most important are: +The data: For using ggplot2, the data has to be stored as a data frame or tibble. +The geoms: They describe the objects that represent the data (e.g., points, lines, polygons, etc..). This is what gets drawn. You can have various types layered over each other in the same visualisation. +The aesthetics: They describe the visual characteristics that represent data (e.g., position, size, colour, shape, transparency). +Facets: They describe how data is split into subsets and displayed as multiple small graphs. +Stats: They describe statistical transformations that typically summarise data. Let’s take it one step at a time. 3.2 Anatomy of a plot The philosophy behind this is that all graphics are made up of layers. The package ggplot2 is based on the grammar of graphics, the idea that you can build every graph from the same few components: a data set, a set of geoms—visual marks that represent data points—and a coordinate system. Take this example (all taken from Wickham, H. (2010). A layered grammar of graphics. Journal of Computational and Graphical Statistics, 19(1), 3-28.) You have a table such as: You then want to plot this. To do so, you want to create a plot that combines the following layers: This will result in a final plot: Let’s have a look at what this looks like for a graph. Let’s have a look at some data about banning orders for different football clubs. First, you need to read the data. We keep this data on a website, and you can download it with the following code: # load readr library and import the data using read_csv() function library(readr) fbo &lt;- read_csv(&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/FootbalBanningOrders.csv&quot;) ## New names: ## Rows: 119 Columns: 4 ## ── Column specification ## ──────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (2): Club.Supported, League.of.the.Club.Supported dbl (2): ...1, Banning.Orders ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ ## Specify the column types or set `show_col_types = FALSE` to quiet this message. ## • `` -&gt; `...1` You can also find this on the Blackboard page for this week’s learning materials. If you download it from there, make sure to save it in your project directory, possibly in a subfolder called “Datasets.” Then, you can read it from there. One thing we mentioned from the first lab is conventions in the naming of objects. This also applies to the names of your variables (i.e. your column names) within your data. If you look at the fbo data frame, either with the View() function or by printing the names of the columns with the names() function, you can see this dataset violates that requirement: names(fbo) ## [1] &quot;...1&quot; &quot;Club.Supported&quot; ## [3] &quot;Banning.Orders&quot; &quot;League.of.the.Club.Supported&quot; To address this, we can use a function called clean_names(), which lives inside the janitor package. This will replace any spaces with an underscore and turn capital letters into lowercase. Much more tidy! library(janitor) fbo &lt;- clean_names(fbo) Now let’s explore the question of the number of banning orders for clubs in different leagues. But as a first step, let’s just plot the number of banning orders for each club. Let’s build this plot: ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_point() + #geometry theme_bw() #backgroud coordinate system The first line above begins a plot by calling the ggplot() function and putting the data into it. You have to name your data frame with the data argument, and then, within the aes() command, you pass the specific variables that you want to plot. In this case, we only want to see the distribution of one variable, banning orders, on the y-axis, and we will plot the club supported on the x-axis. The second line is where we add the geometry. This is where we tell R what we want the graph to be. Here we say we want it to be points by using geom_points. You can see a list of all possible geoms here. The third line is where we can tweak the display of the graph. Here, I used theme_bw(), a nice clean theme. You can try with other themes. To get a list of themes, you can also see the resource here. If you want more variety, you can explore the package ggthemes. ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_point() + #geometry theme_dark() #backgroud coordinate system Changing the theme is not all you can do with the third element. For example, here, you can’t really read the axis labels because they’re all overlapping. One solution would be to rotate your axis labels 90 degrees, with the following code: axis.text.x = element_text(angle = 90, hjust = 1). You pass this code to the theme argument. ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) OK, what if we don’t want it to be points, but instead, we want it to be a bar graph? ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_bar(stat = &quot;identity&quot;) + #geometry theme(axis.text.x = element_text(angle = 90, hjust = 1)) #backgroud coordinate system You might notice here we pass an argument stat = \"identity\" to geo_bar() function. This is because you can have a bar graph where the height of the bar shows frequency (stat = \"count\") or where the height is taken from a variable in your data frame (stat = \"identity\"). Here, we specified a y-value (height) as the banning_orders variable. So this is cool! But what if I like both? Well, this is the beauty of the layering approach of ggplot2. You can layer on as many geoms as your little heart desires! XD ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_bar(stat = &quot;identity&quot;) + #geometry 1 geom_point() + #geometry 2 theme(axis.text.x = element_text(angle = 90, hjust = 1)) #backgroud coordinate system You can add other things too. For example, you can add the mean number of banning_orders: ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_bar(stat = &quot;identity&quot;) + #geometry 1 geom_point() + #geometry 2 geom_hline(yintercept = mean(fbo$banning_orders)) + #mean line theme(axis.text.x = element_text(angle = 90, hjust = 1)) #backgroud coordinate system This is basically all you need to know to build a graph! So far, we have introduced a lot of code, some of which you may not fully understand. Don’t worry too much; we just wanted to give you a quick introduction to some of the possibilities. Later in the session, we will go back to some of these functions in a slower way. The takeaway from this section is to understand the basic elements of the grammar of graphics. 3.3 What graph should I use? There are a lot of points to consider when you are choosing what graph to use to visually represent your data. There are some best practice guidelines, but at the end of the day, you need to consider what is best for your data. What do you want to show? What graph will best communicate your message? Is it a comparison between groups? Is it the frequency distribution of 1 variable? As some guidance, you can use the below cheatsheet, taken from Nathan Yau’s blog Flowingdata: However, keep in mind that this is more of a guideline aimed to nudge you in the right direction. There are many ways to visualise the same data, and sometimes, you might want to experiment with some of these and see what the differences are. There is also a vast amount of research into what works in displaying quantitative information. The classic book is by Edward Tufte1, but since then, there have been many other researchers as well who focus on approaches to displaying data. Two useful books you may want to consider are Few (2012)2 and Cairo (2016)3. Claus Wilke is also producing a textbook that is freely available on the internet. These authors tend to produce recommendations on what to use (and not use) in certain contexts. For example, most data visualisation experts agree that you should not use 3D graphics unless there is a meaning to the third dimension. So using 3D graphics just for decoration, as in this case is normally frowned upon. However, there are cases when including a third dimension is vital to communicating your findings. See this example. Also, often, certain chart types are vilified. For example, the pie chart is one such example. A lot of people (including your course leader) really dislike pie charts, e.g. see here. If you want to display proportion, research indicates that a square pie chart is more likely to be interpreted correctly by viewers. See here. Also, in some cases, bar plots (if used to visualise quantitative variables) can hide important features of your data and might not be the most appropriate means for comparison: This has led to a kickstarter campaign around actually banning bar plots…! So, choosing the right plot and how to design the different elements of a plot is somehow an art that requires practice and a good understanding of the data visualisation literature. Here, we can only provide you with an introduction to some of these issues. At the end of the chapter, we will also highlight additional resources you may want to explore on your own. An important consideration is that the plot that you use depends on the data you are plotting, as well as the message you want to convey with the plot, the audience that it is intended for, and even the format in which it will be presented (a website, a printed report, a PowerPoint presentation, etc.). So, for example, returning to the difference in the number of banning orders between clubs in different leagues, what are some ways of plotting these? One suggestion is to make a histogram for each one. You can use ggplot’s facet_wrap() option to split graphs by a grouping variable. For example, to create a histogram of banning orders, you write: ggplot(data = fbo, aes(x = banning_orders)) + geom_histogram() Now to split this by league_of_the_club_supported, you use facet_wrap() in the coordinate layer of the plot. ggplot(data = fbo, aes(x = banning_orders)) + geom_histogram() + facet_wrap(~league_of_the_club_supported) Well, you can see there’s a different distribution in each league. But is this easy to compare? Maybe another approach would make it easier? Personally, I like boxplots (we will explain them in greater detail below) for showing distribution. So let’s try: ggplot(data = fbo, aes(x = league_of_the_club_supported, y = banning_orders)) + geom_boxplot() This makes the comparison significantly easier, right? But the order is strange! Remember we talked about factors in previous weeks? Well, the good thing about factors is that we can arrange them in their natural order. If we don’t describe an order, then R uses the alphabetical order. So, let’s reorder our factor. To do that, we specify the levels in the order in which we want to be embedded within the factor. We use the code we introduced last week to do this. fbo$league_of_the_club_supported &lt;- factor(fbo$league_of_the_club_supported, levels = c(&quot;Premier League&quot;, &quot;Championship&quot;, &quot;League One&quot;, &quot;League Two&quot;, &quot;Other clubs&quot;)) And now, create the plot again! ggplot(data = fbo, aes(x = league_of_the_club_supported, y = banning_orders)) + geom_boxplot() Now, this is great! We can see that the higher the league, the more banning orders they have. Any ideas why? We’ll now go through some examples of graphs made using the ggplot2 package, stopping a bit more on each of them. 3.4 Visualising numerical variables: Histograms Histograms are useful ways of representing quantitative variables visually. As mentioned earlier, we will emphasise in this course the use of the ggplot() function. With ggplot(), you start with a blank canvas and keep adding specific layers. The ggplot() function can specify the dataset and the aesthetics (the visual characteristics that represent the data). To get the data we’re going to use here, load up the package MASS and then call the Boston data into your environment. library(MASS) data(Boston) This package has a dataframe called Boston. This data shows housing values in the suburbs of Boston (USA). To access the codebook (how you find out what variables are), use the “?”, ?Boxton. OK so let’s make a graph about the variable which represents the per capita crime rate by town (crim). If you want to produce a histogram with the ggplot function, you would use the following code: ggplot(Boston, aes(x = crim)) + geom_histogram() So you can see that ggplot works in a way that allows you to add a series of additional specifications (layers, annotations). In this simple plot, the ggplot function simply maps crim as the variable to be displayed (as one of the aesthetics) and the dataset. Then, you add the geom_histogram to tell R that you want this variable to be represented as a histogram. Later, we will see what other things you can add. A histogram is simply putting cases in “bins” and then creates a bar for each bin. You can think of it as a visually grouped frequency distribution. The code we have used so far has used a bin-width of size range/30, as R kindly reminded us in the output. But you can modify this parameter if you want to get a rougher or a more granular picture. In fact, you should always play around with different specifications of the bin width until you find one that tells the full story in a parsimonious way. ggplot(Boston, aes(x = crim)) + geom_histogram(binwidth = 1) We can pass arguments to the geoms, as you see. Here, we are changing the size of the bins (for further details on other arguments, you can check the help files). Using a bin-width of 1, we are essentially creating a bar for every one-unit increase in the percent rate of crime. We can still see that most towns have a very low level of crime. Let’s sum the number of towns with a value lower than 1 in the per capita crime rate. We use the sum function for this, specifying we are only interested in adding cases where the value of the variable crim is lower than 1. sum(Boston$crim &lt; 1) ## [1] 332 We can see that the large majority of towns, 332 out of 506, have a per capita crime rate below 1%. But we can also see that there are some towns that have a high concentration of crime. This is a spatial feature of crime; it tends to concentrate in particular areas and places. You can see how we can use visualisations to show the data and get a first feeling for how it may be distributed. When plotting a continuous variable, we are interested in the following features: Asymmetry: whether the distribution is skewed to the right or to the left or follows a more symmetrical shape. Outliers: Are there one or more values that seem very unlike the others? Multimodality: How many peaks does the distribution have? More than one peak may indicate that the variable is measuring different groups. Impossibilities or other anomalies: Values that are simply unrealistic given what we are measuring (e.g., somebody with an age of 1000 years). Sometimes, you may come across a distribution of data with a very high (and implausible) frequency count for a particular value. Maybe you measure age, and you have a large number of cases aged 99 (which is often a code used for missing data). Spread: This gives us an idea of the variability of our data. Often, we visualise data because we want to compare distributions. Most data analysis is about making comparisons. We are going to explore whether the distribution of crime in this dataset is different for less affluent areas. The variable medv measures in the Boston dataset the median value of owner-occupied homes. For the purposes of this illustration, I want to dichotomise4 this variable to create a group of towns with particularly low values versus all the others. For further details on how to recode variables with R, you may want to read the relevant sections in Quick R or the R Cookbook. We will learn more about recoding and transforming variables in R soon. How can we create a categorical variable based on information from a quantitative variable? Let’s look at the following code and pay attention to it, as well as the explanation below. Boston$lowval[Boston$medv &lt;= 17.02] &lt;- &quot;Low value&quot; Boston$lowval[Boston$medv &gt; 17.02] &lt;- &quot;Higher value&quot; First, we tell R to create a new vector (lowval) in the Boston data frame. This vector will be assigned the character value “Low value” when the condition within the square brackets is met. That is, we are saying that whenever the value in medv is below 17.02, then the new variable lowval will equal “Low value”. I have chosen 17.02 as this is the first quartile for medv (try this code summary(Boston$medv) and find 17.02). Then we tell R that when the value is greater than 17.02, we will assign those cases to a new textual category called “Higher Value”. The variable we created was a character vector (as we can see if we run the class function). So, we are going to transform it into a factor using the as.factor function (many functions designed to work with categorical variables expect a factor as an input, not just a character vector). If we rerun the class function, we will see we changed the original variable. class(Boston$lowval) ## [1] &quot;character&quot; Boston$lowval &lt;- as.factor(Boston$lowval) class(Boston$lowval) ## [1] &quot;factor&quot; Now, we can produce the plot. We will do this using facets. Facets are another element of the grammar of graphics, we use it to define subsets of the data to be represented as multiple groups; here, we are asking R to produce two plots defined by the two levels of the factor we just created. ggplot(Boston, aes(x = crim)) + geom_histogram(binwidth = 1) + facet_grid(lowval ~ .) Visually, this may not look great, but it begins to tell a story. We can see that there is a considerably lower proportion of towns with low levels of crime in the group of towns that have cheaper homes. It is a flatter, less skewed distribution. You can see how the facet_grid() expression is telling R to create the histogram of the variable mentioned in the ggplot function for the groups defined by the categorical input of interest (the factor lowval). We could do a few things that may perhaps help to emphasise the comparison, such as adding colour to each of the groups. ggplot(Boston, aes(x = crim, fill = lowval)) + geom_histogram(binwidth = 1) + facet_grid(lowval ~ .) + theme(legend.position = &quot;none&quot;) The fill argument within the aes is telling R what variable to assign colours. Now, each of the levels (groups) defined by the lowval variable will have a different colour. The theme statement that we add is telling R not to place a legend in the graphic explaining that red is a higher value and the greenish colour is a low value. We can already see that without a label. Instead of using facets, we could overlay the histograms with a bit of transparency. Transparencies work better when projecting on screens than in printed documents, so keep in mind this when deciding whether to use them instead of facets. The code is as follows: ggplot(Boston, aes(x = crim, fill = lowval)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) In the code above, the fill argument identifies again the factor variable in the dataset grouping the cases. Also, position = identity tells R to overlay the distributions, and alpha asks for the degree of transparency, so a lower value (e.g., 0.2) will be more transparent. In this case, part of the problem we have is that the skew can make it difficult to appreciate the differences. When you are dealing with skewed distributions such as this, it is sometimes convenient to use a transformation 5. We will come back to this later this semester. For now, it suffices to say that taking the logarithm of a skewed variable helps to reduce the skew and to see patterns more clearly. In order to visualise the differences here a bit better, we could ask for the logarithm of the crime per capita rate. Notice how I also add a constant of 1 to the variable crim; this is to avoid NA values in the newly created variable if the value in crim is zero (you cannot take the log of 0). ggplot(Boston, aes(x = log10(crim + 1), fill = lowval)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) The plot now is a bit clearer. It seems pretty evident that the distribution of crime is quite different between these two types of towns. 3.5 Visualising numerical variables: Density plots For smoother distributions, you can use a density plot. You should have a healthy amount of data to use these, or you could end up with a lot of unwanted noise. Let’s first look at the single-density plot for all cases. Notice all we are doing is invoking a different kind of geom: ggplot(Boston, aes(x = crim)) + geom_density() In a density plot, we attempt to visualize the underlying probability distribution of the data by drawing an appropriate continuous curve. So, in a density plot then, the area under the lines sums to 1 and the Y, vertical, axis now gives you the estimated (guessed) probability for the different values in the X, horizontal, axis. This curve is guessed from the data, and the method we use for this guessing or estimation is kernel density estimation. You can read more about density plots here. In this plot, we can see that there is a high estimated probability of observing a town with near zero per capita crime rate and a low estimated probability of seeing towns with large per capita crime rates. As you can observe, it provides a smoother representation of the distribution (as compared to the histograms). You can also use this to compare the distribution of a quantitative variable across the levels in a categorical variable (factor), and, as before, it is possibly better to take the log of skewed variables such as crime: #We are mapping &quot;lowval&quot; as the variable colouring the lines ggplot(Boston, aes(x = log10(crim + 1), colour = lowval)) + geom_density() Or you could use transparencies: ggplot(Boston, aes(x = log10(crim + 1), fill = lowval)) + geom_density(alpha = .3) Did you notice the difference in the comparative histograms? By using density plots, we are rescaling to ensure the same area is used for each of the levels in our grouping variable. This makes it easier to compare two groups that have different frequencies. The areas under the curve add up to 1 for both of the groups, whereas in the histogram, the area within the bars represents the number of cases in each of the groups. If you have many more cases in one group than the other it may be difficult to make comparisons or to clearly see the distribution for the group with fewer cases. So, this is one of the reasons why you may want to use density plots. Density plots are a good choice when you want to compare up to three groups. If you have many more groups, you may want to consider other alternatives. One such alternative is the ridgeline plot, also often called the Joy Division plot (since it was immortalised in the cover of one of their albums): They can be produced with the ggridges package. Before we dichotomise the variable medv manually, we can use more direct ways of splitting numerical variables into various categories using information already embedded in them. Say we want to split medv into deciles. We could use the mutate function in dplyr for this. library(dplyr) Boston &lt;- mutate(Boston, dec_medv = ntile(medv, 10)) The mutate function adds a new variable to our existing data frame object. We are naming this variable dec_medv because we are going to split medv into ten groups of equal size (this name is arbitrary; you may call it something else). To do this, we will use the ntile function as an argument within mutate. We will define the new dec_medv variable, explaining to R that this variable will be the result of passing the ntile function to medv. So that ntile breaks medv into 10, we pass this value as an argument to the function. So that the result of executing mutate is stored, we assign this to the Boston object. Check the results: table(Boston$dec_medv) ## ## 1 2 3 4 5 6 7 8 9 10 ## 51 51 51 51 51 51 50 50 50 50 We can now use this new variable to illustrate the use of the ggridge package. First, you will need to install this package and then load it. You will see all this package does is extend the functionality of ggplot2 by adding a new type of geom. Here, the variable defining the groups needs to be a factor, so we will tell ggplot to treat dec_medv as a factor using as.factor. Using as.factor in this way saves us from having to create yet another variable that we are going to store as a factor. Here, we are not creating a new variable; we are just telling R to treat this numeric variable as if it were a factor. Make sure you understand this difference. library(ggridges) ggplot(Boston, aes(x = log10(crim + 1), y = as.factor(dec_medv))) + geom_density_ridges() We can see that the distribution of crime is particularly different when we focus on the three deciles with the lowest level of income. For more details on this kind of plot, you can read the vignette for this package. 3.6 Visualising numerical variables: Box plots Box plots are an interesting way of presenting the 5-number summary (the minimum value, the first quartile, the median, the third quartile, and the maximum value of a set of numbers) in a visual way. If we want to use ggplot to plot a single numerical variable, we need some convoluted code since ggplot assumes you want a boxplot to compare various groups. Therefore, we need to set some arbitrary value for the grouping variable, and we may also want to remove the x-axis tick markers and label. For this illustration, I am going to display the distribution of the median value of property in the various towns instead of crime. ggplot(Boston, aes(x = 1, y = medv)) + geom_boxplot() + scale_x_continuous(breaks = NULL) + #removes the tick markers from the x axis theme(axis.title.x = element_blank()) Boxplots, however, really come to life when you use them to compare the distribution of a quantitative variable across various groups. Let’s look at the distribution of log(crime) across cheaper and more expensive areas: ggplot(Boston, aes(x = lowval, y=log10(crim + 1))) + geom_boxplot() With a boxplot like this, you can see straight away that the bulk of cheaper areas are very different from the bulk of more expensive areas. The first quartile of the distribution for low areas just about matches the point at which we start to see outliers for the more expensive areas. This can be even more helpful when you have various groups. Let’s try an example using the BCS0708 data frame. This is a dataset from the 2007/08 British Crime Survey. You can download it using the code below. #We create a data frame object reading the data from the webaddress.csv file BCS0708&lt;-read.csv(&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/BCS0708.csv&quot;) This dataset contains a quantitative variable that measures the level of worry about crime (tcviolent): high scores represent high levels of worry. We are going to see how the score in this variable changes according to ethnicity (ethgrp2). #A comparative boxplot of ethnicity and worry about violent crime ggplot(BCS0708, aes(x = ethgrp2, y = tcviolent)) + geom_boxplot() Nice. But it could be nicer. To start with, we could order the groups along the X-axis so that the ethnic groups are positioned according to their level of worry. Secondly, we may want to exclude information on ethnicity for the NA cases (represented by a flat line). #A nicer comparative boxplot (excluding NA and reordering the X variable) ggplot(filter(BCS0708, !is.na(ethgrp2) &amp; !is.na(tcviolent)), aes(x = reorder(ethgrp2, tcviolent, FUN = median), y = tcviolent)) + geom_boxplot() The filter function from dplyr uses a logical argument to tell R to only use the cases that do not have NA values in the two variables that we are using. The exclamation mark followed by is.na and then the name of a variable is R way of saying “the contrary of is NA for the specified variable”. So, in essence, we are saying to R to just look at data that is not NA in these variables. The reorder function, on the other hand, asks R to reorder the levels of ethnicity according to the median value of worry of violent crime. Since we are using those functions within the ggplot function, this subsetting and this reordering (as with as.factor earlier) are not introducing permanent changes in your original dataset. If you prefer to reorder according to the mean, you only need to change that parameter after the FUN option (e.g., FUN = mean). 3.7 Exploring relationships between two quantitative variables: scatterplots So far we have seen how you can use histograms, density plots and boxplots to plot numerical variables and to compare groups in relation to numerical variables. Another way of saying that is that you can use comparative histograms, density plots, or boxplots to assess the relationship between a numerical variable and a categorical variable (the variable that defines the groups). How do you explore the relationship between two numerical variables? When looking at the relationship between two quantitative variables, nothing beats the scatterplot. This is a lovely article in the history of the scatterplot. A scatterplot plots one variable on the Y-axis and another on the X-axis. Typically, if you have a clear outcome or response variable in mind, you place it in the Y axis, and you place the explanatory variable in the X axis. This is how you produce a scatterplot with ggplot(): #A scatterplot of crime versus median value of the properties ggplot(Boston, aes(x = medv, y = crim)) + geom_point() Each point represents a case in our dataset, and the coordinates attached to it in this two-dimensional plane are given by their value in the Y (crime) and X (median value of the properties) variables. What do you look for in a scatterplot? You want to assess global and local patterns, as well as deviations. We can clearly see that at low levels of medv, there is a higher probability in this data that the level of crime is higher. Once the median value of the property hits $30,000, the crime rate will be nearly zero for all towns. So far, it’s so good and surely predictable. The first reason why we look at scatterplots is to check our hypothesis (e.g., poorer areas, more crime). However, something odd seems to be going on when the median property value is around $50,000. All of a sudden, the variability in crime increases. We seem to have some of the more expensive areas also exhibiting some fairly decent level of crime. In fact, there is quite a break in the distribution. What’s going on? To be honest, I have no clue. However, the pattern at the higher level of property value is indeed odd; it is just too abrupt to be natural. This is the second reason why you want to plot your data before you do anything else. It helps you to detect apparent anomalies. I say this is an anomaly because the break-in pattern is quite noticeable and abrupt. It is hard to think of a natural process that would generate this sudden radical increase in crime once the median property value reaches the 50k dollars mark. If you were analysing this for real, you would want to know what’s really driving this pattern (e.g., find out about the original data collection, the codebook, etc.): perhaps the maximum median value was capped at 50K dollars, and we are seeing this as a dramatic increase when the picture is more complex? For now, we are going to let this rest. One of the things you may notice with a scatterplot is that even with a smallish dataset such as this, with just about 500 cases, overplotting may be a problem. When you have many cases with similar (or, even worse, the same) values, it is difficult to tell them apart. Imagine there is only 1 case with a particular combination of X and Y values. What do you see? A single point. Then imagine you have 500 cases with that same combination of values for X and Y. What do you see? Still a single point. There are a variety of ways to deal with overplotting. One possibility is to add some transparency to the points: ggplot(Boston, aes(x = medv, y = crim)) + geom_point(alpha=.4) #you will have to test different values for alpha Why this is an issue may be more evident with the BCS0708 data. Compare the two plots: ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point() ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point(alpha=.2) The second plot gives us a better idea of where the observations seem to concentrate in a way that we could not see with the first. Overplotting can occur when a continuous measurement is rounded to some convenient unit. This has the effect of changing a continuous variable into a discrete ordinal variable. For example, age is measured in years, and body weight is measured in pounds or kilograms. Age is a discrete variable; it only takes integer values. That’s why you see the points lined up in parallel vertical lines. This also contributes to the overplotting in this case. One way of dealing with this particular problem is by jittering. Jittering is the act of adding random noise to data in order to prevent overplotting in statistical graphs. In ggplot, one way of doing this is by passing an argument to geom_point specifying you want to jitter the points. This will introduce some random noise so that age looks less discrete. ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point(alpha=.2, position=&quot;jitter&quot;) #Alternatively, you could replace geom_point() with geom_jitter(), #in which case you don&#39;t need to specify the position Another alternative for solving overplotting is to bin the data into rectangles and map the density of the points to the fill of the colour of the rectangles. ggplot(BCS0708, aes(x = age, y = tcviolent)) + stat_bin2d() #The same but with nicer graphical parameters ggplot(BCS0708, aes(x = age, y = tcviolent)) + stat_bin2d(bins=50) + #by increasing the number of bins we get more granularity scale_fill_gradient(low = &quot;lightblue&quot;, high = &quot;red&quot;) #change colors What this is doing is creating boxes within the two-dimensional plane, counting the number of points within those boxes, and attaching a colour to the box in function of the density of points within each of the rectangles. When looking at scatterplots, sometimes it is useful to summarise the relationships by means of drawing lines. You could, for example, add a line representing the conditional mean. A conditional mean is simply the mean of your Y variable for each value of X. Let’s go back to the Boston dataset. We can ask R to plot a line connecting these means using geom_line() and specifying you want the conditional means. ggplot(Boston, aes(x = medv, y = crim)) + geom_point(alpha=.4) + geom_line(stat=&#39;summary&#39;, fun.y=mean) ## Warning in geom_line(stat = &quot;summary&quot;, fun.y = mean): Ignoring unknown ## parameters: `fun.y` With only about 500 cases, there are loads of ups and downs. If you have many more cases for each level of X, the line would look less rough. You can, in any case, produce a smoother line using geom_smooth instead. We will discuss later this semester how this line is computed (although you will see the R output tells you we are using something called the “loess” method). For now, just know that it is a line that tries to estimate, to guess, the typical value for Y for each value of X. ggplot(Boston, aes(x = medv, y = crim)) + geom_point(alpha=.4) + geom_smooth(colour=&quot;red&quot;, size=1, se=FALSE) #We&#39;ll explain later this semester what the se argument does; #colour is simply asking for a red line instead of blue #(which I personally find harder to see). #I&#39;m also making the line a bit thicker with size 1. As you can see here, you produce a smoother line than with the conditional means. The line, as the scatterplot, seems to be suggesting an overall curvilinear relationship that almost flattens out once property values hit $20k. 3.8 Scatterplots conditioning in a third variable There are various ways to plot a third variable in a scatterplot. You could go 3D, and in some contexts, that may be appropriate. But more often than not, it is preferable to use only a two-dimensional plot. If you have a grouping variable, you could map it to the colour of the points as one of the aesthetics arguments. Here, we return to the Boston scatterplot but will add a third variable, which indicates whether the town is located by the river or not. # Scatterplot with two quantitative variables and a grouping variable, # We are telling R to tell &quot;chas&quot;, a numeric vector, as a factor. ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() Curiously, we can see that there are quite a few of those expensive areas with high levels of crime that seem to be located by the river. Maybe that is a particularly attractive area? As before, you can add smooth lines to capture the relationship. What happens now, though, is that ggplot will produce a line for each of the levels in the categorical variable grouping the cases: ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point(alpha=.4) + #I am making the points semi-transparent to see the lines better geom_smooth(se=FALSE, size=1) #I am doing the lines thicker to see them better You can see how the relationship between crime and property values is more marked for areas not bordering the river, mostly because you have considerably fewer cheaper areas bordering the river. Notice as well the upward trend in the green line at high values of medv. As we saw, there seem to be quite a few of those particularly more expensive areas that have high crime and seem to be by the river. We can also map a quantitative variable to the colour aesthetic. When we do that, instead of different colours for each category we have a gradation in colour from darker to lighter depending on the value of the quantitative variable. Below, we display the relationship between crime and property values conditioning on the status of the area (high values in lstat represent lower status). ggplot(Boston, aes(x = medv, y = crim, colour = lstat)) + geom_point() As one could predict lstat and medv seem to be correlated. The areas with low status tend to be the areas with cheaper properties (and more crime), and the areas with higher status tend to be the areas with more expensive properties (and less crime). You could map the third variable to a different aesthetic (rather than colour). For example, you could map lstat to the size of the points. This is called a bubblechart. The problem with this, however, is that it can sometimes cause overplotting to become more acute. ggplot(Boston, aes(x = medv, y = crim, size = lstat)) + geom_point() #You may want to add alpha for some transparency here. If you have larger samples and the patterns are not clear (as we saw when looking at the relationship between age and worry of violent crime), conditioning in a third variable can produce hard-to-read scatterplots (even if you use transparencies and jittering). Let’s look at the relationship between worry of violent crime and age conditioned on victimisation during the previous year: ggplot(BCS0708, aes(x = age, y = tcviolent, colour = bcsvictim)) + geom_point(alpha=.4, position=&quot;jitter&quot;) You can possibly notice that there are more green points on the left-hand side (since victimisation tends to be more common among youth). But it is hard to read the relationship with age. Could we try to use facets instead (hint: facet_grid)? ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point(alpha=.4, position=&quot;jitter&quot;) + facet_grid( .~ bcsvictim) It is still hard to see anything, though perhaps you can notice the lower density of the points in the bottom right corner in the facet displaying victims of crime. In a case like this, it may be helpful to draw a smooth line. ggplot(BCS0708, aes(x = age, y = tcviolent, colour = bcsvictim)) + geom_point(alpha=.1, position=&quot;jitter&quot;) + geom_smooth(size=1.5, se=FALSE) What we see here is that, for the most part, the relationship between age and worry of violent crime looks quite flat, regardless of whether you have been a victim of crime or not, at least for most people. However, once we get to the 60s, things seem to change a bit. Those over 62 who have not been a victim of crime in the past year start to manifest a lower concern with crime as they age (in comparison with those who have been a victim of crime). 3.9 Scatterplot matrix Sometimes, you want to produce many scatterplots simultaneously to have a first peak at the relationship between the various variables in your data frame. The way to do this is by using a scatterplot matrix. There are some packages that are particularly good for this. One of them is GGally, basically an extension for ggplot2. Not to overcomplicate things, we will only use a few variables from the Boston dataset: #I create a new data frame that only #contains 4 variables included in the Boston dataset, #and I am calling this new data frame object Boston_spm Boston_spm &lt;- dplyr::select(Boston, crim, medv, lstat) Then we load GGally) and run the scatterplot matrix using the ggpairs function: library(GGally) ggpairs(Boston_spm) The diagonal set of boxes that go from the top left to the bottom right gives you the univariate density plot for each of the variables. So, for example, at the very top left, you have the density plot for the crim variable. If you look underneath this one, you see a scatterplot between crim and medv. In this case, crim defines the X axis and medv the Y axis, which is why it looks a bit different from the one we saw earlier. The labels at the top and the left tell you what variables are plotted in each faceted rectangle. In the top right-hand side of this matrix, you see that the rectangles say “corr” and then give you a number. These numbers are correlation coefficients, which are a metric we use to indicate the strength of a relationship between two quantitative or numeric variables. The closer to 1.000 (whether positive or negative) this value is, the stronger the relationship is. The closer to zero, the weaker the relationship. The stronger relationship here is between medv and istat (correlation coefficient value: -0.738). The fact that it is negative indicates that as the values in one increase, the values in the other tend to decrease. So, high crime values correspond to low property prices, as we saw earlier. This coefficient is a summary number of this relationship. We will come back to it later on. For now, keep in mind this metric only works well if the relationship shown in the scatterplot is well represented by a straight line. If the relationship is curvilinear, it will be a very bad metric that you should not trust. R gives you a lot of flexibility, and there are often competing packages that aim to do similar things. So, for example, for a scatterplot matrix, you could also use the spm function from the car package. library(car) #The regLine argument is used to avoid displaying #something we will cover in regression analysis. spm(Boston_spm, regLine=FALSE) This is a bit different from the one above because rather than displaying the values of the correlation coefficient, you get another set of scatterplots with the Y and X axes rotated. You can see the matrix is symmetrical. So, the first scatterplot that you see in the top row (second column from the left) shows the relationship between medv (in the X axis) and crim (in the Y axis). This is the same relationship shown in the first scatterplot in the second row (first column); only here, crim defines the X axis and medv the Y axis. In this scatterplot, you can see, although not very well, that smoothed lines representing the relationship have been added to the plots. library(car) spm(Boston_spm, smooth=list(col.smooth=&quot;red&quot;), regLine=FALSE) You can also condition in a third variable. For example, we could condition on whether the areas bound the Charles River (variable chas). Boston_spm &lt;- dplyr::select(Boston, crim, medv, lstat, chas) spm(~crim+medv+lstat, data=Boston_spm, groups=Boston_spm$chas, by.groups=TRUE, smooth=FALSE, regLine=FALSE) Getting results, once you get the knack of it, is only half of the way. The other, and more important, half is trying to make sense of the results. What are the stories this data is telling us? R cannot do that for you. For this, you need to use a better tool: your brain (scepticism, curiosity, creativity, a lifetime of knowledge) and what Kaiser Fung calls “numbersense”. 3.10 Titles, legends, and themes in ggplot2 We have introduced a number of various graphical tools, but what if you want to customise the way the produced graphic looks like? Here I am just going to give you some code for how to modify the titles and legends you use. To add a title to a ggplot graph, you use ggtitle(). #Notice how here we are using an additional function #to ask R to treat the variable chas, which is numeric #in our dataset, as if it were a factor (as.factor()). #You need to do this if your variable is categorical #but is encoded as numeric in your data frame. ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) If you don’t like the default background theme for ggplot you can use a theme as discussed at the start, for example, creating a black and white background by adding theme_bw() as a layer: ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) + theme_bw() As we said earlier, ggthemes gives you additional themes you can use. For example, you can use the style inspired by The Economist magazine. library(ggthemes) ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) + theme_economist() Using labs(), you can change the text of axis labels (and the legend title), which may be handy if your variables have cryptic names. Equally, you can manually name the labels in a legend. The values for chas are 0 and 1. This is not informative. We can change that. ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) + labs(x = &quot;Median Property Value (in US Dollars x 1000)&quot;, y = &quot;Per capita crime rate&quot;, colour = &quot;Borders the river&quot;) + scale_colour_discrete(labels = c(&quot;No&quot;, &quot;Yes&quot;)) Sometimes, you may want to present several plots together. For this, the gridExtra package is very good. You will first need to install it and then load it. You can then create several plots and put them all in the same image. #You may need to install it first with install.packages(&quot;gridExtra&quot;) library(gridExtra) #Store your plots in various objects p1 &lt;- qplot(x=crim, data=Boston) p2 &lt;- qplot(x=indus, data=Boston) p3 &lt;- qplot(x=medv, data=Boston) #Then, put them all together using grid.arrange() grid.arrange(p1, p2, p3, ncol=3) #ncol tells R we want them side by side; #if you want them one on top of the other, try ncol=1; #in this case, however, ncol=2 would possibly be the better solution. Try it! We don’t have time to get into the details of all the customisation features available for ggplot2. You can find some additional solutions in the cookbook put together by the data journalists at the BBC or in Kieran Healy’s free online book. 3.11 Plotting categorical data: bar charts You may be wondering, ‘What about categorical data’? So far, we have only discussed various visualisations where at least one of your variables is quantitative. When your variable is categorical, you can use bar plots (similar to histograms). We map the factor variable in the aesthetics and then use the geom_bar() function to ask for a bar chart. ggplot(BCS0708, aes(x=walkday)) + geom_bar() You can see the label count on the Y-axis. This is not a variable in your data. When you run a geom_bar like this you are invoking as a hidden default a call to the stat_ function. In this case, what is happening is that this function is counting the number of cases in each of the levels of walkday, and this count is what is used to map the height of each of the bars. The stat_ function, apart from counting the cases in each level, computes their relative frequency and proportion and stores this information in a temporal variable called ..prop... If we want this variable to be represented in the Y-axis, we can change the code as shown below: ggplot(BCS0708, aes(x=walkday)) + geom_bar(mapping = aes(y = ..prop..)) ## Warning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(prop)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. As Kieran Helay (2018) indicates: “The resulting plot is still not right. We no longer have a count on the y-axis, but the proportions of the bars all have a value of 1, so all the bars are the same height. We want them to sum to 1 so that we get the number of observations per” (level) “as a proportion of the total number of observations. This is a grouping issue again… we need to tell ggplot to ignore the x-categories when calculating the denominator of the proportion and use the total number of observations instead. To do so, we specify group = 1 inside the aes() call. The value of 1 is just a kind of “dummy group” that tells ggplot to use the whole dataset when establishing the denominator for its prop calculations.” ggplot(BCS0708, aes(x=walkday)) + geom_bar(mapping = aes(y = ..prop.., group = 1)) Unfortunately, the levels in this factor are ordered in alphabetical order, which is confusing. We can modify this by reordering the factors’ levels first – click here for more details. You could do this within the ggplot function (just for the visualisation), but in real life, you would want to sort out your factor levels in an appropriate manner more permanently. As discussed last week, this is the sort of thing you do as part of pre-processing your data. And then plot. #Print the original order print(levels(BCS0708$walkday)) ## NULL #Reordering the factor levels from very safe #to very unsafe (rather than by alphabet). #Notice that I am creating a new variable; #it is often wise to do this to avoid #messing up your original data. BCS0708$walkdayR &lt;- factor(BCS0708$walkday, levels=c(&#39;very safe&#39;, &#39;fairly safe&#39;,&#39;a bit unsafe&#39;,&#39;or very unsafe&#39;)) #Plotting the variable again (and subsetting out the NA data) ggplot(subset(BCS0708, !is.na(walkdayR)), aes(x=walkdayR)) + geom_bar(mapping = aes(y = ..prop.., group = 1)) We can also map a second variable to the aesthetics; for example, let’s look at ethnicity in relation to feelings of safety. For this, we produce a stacked bar chart. bcs_bar &lt;-filter(BCS0708, !is.na(walkdayR), !is.na(ethgrp2)) ggplot(data=bcs_bar, aes(x=walkdayR, fill=ethgrp2)) + geom_bar() These types of stacked bar charts are not terribly helpful if you are interested in understanding the relationship between these two variables. It is hard to see if any group is proportionally more likely to feel safe or not. Instead, what you want is a different kind of stacked bar chart that gives you the proportion of your “explanatory variable” (ethnicity) within each of the levels of your “response variable” (here, feelings of safety). ggplot(data=bcs_bar, aes(x=walkdayR, fill=ethgrp2)) + geom_bar(position = &quot;fill&quot;) Now, we can more easily compare proportions across groups since all the bars have the same height. However, it is more difficult to see how many people there are within each level of the X variable. p &lt;- ggplot(data=bcs_bar, aes(x=walkdayR, fill=ethgrp2)) + geom_bar(position = &quot;dodge&quot;, mapping = aes(y = ..prop.., group = ethgrp2)) p Now, we have a bar chart where the values of ethnicity are broken down across the levels of fear of crime, with a proportion showing on the y-axis. Looking at the bars, you will see that they do not sum to one within each level of fear. Instead, the bars for any particular ethnicity sum to one across all the levels of fear. You can see, for example, that nearly 75% of the White respondents are in the “Very Safe” level, whereas, for example, less than 60% of the Black respondents feel “Very Safe”. Sometimes, you may want to flip the axis so that the bars are displayed horizontally. You can use the coord_flip() function for that. #First, we invoke the plot we created and stored earlier, #and then we add an additional specification with coord_flip() p + coord_flip() You can also use coord_flip() with other ggplot plots (e.g., boxplots). A particular type of bar chart is the divergent stacked bar chart, often used to visualise Likert scales. You may want to look at some of the options available for it via the HH package or sjPlot. But we won’t cover them here in detail. Keep in mind that knowing how to get R to produce a particular visualisation is only half the job. The other half is knowing when to produce a particular kind of visualisation. This blog, for example, discusses some of the problems with stacked bar charts and the exceptional circumstances in which you may want to use them. There are other tools sometimes used for visualising categorical data. Pie charts are one of them. However, as mentioned at the beginning, many people advocate strongly against using pie charts, and therefore, this is the only pie chart you will see in this course: cute pie chart What I would use instead are waffle charts. They’re super easy to make with the “waffle” package, but I don’t think there’s time for them at this point but look into them here. 3.12 Further resources By now, you should know the path to data analysis, but wisdom will take time. The good news is that we live in a time where there are multiple (very often free) resources to help you along the way. The time when this knowledge was just the preserve of a few is long distant. If you are a motivated and disciplined person, there is a lot that you can do to consolidate and expand your data visualisation skills further without spending money. I have already recommended the excellent ggplot2 online documentation to you. Here, we just want to point you to a few useful resources that you can pursue in the future. First MOOCS (Massive Online Open Courses). There are loads of useful MOOCS that provide training in data visualisation for free (well, you can pay if you want a certificate, but you can also use the resources and learn for free). Second, online tutorials. One of the things you will also soon discover is that R users are keen to write “how to” guides in some of the 750 R devoted blogs or as part of Rpubs. Using Google or similar, you will often find solutions to almost any problem you will encounter using R. For example, in this blog, there are a few tutorials that you may want to look to complement my own: Cheatsheet for visualising scatterplots Cheatsheet for visualizing distributions Cheatsheet for barplots Third, blogs on data visualisation. If you use Feedly or a similar blog aggregator, you should add the following blogs to your reading list. They are written by leading people in the field and are full of useful advice: + Flowing Data + Visual Business Intelligence + chartsnthings + Data Revelations Fourth, resources for visualisations we don’t have the time to cover. R is way ahead of some of the more popular data analysis programs you may be more familiar with (e.g. SPSS or Excel). There are many things you can do. For example, if you like maps, R can also be used to produce visualisations of spatial data. There are various resources to learn how to do this, and we will teach this in our Crime Mapping module in the third year. 3.13 Summary This week, we used the following R functions: import data: ‘readr’ package read_csv() explore data names() clean data: janitor package clean_names() visualise data ggplot() geom_bar() geom_point() geom_histogram() geom_density() geom_hline() geom_boxplot() facet_warp() facet_grid transform a variable into a factor variable as_factor() create a categorical variable using a numeric variable Boston\\(lowval[Boston\\)medv &lt;= 17.02] &lt;- “Low value” Tufte, Edward (2001) The visual display of quantitative information. 2nd Edition. Graphic Press.↩︎ Few, Stephen (2012) Show me the numbers: designing graphics and tables to enlighten. 2nd Edition. Analytics Press.↩︎ Cairo, Alberto (2016) The truthful art: data, charts, and maps for communication. New Riders.↩︎ Split into two groups.↩︎ This is an interesting blog entry on solutions when you have highly skewed data.↩︎ "],["refresher-on-descriptive-statistics-data-carpentry.html", "Chapter 4 Refresher on descriptive statistics &amp; data carpentry 4.1 Introduction 4.2 Getting some data from Eurobarometer 4.3 Thinking about your data: filtering cases 4.4 Selecting variables: using dplyr::select 4.5 Creating summated scales 4.6 Collapsing categories in character variables 4.7 Working with apparently cryptic variable names and levels 4.8 Recoding factors 4.9 Understanding missing data 4.10 Exploring data frames visually 4.11 A quick recap on descriptive statistics 4.12 Further resources 4.13 Summary", " Chapter 4 Refresher on descriptive statistics &amp; data carpentry 4.1 Introduction We have already introduced the typical workflow of data analysis. In this figure produced by Hadley Wickham, you can see the first stage of data analysis involves importing your data, getting it into a tidy format, and then doing some transformations so that you get the data in good shape for analysis. There is a famous and possibly false statistic that says that 80% of an analyst’s time is often devoted to these kinds of operations. Although the statistic is likely made up, the truth is that the experience of many analysts resonates with it. So, you should not underestimate data carpentry or data wrangling (as these processes are often called) as a part of your analysis. For various decades, social scientists of a quantitative persuasion worked primarily with survey data (for an excellent history of how this came to be, you can read this book), which came in rather tidy formats but often required some transformations. Today, we are more likely to rely on “big” and other new forms of data (from the web, administrative sources, or a variety of sensors) that may require more significant processing before we can do any analysis with it. Think, for example, of data from online vendors of drugs available in the Dark Net. Some people talk of the advent of a new computational social science around these new methods. This kind of data, indeed, opens avenues for research we could only dream of in the past, as argued by some of our colleagues. But getting this kind of data requires the development of new skills (e.g. web scraping) and generally requires more processing before they are tidy and ready for analysis. R is particularly well suited for this new world. In this module, we only work with survey data, which tends to be tidier and easier to work within the context of an introductory course unit. However, even when working with this kind of data, you often have to think hard about the required tidying and transformations before you can start your analysis. In this module, we expect you to download a survey dataset for analysis. These datasets are already rather tidy and have been professionally cleaned and prepared for analytical consumption. However, you may still have to select cases and variables that are appropriate for your own analysis. Also, you likely will need to generate new variables or change existing ones for various reasons. It’s a rare data set in which every variable you need is measured directly. Examples of things you may need to do include: Combine various variables into a new one (e.g., computing a rate) Reduce the number of levels in a categorical variable Format the variable to assign it to a more appropriate class if this is called for (e.g., character into factor). You will need to format date variables as dates, numerical variables as numbers, etc.). Recode values identifying missing cases as NA. Labelling all variables and categorical values so you don’t have to keep looking them up. Change the labels associated with the levels of a categorical variable. 4.2 Getting some data from Eurobarometer We’re going to go ahead and download some data for the session today, specifically data from a Eurobarometer. Eurobarometers are opinion polls conducted regularly on behalf of the European Commission since 1973. Some of them ask the same questions over time to evaluate changes in European’s views on a variety of subjects (standard Eurobarometers). Others are focused on special topics and are conducted less regularly (special Eurobarometers). They are a useful source of datasets that you could use, for example, for your undergraduate dissertation. The data from these surveys is accessible through the data catalogue of GESIS, a data warehouse at the Leibniz Institute for the Social Sciences in Germany. To download data from GESIS, you have to register with them (following the registration link) here. Once you activate your registration you should be able to access the data at GESIS. Use the You are not yet registered? option to fill in your details. GESIS has a section of their website devoted to the Eurobarometer survey. You can access that section here. You can navigate this webpage to find the data we will be using for this tutorial, but to save time, I will provide you a direct link to it. We will be using the special Eurobarometer 85.3 from 2016. This survey, among other things, asked Europeans about their views on gender violence. You can access the data for this Eurobarometer here. You will see here that there are links to the files with the data in SPSS and STATA format. You can also see a tab where you can obtain the questionnaire for the survey. Once you are registered, download the STATA (.dta) version of the file and the English version of the questionnaire. Make sure you place the file in your working directory/ datasets folder. Once you do this, you should be able to use the code we are using in this session. First, we will load the data into our session. Since the data is in STATA format, we will need to read the data into R using the haven package. Specifically, we will use the read_dta() function to import STATA data into R. As an argument, we need to write the name of the file with the data (and if it is not in your working directory, the appropriate path file). library(haven) #option 1: import from your local computer eb85_3 &lt;- read_dta(&quot;datasets/ZA6695_v2-0-0.dta&quot;) dim(eb85_3) ## [1] 27818 483 Alternatively, you can import the file from the website where we keep the data: #option 2: import from the website library(haven) eb85_3 &lt;- read_dta(&quot;https://www.dropbox.com/s/sul9ezr4k9aua70/ZA6695_v2-0-0.dta?dl=1&quot;) dim(eb85_3) ## [1] 27818 483 We can see there are 27818 cases (survey participants) and 483 variables. 4.3 Thinking about your data: filtering cases Imagine that we needed to write a report about attitudes to sexual violence. First, we would need to think about whether we wanted to use all cases in the data or only a subset of the cases. For example, when using something like the Eurobarometer, we would need to consider whether we are interested in exploring the substantive topic across Europe or only for some countries. Alternatively, you may want to focus your analysis only on men’s attitudes to sexual violence. In a situation like this, you would need to filter cases. This decision needs to be guided by your theoretical interests and your driving research question. Here is a video about Eurobarometer 2018. It’s not a fancy video, but if anyone wants to know about Eurobarometer within a minute, have a look. So, for example, if we only wanted to work with the UK sample, we would need to figure out if there is a variable that identifies the country in the dataset. To know this, we need to look at the codebook (sometimes called data dictionary). You can access this facility in the link highlighted in the image below: Let’s explore the codebook for the Eurobarometer. If we expand the menus on the left-hand side by clicking on “Explanation of the variable documentation”, you will see country codes. The name of this variable, as it will appear in the dataset, is isocntry, and we can see this variable uses ISO 3166 codes to designate the country. This is an international standard set of abbreviations for country names. For the UK these codes are “GB-GBN” and “GB-NIR” (for Northern Ireland). Now that we have this information, we can run the code to select only the cases that have these values in the dataset. To do something like that, we would use the dplyr::filter function. We used the filter function in week 2. You can read more about it in the dplyr vignette. library(dplyr) #First, let&#39;s see what type of vector isocntry is class(eb85_3$isocntry) ## [1] &quot;character&quot; uk_eb85_3 &lt;- filter(eb85_3, isocntry %in% c(&quot;GB-GBN&quot;, &quot;GB-NIR&quot;)) The variable isocntry is a character vector with codes for the different participating countries. Here, we want to select all the cases in the survey that have either of two values in this vector (GB-GBN or GB-NIR). Since these values are text, we need to use quotes to wrap them up. Because we are selecting more than one value, we cannot simply say isocntry == \"GB-BGN\". We also need the cases from Northern Ireland. So, we use a particular operator introduced by dplyr called a built-in infix operator (%in%). The %in% operator is essentially saying to R ‘returns logical vector’ if there is a match, or not, for its left operand. Basically, here we are creating a vector with the values matching conditions provided in a vector using the c() function, so R to look at those values within the list (containing the right labels) in the isocntry vector so that we can filter everything else out. If you run this code, you will end up with a new object called uk_eb85_3 that only has 1306 observations. We now have a dataset that only has the British participants. 4.4 Selecting variables: using dplyr::select Perhaps, for the sake of this example, we decided to do an analysis that focuses on looking at attitudes toward sexual violence for all of Europe and for all participants. Yet, you won’t be using 483 variables for certain. Among other reasons because our guidelines for the essay suggest you use fewer variables. But more generally, because, typically, your theoretical model will tell you that some things matter more than others. The first thing you need to do is to think about what variables you are going to use. This involves first thinking about what variables are available in the dataset that measure your outcome of interest but then also considering what your theory of attitudes to gender violence says (this generally will include things that are not measured in the survey, such is life!). For the sake of this exercise, we are assuming the thing you are interested in explaining or better understanding is attitudes regarding sexual violence. So, before anything else, you would need to spend some time thinking about how this survey measures these attitudes. You would need to screen the questionnaire and the codebook to identify these variables and their names in the dataset. Have a look at the questionnaire. The questions about gender violence start at the bottom of page 7. Which of these questions are questions about attitudes towards sexual violence? Once you have all of this, you will need to think about which of these survey questions and items make more sense for your research question. This is something where you will need to use your common sense but also your understanding of the literature on the topic. Criminologists and survey researchers spend a lot of time thinking about the best way of asking questions about topics or concepts of interest. They often debate and write about this. In data analysis, measurement is key and is the process of systematically assigning numbers to objects and their properties, to facilitate the use of mathematics in studying and describing objects and their relationships. So, as part of your essay, you will need to consider what researchers consider are good questions to measure, to tap into, the abstract concepts you are studying. There are many items in this survey that relate to this topic, but for the purpose of continuing our illustration, we are going to focus on the answers to question QB10. This question asks respondents to identify in what circumstances it may be justified to have sexual intercourse without consent. The participants are read a list of items (e.g., “flirting beforehand”) and they can select various of them if so they wish. If you want to look at the codebook again to see how qb10_1 is measured, return to the document you downloaded (the codebook) and go to page 384. What name is associated with this variable? Well, you can see that depending on which thing they asked about, it might be qb10_1, qb10_2, qb10_3, etc etc. Damn! We have one question but several variables! This is common when the question in the survey allows for multiple responses. Typically, when this is read into a dataset, survey researchers create a variable for each of the possible multiple responses. If the respondents identified one of those potential responses, they will be assigned a “yes” or a “1” for that column. If they did not, they will be assigned a “no” or a “0”. Let’s see how this was done in this case: class(eb85_3$qb10_1) ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; This is a vector labelled by haven. We could see what labels were used using the attributes function. attributes(eb85_3$qb10_1) ## $label ## [1] &quot;INTERCOURSE W/O CONSENT APPROPRIATE WHEN: WEARING SEXY CLOTHES&quot; ## ## $format.stata ## [1] &quot;%8.0g&quot; ## ## $class ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; ## ## $labels ## Not mentioned Mentioned ## 0 1 We can see here that the value 1 corresponds to cases where this circumstance was mentioned. Let’s see how many people considered this a valid circumstance to have sex without consent. table(eb85_3$qb10_1) ## ## 0 1 ## 24787 3031 Fortunately, only a minority of respondents. Apart from thinking about the variables we will use to measure our outcome of interest, we will need to select some variables that we think may be associated with this outcome. Here, we will only do a few. Again, this is something that needs to be informed by the literature (what variables does the literature consider important) and our own interest. For the sake of illustration, let’s say we are going to look at gender, political background of the respondent, country of origin, age, occupation of the respondents, and type of community they live in. Most of these are demographic variables (not always the more fun or theoretically interesting), but that’s all we have in this Eurobarometer and so they will have to do. In the same way, you try to identify the names of the variables for your outcome variable; you would need to do this for the variables you use to “explain” your outcome. Once you have done your variable selection you can subset your data to only include these. #option 1: select() df &lt;- select(eb85_3, qb10_1, qb10_2, qb10_3, qb10_4, qb10_5, qb10_6, qb10_7, qb10_8, qb10_9, qb10_10, qb10_11, qb10_12, d10, d11, isocntry, d1, d25, d15a, uniqid) Ta-da! We now have a new object called df with only the variables we will be working with. In this format, it is easier to visualise the data. Notice we have also added a variable called uniqid. With many datasets like this, you will have a unique ID value that allows you to identify individuals. This ID may be handy later on, so we will preserve it in our object. If you View this object df, you will notice that the selected variables appear in the order in which you selected them. If you wanted a different arrangement, for example, you may have preferred to have uniqid as your first column, you could have modified the code like so: #option 2: select() df &lt;- select(eb85_3, uniqid, qb10_1, qb10_2, qb10_3, qb10_4, qb10_5, qb10_6, qb10_7, qb10_8, qb10_9, qb10_10, qb10_11, qb10_12, d10, d11, isocntry, d1, d25, d15a) If you want to add a lot of columns, it can save you some typing to have a good look at your data and see whether you can’t get to your selection by using chunks. Since qb10_1 and the others are one after the other in the dataset, we can use the start.col:end.col syntax like the below: #option 3: select() df &lt;- select(eb85_3, uniqid, qb10_1:qb10_12, d10, d11, isocntry, d1, d25, d15a) If you have a lot of columns with a similar structure, you can use partial matching by adding starts_with(), ends_with(), or contains() to your select statement. So, for example, as an alternative to the syntax above, we could use the following: #option 4: select() df &lt;- select(eb85_3, uniqid, starts_with(&quot;qb10&quot;), d10, d11, isocntry, d1, d25, d15a) Notice how the text we pass as an argument goes between double quotes. An alternative is to deselect columns by adding a minus sign in front of the column name. You can also deselect chunks of columns. Don’t execute the code below for this practical, but if you wanted, for example, to get rid of uniqid, you could do the following: #DO NOT RUN THIS. THIS IS AN EXAMPLE. df &lt;- select(df, -uniqid) Yes, a lot of these tips are about saving you some typing. Being lazy (productive, efficient) is fine. 4.5 Creating summated scales Now comes the next part. What are we going to do with these variables? How are we going to use them? Here, you need to do some thinking using your common sense and also consider how other researchers may have used this question about attitudes to sexual violence. There are always many possibilities. We could, for example, consider that we are going to split the sample into two groups: those that consider any of these circumstances valid and those that don’t. We would then end up with a binary indicator that we could use as our outcome variable in our analysis. The thing is that doing that implies losing information. We may think that someone who considers many circumstances as valid is not the same as the person who only considers one as valid. Yet, creating a global binary indicator would treat these two individuals in the same way. Another alternative could be to see how many of these circumstances are considered valid excuses for each individual and to produce a sum then for every respondent. Since there are 9 “excuses”, we could have a sum from 0 to 9. This is a very rough summated scale. You can read more about the proper development of summated scales here. Let’s do this. We are going to create a new variable that adds up the responses to qb10_1 all the way to qb10_9. For this, we use the mutate function from the dplyr package. df &lt;- mutate(df, at_sexviol = qb10_1 + qb10_2 + qb10_3 + qb10_4 + qb10_5 + qb10_6 + qb10_7 + qb10_8 + qb10_9) table(df$at_sexviol) ## ## 0 1 2 3 4 5 6 7 8 9 ## 19681 2529 2117 1643 841 416 255 155 57 124 We have a skewed distribution. Most people (19681 here, right?) in the survey consider that none of the identified circumstances are valid excuses for having sexual intercourse without consent. On the other hand, only a minority of individuals (124) consider that all of these 9 circumstances are valid excuses for having sex without consent. A high score in this count variable tells you that the participant is more likely to accept a number of circumstances in which sexual intercourse without consent is acceptable. You may read more about count data of this kind here. Hold on for a second, though. There is a variable qb10_10 that specifies people that answered “none of these” circumstances. In theory, the number of people with a “1” in that variable (that is, selected this item) should equal 19681 (the number of zeros in our new variable). Let’s check this out: table(df$qb10_10) ## ## 0 1 ## 9400 18418 Oops! Something doesn’t add up! Only 18418 people said that none of these circumstances were valid. So why, when we add all the other items we end up with 19681 rather than 18418? Notice that there is also a qb10_11 and a qb10_12. These two items identify the people who ‘refused’ to answer this question and those who ‘did not know’ how to answer it. table(df$qb10_11) ## ## 0 1 ## 27563 255 table(df$qb10_12) ## ## 0 1 ## 26810 1008 In ‘qb10_11’, there are 255 people that refused to answer and, in ‘qb10_12’, 1008 that did not know how to answer. If you add 1008, 255, and 18418, you get 19681. So our new variable is actually computing as zeroes people that did not know how to answer this question or refused to answer it. We do not want that. We do not know what these people think, so it would be wrong to assume that they consider that none of these circumstances are valid excuses for sexual intercourse without consent. There are many ways to deal with this. We could simply filter out cases where we have values of 1 in these two variables (since we don’t know their answers, we could also get rid of them). But we could also re-code the variable to define these values as what they are: NA (missing data, cases for which we have no valid information). df$at_sexviol[df$qb10_11 == 1 | df$qb10_12 == 1] &lt;- NA table(df$at_sexviol) ## ## 0 1 2 3 4 5 6 7 8 9 ## 18418 2529 2117 1643 841 416 255 155 57 124 Pay attention to the code above. When we want to recode based on a condition, we use something like that. What we are saying with that code is that we are going to assign as “NA” (missing data) the cases for which the condition between the square brackets is met. Those conditions are as defined when the participants answered don’t know or (the logical operator for “or” is “|”) refused to answer the question (that is, when qb10_11 or qb10_12 equals 1). You can see other scenarios for re-coding using this kind of syntax in the resources we link below. You can see other logical operators used in R here. Notice that once we do the recoding and rerun the frequency distribution, you will see that we achieved what we wanted. Now, all those missing cases are not counted as zero. Summated scales like the one we created here are quick and not the most kosher way of combining several questions into a single measure. In research methods, you probably learnt that we often work with theoretical constructs that we do not observe directly (e.g., self-control, anomie, collective efficacy, etc.). We often call these latent variables. In order to study these constructs, we create measures or scales that are based on observed variables (those we actually include and ask in the survey). This is a famous measure of depression often used in research. As you can see we have different questions (our observed variables) that we think are related to our latent or not observed (directly) variable (depression). You can also see there are several items or questions all of which we researchers think are linked to depression in this scale. If you provide positive answers to many of these questions, we may think that you have this unobserved thing we call depression. When measuring latent variables, it is a good idea to have multiple items that all capture aspects of the unobserved variable we are really interested in measuring. There is a whole field of statistics that focuses on how to analyse if your observed variables are good indicators of your unobserved variable (psychometry is how we call this field in psychology) and also that focuses on how to best combine the answers to our observed variables in a single score (latent variable modelling). Some of the scores in the Crime Survey for England and Wales that you may use for your essay have been created with these more advanced methods (some of the measures on confidence in the police, for example). Summated scales are not really the best way to do this. However, these are more advanced topics that are covered in upper-level undergraduate or postgraduate courses. So, for now, we will use summated scales as a convenient, if imperfect, way of aggregating observed variables. 4.6 Collapsing categories in character variables One of the variables we selected is the country in which the participant lives. Let’s have a look at this variable. table(df$isocntry) ## ## AT BE BG CY CZ DE-E DE-W DK EE ES FI ## 1016 1029 1001 501 1060 533 1052 1010 1001 1008 1042 ## FR GB-GBN GB-NIR GR HR HU IE IT LT LU LV ## 1009 1006 300 1000 1026 1046 1002 1013 1004 508 1010 ## MT NL PL PT RO SE SI SK ## 500 1003 1002 1000 1007 1109 1012 1008 There are 30 countries in the sample. You may consider that, for the purposes of your analysis maybe that is too much. For the sake of this tutorial, let’s say that maybe you are not really interested in national differences but in regional differences across different parts of Europe. Say you may want to explore whether these attitudes are different across Western/Central Europe, Scandinavian countries, Mediterranean countries, and Eastern Europe. You do not have a variable with these categories, but since you have a variable that gives you the nations, you could create such a variable. How would you do that? First, you need to consider what the new categories are going to be and how you are going to distribute the countries in your sample across those categories. You may do that on a piece of paper. Then, you would need to write the code to have a new variable with this information. We are going to group the countries into 4 regions: Western (AT, BE, CZ, DE-E, DE-W, FR, GB-GBN, GB-NIR, IE, LU, NL), Eastern (BG, EE, HU, LT, LV, PL, RO, SK), Southern (CY, ES, GR, HR, IT, MT, PT, SI), and Northern Europe (DK, FI, SE). Then you need to figure out what kind of variable we are dealing with here: class(df$isocntry) ## [1] &quot;character&quot; Ok, this is a categorical unordered variable; we know that. But these kinds of variables could be encoded into R as either character vectors, factor variables, or, as we have seen, as well as haven_labelled. How you recode a variable is contingent on how it is encoded. Here we are going to show you how you would do the recoding with a character variable such as isocntry into another character variable we will call region. We will see later examples of how to recode factors. We will have a variable with four new categories (Western, Southern, Eastern, and Northern) whenever the right conditions are met. For this, we can use the case_when() function from the dplyr package. What this does is it goes through every case, and does something when it is true. We wrap this in the mutate() function, which allows us to create a new variable. We’ll call this new variable “region”. First, let’s create four lists, which contain the codes for countries we would like to code Western, Eastern, Northern, and Southern: western_list &lt;- c(&quot;AT&quot;, &quot;BE&quot;, &quot;CZ&quot;, &quot;DE-E&quot;, &quot;DE-W&quot;, &quot;FR&quot;, &quot;GB-GBN&quot;, &quot;GB-NIR&quot;, &quot;IE&quot;, &quot;LU&quot;, &quot;NL&quot;) eastern_list &lt;- c(&quot;BG&quot; , &quot;EE&quot;, &quot;HU&quot;, &quot;LT&quot; , &quot;LV&quot; , &quot;PL&quot; , &quot;RO&quot;, &quot;SK&quot;) northern_list &lt;- c(&quot;DK&quot;, &quot;FI&quot;, &quot;SE&quot;) southern_list &lt;- c(&quot;CY&quot;, &quot;ES&quot;, &quot;GR&quot;, &quot;HR&quot; , &quot;IT&quot; , &quot;MT&quot;, &quot;PT&quot;, &quot;SI&quot;) Now that we have these lists, our condition will be to evaluate whether the value for each row for the isocntry variable falls within one of these, and when this is the case, then code if appropriately: df &lt;- df %&gt;% mutate(region = case_when(isocntry %in% western_list ~ &quot;Western&quot;, isocntry %in% eastern_list ~ &quot;Eastern&quot;, isocntry %in% northern_list ~ &quot;Northern&quot;, isocntry %in% southern_list ~ &quot;Southern&quot;) ) table(df$region) ## ## Eastern Northern Southern Western ## 8079 3161 7060 9518 What we are doing above is initialising a new variable in the df object that we are calling region. We then assign to each of the four categories in this character vector those participants in the survey who have the corresponding values in the isocntry variable as defined for each category. So, for example, if Austria is the value in isocntry, we are telling R we want this person to be assigned the value of “Western” in the region variable. And so on. You can see the list of ISO country codes here. Once you have created the variable, you can start exploring if there is any association with our outcome variable. For example, using mosaic plots from the vcd package (if you don’t have it installed, the code won’t run). library(vcd) ## Loading required package: grid mosaic(~region + at_sexviol, data = df) In a mosaic plot like this, the height of the region levels indicates how big that group is. You can see there are many more observations in our sample that come from Western countries than from Northern countries. Here, what we are interested in is the length. We see that Northern countries have proportionally more people in the zero category than any other group. On the other hand, Eastern countries have fewer zeros (so looking as if attitudes more permissive towards sexual violence are more common there, even if still a minority). We will come back to these kinds of plots later this semester. 4.7 Working with apparently cryptic variable names and levels Let’s look at the variable d10 in our dataset: table(df$d10) ## ## 1 2 ## 12230 15588 What is this? Unclear, isn’t it? If you look at the questionnaire, you will see that this variable measures gender, that values of 1 correspond to men and that values of 2 correspond to women. But this is far from straightforward just by looking at the printed table or the name of the variable. To start with, we may want to change the name of the variable. One way to do this is the following: #THIS IS AN EXAMPLE colnames(data)[colnames(data)==&quot;old_name&quot;] &lt;- &quot;new_name&quot; Of course, we need to change the names to be valid ones. So, adapting that code, we would write as follows: #option 1: to change a variable name colnames(df)[colnames(df)==&quot;d10&quot;] &lt;- &quot;gender&quot; If you prefer the tydiverse dialect (that aims to save you typing, among other things), then you would use the rename function as below (beware, if you already renamed d10 using colnames, there will be no longer a d10 variable and therefore R will return an error). #option 2: to change a variable name df &lt;- rename(df, gender=d10) If you now check the names of the variables (with the names function) in df, you will see what has happened: names(df) ## [1] &quot;uniqid&quot; &quot;qb10_1&quot; &quot;qb10_2&quot; &quot;qb10_3&quot; &quot;qb10_4&quot; ## [6] &quot;qb10_5&quot; &quot;qb10_6&quot; &quot;qb10_7&quot; &quot;qb10_8&quot; &quot;qb10_9&quot; ## [11] &quot;qb10_10&quot; &quot;qb10_11&quot; &quot;qb10_12&quot; &quot;gender&quot; &quot;d11&quot; ## [16] &quot;isocntry&quot; &quot;d1&quot; &quot;d25&quot; &quot;d15a&quot; &quot;at_sexviol&quot; ## [21] &quot;region&quot; If you want to change many variable names, it may be more efficient to do it all at once. First, we are going to select fewer variables and retain only the ones we will continue using: df &lt;- select(df, uniqid, at_sexviol, gender, d11, d1, d25, d15a, region) Now, we can change a bunch of variables’ names all at once with the following code: names(df)[4:7] &lt;- c(&quot;age&quot;, &quot;politics&quot;, &quot;urban&quot;, &quot;occupation&quot;) names(df) ## [1] &quot;uniqid&quot; &quot;at_sexviol&quot; &quot;gender&quot; &quot;age&quot; &quot;politics&quot; ## [6] &quot;urban&quot; &quot;occupation&quot; &quot;region&quot; You may have seen we wrote [4:7] above. This square bracket notation identifies the columns within that particular object. It is indicating to R that we only wanted to change the name of the variables defining columns 4 to 7 in the data frame; those corresponded to d11, d1, d25, and d15a, which respectively (we can see in the questionnaire) correspond to age, politics, whether the respondent lives in an urban setting and occupation. We assign the names we specify (in the appropriate order) in the list that follows to those columns. Now, we will be less likely to confuse ourselves since we have columns with meaningful names (which will appear in any output and plots we produce). Let’s look at occupation: table(df$occupation) ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 1588 1864 1845 8916 129 14 384 768 517 878 313 1921 2202 856 2062 271 ## 17 18 ## 2471 819 There are 18 categories here. And it is not clear what they mean. class(df$occupation) ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; Remember that this is haven_labelled. If we look at the attributes, we can see the correspondence between the numbers above and the labels. attributes(df$occupation) ## $label ## [1] &quot;OCCUPATION OF RESPONDENT&quot; ## ## $format.stata ## [1] &quot;%8.0g&quot; ## ## $class ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; ## ## $labels ## Responsible for ordinary shopping, etc. ## 1 ## Student ## 2 ## Unemployed, temporarily not working ## 3 ## Retired, unable to work ## 4 ## Farmer ## 5 ## Fisherman ## 6 ## Professional (lawyer, etc.) ## 7 ## Owner of a shop, craftsmen, etc. ## 8 ## Business proprietors, etc. ## 9 ## Employed professional (employed doctor, etc.) ## 10 ## General management, etc. ## 11 ## Middle management, etc. ## 12 ## Employed position, at desk ## 13 ## Employed position, travelling ## 14 ## Employed position, service job ## 15 ## Supervisor ## 16 ## Skilled manual worker ## 17 ## Unskilled manual worker, etc. ## 18 Having to look at this every time is not very convenient. You may prefer to simply use the labels directly. For this, we can use the as_factor function from the haven package. df$occup_f &lt;- as_factor(df$occupation) class(df$occup_f) ## [1] &quot;factor&quot; table(df$occup_f) ## ## Responsible for ordinary shopping, etc. ## 1588 ## Student ## 1864 ## Unemployed, temporarily not working ## 1845 ## Retired, unable to work ## 8916 ## Farmer ## 129 ## Fisherman ## 14 ## Professional (lawyer, etc.) ## 384 ## Owner of a shop, craftsmen, etc. ## 768 ## Business proprietors, etc. ## 517 ## Employed professional (employed doctor, etc.) ## 878 ## General management, etc. ## 313 ## Middle management, etc. ## 1921 ## Employed position, at desk ## 2202 ## Employed position, travelling ## 856 ## Employed position, service job ## 2062 ## Supervisor ## 271 ## Skilled manual worker ## 2471 ## Unskilled manual worker, etc. ## 819 Now you have easier to interpret the output. 4.8 Recoding factors As we said, there are many different types of objects in R, and depending on their nature, the recoding procedures may vary. You may remember that categorical variables are often encoded as factors. Let’s go back to our newly created occup_f. We have 18 categories here. These are possibly too many. Some of them have too few cases, like fisherman. Although this is a fairly large dataset, we only have 14 fishermen. It would be a bit brave to guess what fishermen across Europe think about sexual violence based on a sample of just 14 of them. This is a very common scenario when you analyse data. In these cases, it is helpful to think of ways of adding the groups with small counts (like fishermen in this case) to a broader but still meaningful category. People often refer to this as collapsing categories. You also have to think theoretically: do you have good enough reasons to think that there ought to be meaningful differences in attitudes to sexual violence among these 18 groups? If you don’t, you may want to collapse into fewer categories that may make more sense. When we created the region variable, the first thing to do was to come up with a new coding scheme for this variable. We are going to use the following. I’m not making this scheme up; I am just using the Eurobarometer scheme to simplify these categories. 1 Self-employed (5 to 9 in d15a) 2 Managers (10 to 12 in d15a) 3 Other white collars (13 or 14 in d15a) 4 Manual workers (15 to 18 in d15a) 5 House persons (1 in d15a) 6 Unemployed (3 in d15a) 7 Retired (4 in d15a) 8 Students (2 in d15a) It would be quicker to recode from d15a (here, see the page.618) into a new variable (there would be less typing when dealing with numbers rather than labels). But here, we are going to use this example to show you how to recode from a factor variable, so instead, we will recode from occup_f. df$occup2 &lt;- df$occup_f levels(df$occup2) &lt;- list(&quot;Self-employed&quot; = c(&quot;Farmer&quot; , &quot;Fisherman&quot; , &quot;Professional (lawyer, etc.)&quot; , &quot;Owner of a shop, craftsmen, etc.&quot;, &quot;Business proprietors, etc.&quot;), &quot;Managers&quot; = c(&quot;Employed professional (employed doctor, etc.)&quot;, &quot;General management, etc.&quot;, &quot;Middle management, etc.&quot;), &quot;Other white collar&quot; = c(&quot;Employed position, at desk&quot;, &quot;Employed position, travelling&quot;), &quot;Manual workers&quot; = c(&quot;Employed position, service job&quot;, &quot;Supervisor&quot;, &quot;Skilled manual worker&quot;, &quot;Unskilled manual worker, etc.&quot;), &quot;House persons&quot; = &quot;Responsible for ordinary shopping, etc.&quot;, &quot;Unemployed&quot; = &quot;Unemployed, temporarily not working&quot;, &quot;Retired&quot; = &quot;Retired, unable to work&quot;, &quot;Student&quot; = &quot;Student&quot;) One of the things you need to be very careful with when recoding factors or character variables is that you need to input the chunk of text in the existing variables exactly as they appear in the variable. Otherwise, you will get into trouble. So, for example, if in the code above you wrote fishermen instead of Fisherman, you would have 14 fewer cases in the Self-Employed level than you should have. When doing this kind of operation, it is always convenient to check that the categories add up correctly. For more details on how to recode factors and other potential scenarios, you may want to read this paper by Amelia McNamara and Nicholas Horton. 4.9 Understanding missing data As we have already seen, you will have participants who do not provide you with valid answers to the questions in the survey. In general, in any kind of data set you work with, whether it comes from surveys or not, you will have cases for which you won’t have valid information for particular variables in your data frame. Missing data is common. Let’s explore the politics variable and see how many missing observations are in the variable. class(df$politics) ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; summary(df$politics) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 4.000 5.000 5.196 7.000 10.000 6935 sum(is.na(df$politics)) ## [1] 6935 This is the question as it was asked from the survey respondents. Notice the difference in the response options and the categories in politics. We know that those that see themselves further to the left will have answer 1, and those that see themselves further to the right will have answer 10. According to the codebook, 97 refers to ‘Refusal’ and 98 means ‘DK (don’t know)’. However, if you look at the ‘df’ data, you will see that those two ‘Refusal’ or ‘DK (don’t know)’ in the questionnaire are shown as just NA. In sum, they both are shown as ‘NA’, but actually it has two levels: ‘Refusal’ and ‘DK (don’t know)’. Run the code below, and check we are not lying. Let’s look closer at the attributes: attributes(df$politics) A tip if you don’t want to see as much output. If you want to access directly only some of the attributes, you can call them directly by modifying the code as below: #option 1: to see labels only attributes(df$politics)$labels Or we could use the val_labels function from the labelled package for the same result: #option 2: to see labels only library(labelled) val_labels(df$politics) You may have good theoretical reasons to preserve these NAs in your analysis. Perhaps you think that people who did not answer this question have particular reasons not to do so, and those reasons may be associated with their attitudes to violence. In that case, you may want to somehow preserve them in your analysis, not simply remove NA. Absent that rationale, you may just want to treat them as they are: missing data. You just have no way of knowing if these people are more or less lefty or conservative. Let’s check something about the politics variable: class(df$politics) It says the values are the haven_labelled vector, not numeric. As we could do some descriptive statistics once we treat this ordinal variable as a quantitative variable, now we are going to transform this as a numeric variable. df$politics_n &lt;-as.numeric(df$politics) If you try this, the results of skim() are printed horizontally, with one section per variable type and one row per variable. library(skimr) skim(df$politics_n) If we want to see what percentage of cases is NA across our dataset, we could use colMeans combined with is.na. This will compute the mean (the proportion) of cases that are NA in each of the columns of the data frame: colMeans(is.na(df)) ## uniqid at_sexviol gender age politics urban ## 0.0000000000 0.0454022575 0.0000000000 0.0000000000 0.2492990150 0.0006470631 ## occupation region occup_f occup2 politics_n ## 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.2492990150 Now, we are going to learn how to remove labels. Run the code below, look at the ‘Urban’ and see its labels carefully. library(labelled) val_labels(df) ## $uniqid ## NULL ## ## $at_sexviol ## NULL ## ## $gender ## Man Woman ## 1 2 ## ## $age ## 15 years 96 years ## 15 96 ## 99 and older [not documented] ## 99 ## ## $politics ## Box 1 - left Box 2 Box 3 ## 1 2 3 ## Box 4 Box 5 Box 6 ## 4 5 6 ## Box 7 Box 8 Box 9 ## 7 8 9 ## Box 10 - right Refusal (Spontaneous) DK ## 10 NA NA ## ## $urban ## Rural area or village Small or middle sized town ## 1 2 ## Large town DK ## 3 NA ## ## $occupation ## Responsible for ordinary shopping, etc. ## 1 ## Student ## 2 ## Unemployed, temporarily not working ## 3 ## Retired, unable to work ## 4 ## Farmer ## 5 ## Fisherman ## 6 ## Professional (lawyer, etc.) ## 7 ## Owner of a shop, craftsmen, etc. ## 8 ## Business proprietors, etc. ## 9 ## Employed professional (employed doctor, etc.) ## 10 ## General management, etc. ## 11 ## Middle management, etc. ## 12 ## Employed position, at desk ## 13 ## Employed position, travelling ## 14 ## Employed position, service job ## 15 ## Supervisor ## 16 ## Skilled manual worker ## 17 ## Unskilled manual worker, etc. ## 18 ## ## $region ## NULL ## ## $occup_f ## NULL ## ## $occup2 ## NULL ## ## $politics_n ## NULL Notice that in urban, there are explicit codes ‘DK (don’t know)’ (not two labels ‘Refusal’ and ‘DK (don’t know)’ like the politics variable) for missing data but that these values won’t be treated as such, at least we do something. Let’s see how many cases we have in this scenario: summary(df$urban) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.000 2.000 1.957 3.000 3.000 18 Not too bad. Let’s sort this variable: df$urban_f &lt;- as_factor(df$urban) attributes(df$urban_f) ## $levels ## [1] &quot;Rural area or village&quot; &quot;Small or middle sized town&quot; ## [3] &quot;Large town&quot; &quot;DK&quot; ## ## $class ## [1] &quot;factor&quot; ## ## $label ## [1] &quot;TYPE OF COMMUNITY&quot; You can see that even though the data is NA, the levels ‘DK (don’t know)’ appears printed in the output and shows 18 missing values. This is still a valid level: levels(df$urban_f) ## [1] &quot;Rural area or village&quot; &quot;Small or middle sized town&quot; ## [3] &quot;Large town&quot; &quot;DK&quot; So, we may want to remove it explicitly if we don’t want output reminding us what we already know, that “there are not” DK levels (since now we call them and treat them as NA). levels(df$urban_f)[levels(df$urban_f) == &#39;DK&#39;] &lt;- NA table(df$urban_f) ## ## Rural area or village Small or middle sized town ## 8563 11881 ## Large town ## 7356 Above, we also saw the codes for gender. Let’s use as_factor again for this variable: df$gender_f &lt;- as_factor(df$gender) Let’s just keep the variables we will retain for our final analysis: df_f &lt;- select(df, uniqid, at_sexviol, gender_f, age, politics_n, urban_f, occup_f, region) We are going to explore NA further. Let’s create a new variable that identifies the cases that have missing data in at least one of the columns in the data with the final variables for our analysis. For this, we can use the complete.cases function that creates a logical vector indicating whether any of the cases has missing values in at least one column: df_f$complete &lt;- complete.cases((df_f)) table(df_f$complete) ## ## FALSE TRUE ## 7634 20184 mean(df_f$complete) ## [1] 0.7255734 So, shocking as this may sound, you only have full data for 72% of the participants. Notice how the percentage of missing cases in the variables range: colMeans(is.na(df_f)) ## uniqid at_sexviol gender_f age politics_n urban_f ## 0.0000000000 0.0454022575 0.0000000000 0.0000000000 0.2492990150 0.0006470631 ## occup_f region complete ## 0.0000000000 0.0000000000 0.0000000000 The function complete.cases returns what cases have missing data in any of the variables, not in a singular one. It is not unusual for this percentage to be high. You may end up with a massive loss of cases even though the individual variables themselves do not look as bad as the end scenario. Later this term, we will cover forms of analysis (e.g., multiple regression) in which you have to work with multiple variables at the same time. This form of analysis requires you to have information for all the cases and all the variables. Every case for which there is no information in one of the variables in your analysis is automatically excluded from such analysis. So, the actual sample size you use when using these techniques is the sample size for which you have full information. In our example, your sample size (when you do multiple regression analysis or any multivariate analysis) will be 20184 rather than 27818. This being the case, it makes sense to exclude from your data all cases that have some missing information from the variables you will be using in your analysis. For this, you can use the na.omit function. We create a new object (I name it “full_df”; you can call it whatever) and put inside it the outcome of running na.omit in our original sample (“df_f”). full_df &lt;- na.omit(df_f) Now, we have a dataset ready to start our analysis. 4.10 Exploring data frames visually We have now covered a number of functions you can use to explore your data, such as skimr::skim(), str(), summary(), table(), or dplyr::glimpse(). But sometimes is useful to get a more panoramic way. For this, we can use the visdat package to visualise whole data frames, and its main function is vis_dat(). library(visdat) vis_dat(df_f) Nice one! You get a visual representation of how your variables are encoded in this data frame. You have several categorical variables such as region, urban_f, urban_f, and occup_f. We see that the region is encoded as a character vector, whereas the others are factors. For the purposes of this course, it is generally better to have your categorical variables encoded as factors. So, one of the next steps in our data prep may be to recode region as a factor. df_f$f_region &lt;- as.factor(df_f$region) We can also see we have age, a quantitative variable, age, encoded as haven_labelled. We could as well encode it as numeric. df_f$age &lt;- as.numeric(df_f$age) What we do with at_sexviol depends on how we decide to treat it. But for argument’s sake let’s say we are going to treat it as numerical. df_f$at_sexviol &lt;- as.numeric(df_f$at_sexviol) The other piece of info you get with vis_dat is the prevalence of missing data (NA) for each variable (the dark horizontal cases represent a missing case in the variable). We can summarise missing data visually more clearly with vis_miss. vis_miss(df_f) You can find more details about how to explore missing data in the vignette of the naniar package here. 4.11 A quick recap on descriptive statistics This was covered extensively last semester, and we have also touched on this in the last three weeks, but just as a quick refresher. We can have a review of some descriptive statistics. 4.11.1 Central Tendency Central tendency refers to a descriptive summary of the centre of the data’s distribution; it takes the form of a single number that is meant to represent the middle or average of the data. The mean, median, and mode are common statistics of the central tendency. The mean is the average and it is useful when we want to summarise a variable quickly. Its disadvantage is that it is sensitive to extreme values, so the mean can be distorted easily. For example, the mean value for our political scale variable is: mean(df$politics_n, na.rm = TRUE) ## [1] 5.195805 To address this sensitivity, the median is a better measure because it is a robust estimate, which means that it is not easily swayed by extreme values. It is the middle value of the distribution. median(df$politics_n, na.rm = TRUE) ## [1] 5 The mode helps give an idea of what is the most typical value in the distribution, which is the most frequently occurring case in the data. While mean and median apply to numeric variables, the mode is most useful when describing categorical variables. For example, you may want to know the most frequently occurring category. To query the mode, we use the function mlv() (acronym for most likely values) from the modeest package to answer this question: library(modeest) mlv(df$occup_f) ## [1] Retired, unable to work ## 18 Levels: Responsible for ordinary shopping, etc. ... Unskilled manual worker, etc. 4.11.2 Dispersion You always want to talk about dispersion as well. If you have a numeric variable, you might want to consider the range, which is the difference between the maximum and minimum value in a given distribution: max(df$politics_n, na.rm = TRUE) - min(df$politics_n, na.rm = TRUE) ## [1] 9 You also want to talk about the variance ( \\(s^2\\) ), which tells you about spread, specifically the sum of the squared deviations from the mean, which is then divided by the total number of observations. You will then also come across standard deviation (SD), which is the square root of the variance. You can find these with the sd() and the var() functions: var(df$politics_n, na.rm = TRUE) ## [1] 4.864302 sd(df$politics_n, na.rm = TRUE) ## [1] 2.205516 Visual representations of dispersions, such as a histogram, are also handy for getting an overview of your data. 4.11.3 Bivariate analysis We’ve touched on this already in week two, when we looked at using the group_by() and summarise() functions. Look back at week 2 for more detail. But for example, if we wanted to know what is the mean political score broken down by occupation, we would answer that question using these two functions: politics_by_occ &lt;- df %&gt;% group_by(occup_f) %&gt;% summarise(mean_poli_score = mean(politics_n, na.rm = TRUE)) politics_by_occ ## # A tibble: 18 × 2 ## occup_f mean_poli_score ## &lt;fct&gt; &lt;dbl&gt; ## 1 Responsible for ordinary shopping, etc. 5.23 ## 2 Student 4.91 ## 3 Unemployed, temporarily not working 4.96 ## 4 Retired, unable to work 5.17 ## 5 Farmer 5.69 ## 6 Fisherman 4.82 ## 7 Professional (lawyer, etc.) 5.14 ## 8 Owner of a shop, craftsmen, etc. 5.40 ## 9 Business proprietors, etc. 5.72 ## 10 Employed professional (employed doctor, etc.) 5.41 ## 11 General management, etc. 5.59 ## 12 Middle management, etc. 5.16 ## 13 Employed position, at desk 5.21 ## 14 Employed position, travelling 5.38 ## 15 Employed position, service job 5.11 ## 16 Supervisor 5.39 ## 17 Skilled manual worker 5.25 ## 18 Unskilled manual worker, etc. 5.30 We can now begin to hypothesise whether there are differences in political leanings between different occupations. 4.12 Further resources There are many other ways to recode variables and create new variables based on existing ones. Here, we only provided some examples. We cannot exhaust all possibilities in this tutorial. You can find additional examples and code for recoding variables in the following links. Please make sure that you spend some time looking at these additional resources. They are not optional. Wrangling categorical data in R http://www.cookbook-r.com/Manipulating_data/Recoding_data/ https://mgimond.github.io/ES218/dplyr.html https://www.r-bloggers.com/from-continuous-to-categorical/ You should also look for the dplyr documentation for the functions mutate() and recode(). If you would like to combine several variables into one for your analysis in more complex ways but do not know how to do that, please do get in touch. Also, do not hesitate to approach us if you have any other specific queries. 4.13 Summary This week, we used the following R functions: import data: ‘haven’ package read_dta() explore data class() attributes() summary() skim() mosaic() vis_dat() explore data: descriptive statistics table() mean() median() modeest::mlv() manipulate data: dplyr package select() mutate() manpulate data: missing variable na.omit() colMeans(is.na()) df\\(at_sexviol[df\\)qb10_11 == 1 | df$qb10_12 == 1] &lt;- NA levels(df\\(urban_f)[levels(df\\)urban_f) == ‘DK’] &lt;- NA change a variable name rename() colnames(df)[colnames(df)==“d10”] &lt;- “gender” names(df)[4:7] &lt;- c(“age”, “politics”, “urban”, “occupation”) create list northern_list &lt;- c(“DK”, “FI”, “SE”) transform a variable into a factor variable as_factor() transform a variable into a numeric variable as.numeric() "],["regression-i-mean-differences.html", "Chapter 5 Regression I: Mean differences 5.1 Introduction 5.2 Dependent variable: numerical | Independent variable: binary 5.3 Calculating mean differences in R 5.4 Visual exploration 5.5 Using linear regression to calculate mean differences 5.6 Effect size 5.7 Lab Exercises", " Chapter 5 Regression I: Mean differences 5.1 Introduction Up to now we have introduced a series of concepts and tools that help describe variables. Descriptive statistics are crucial for summarising information about individual variables. However, in the social sciences, we often aim to go beyond description. We use data to test theories, investigate relationships, and uncover associations. This requires moving beyond single-variable analysis to examining the relationship between two variables. In this chapter, we begin exploring bivariate associations. In bivariate analysis, we always start with a research question. Do Black and other ethnic minority citizens experience police stops more often than White citizens (i.e., ethnicity \\(\\rightarrow\\) police stops)? Are violent crimes more common in low-income countries than in high-income countries (income level \\(\\rightarrow\\) violent crimes)? Are people living in urban areas more likely to be victims of crime than those in rural areas (urbanisation \\(\\rightarrow\\) crime victimisation)? These criminologically relevant research questions require the analysis of two variables simultaneously. To answer a research question, we formulate a research hypothesis (or sometimes several research hypotheses related to it). A research hypothesis is simply a proposed answer to our research question that we can test by carrying out some research. Research hypotheses can be directional and non-directional: “When the research hypothesis does not indicate a specific type of outcome, stating only that there is a relationship or a difference, we say that it is a non-directional hypothesis. However, in those cases where a researcher has a very clear idea of what to expect—based on prior research evidence and/or theory—the research hypothesis may be more precise. In this case, the researcher may specify the nature of the relationship that is expected. Such a research hypothesis is called a directional hypothesis. When a directional hypothesis is used, the researcher states at the outset that he or she is interested in a specific type of outcome -for example, that one group has more arrests than another. Suppose we are interested in comparing the arrest records of drug-involved offenders with those of offenders who do not use drugs. Our research hypothesis might be simply that the arrest records of drug-involved offenders and offenders who do not use drugs are different (a nondirectional hypothesis). But based on prior knowledge of criminal behaviour among drug-involved offenders, we might want to state a directional hypothesis -that drug-involved offenders have more serious arrest records than non-drug-involved offenders do. One problem with choosing the latter option is that if we state our research hypothesis as a directional hypothesis, we are stating that we are not interested in outcomes that fall in the opposite direction. In criminal justice research, we can often be surprised by what we learn in a study. Accordingly, researchers generally are cautious in defining a directional research hypothesis” (Weisburd and Britt, 2010: 120) When formulating a research hypothesis, it is common practice to also formulate a null hypotehsis. We will return to this discussion in more detail in Chapter 9, when introducing statistical inference. For now, it suffices to say that in science we always take a sceptic approach and test hypotheses against empirical data. For example, consider the research question: do Black and other ethnic minority citizens experience police stops more often than White citizens? Based on prior research (e.g., here or here), our research hypothesis could be that Black and other ethnic minority citizens are stopped by the police more frequently than White citizens. A sceptical approach, however, would begin with a null hypothesis: there is no difference in the frequency of police stops experienced by Black and other minority citizens compared to White citizens. We then test test this null hypothesis against empirical data to draw conclusions about the association between ethnicity and the experience of police stops. More details on the rationale behind null hypotheses and the principles of hypothesis testing will be provided in Chapter 9! From the research question and the research hypothesis (as well as the null hypothesis), we identify two variables, each with a distinct role. One variable represents the explanandum, the phenomenon we aim to explain—this is called the dependent variable (also referred to as the outcome variable or response variable). The other variable is the explanans, the phenomenon used to explain it—this is called the independent variable (also known as the explanatory variable or predictor variable). For example, in the question we explored earlier—Do Black and other ethnic minority citizens experience police stops more often than White citizens?—we are examining how the frequency of police stops varies depending on a person’s ethnicity. In this case, the frequency of police stops is the dependent variable, as it is the phenomenon we want to explain. Ethnicity is the independent variable, as it is the factor we believe influences the dependent variable. From now on, we will consistently identify dependent and independent variables based on research questions and hypotheses. details { margin-bottom: 1em; /* Adds space below each details block */ } Your turn! In the research questions below, identify the dependent variable and the independent variable: Are violent crimes more common in low-income countries than in high-income countries? Reveal answer! Unit of analysis: countries Dependent variable: frequency of violent crimes Independent variable: income level Are people living in urban areas more likely to be victims of crime than those in rural areas? Reveal answer! Unit of analysis: people / members of the public Dependent variable: likelihood of crime victimisation Independent variable: urbanisation, or residence area characteristics (e.g., urban vs. rural areas) Do neighbourhoods with a heavier police presence experience less crime? Reveal answer! Unit of analysis: neighbourhoods Dependent variable: frequency of crimes Independent variable: police prevalence (e.g., heavily policed vs. lightly policed) Does being on probation reduce the likelihood of reoffending compared to individuals not on probation? Reveal answer! Unit of analysis: individuals / previous offenders Dependent variable: likelihood of reoffending Independent variable: probation (e.g., being on probation vs. not on probation) Does cannabis legalisation lead to higher self-reported levels of cannabis use? Reveal answer! Unit of analysis: countries / cities / states Dependent variable: self-reported levels of cannabis use Independent variable: cannabis legislation (e.g., legalised vs. prohibited) 5.2 Dependent variable: numerical | Independent variable: binary Throughout the semester, we will explore how to analyse relationships between variables across various combinations. If you examine the research questions above, they share a key feature: all the dependent variables are numerical, and all the independent variables are binary. Numerical variables represent measurable or countable quantities where arithmetic operations like addition, subtraction, multiplication, and division are meaningful. Examples from the research questions include the frequency of police stops, frequency of violent crimes, likelihood of crime victimisation, frequency of crimes, likelihood of reoffending, and self-reported levels of cannabis use. These are all numerical variables because they have numerical values that can be counted or measured. Binary variables (also referred to as dichotomous or dummy variables), on the other hand, represent categorical data with two mutually exclusive and exhaustive categories. Binary variables include only two possible levels, such as TRUE or FALSE, yes or no, or two distinct groupings. Examples from the research questions include: ethnicity (e.g., White vs. Black and other ethnic minorities), countries’ income level (e.g., low-income vs. high-income), area characteristics (e.g., urban vs. rural areas), police prevalence in neighbourhoods (e.g., heavily policed vs. lightly policed), probation status (e.g., being on probation vs. not being on probation), and cannabis legislation (e.g., legalised vs. prohibited) This is the first bivariate analysis we will study. div { margin-bottom: 1em; /* Adds space below each details block */ } FIRST BIVARIATE ANALYSIS Dependent variable: Numerical Independent variable: Binary Let’s elaborate with an example. Let’s start with the following research question: are women more afraid of violent crime than men? Previous research has consistently demonstrated a gender disparity in fear of crime, with women reporting higher levels of fear compared to men (e.g., here, here, here). While the dynamics of crime victimisation risk differ between men and women, the fear-gender gap persists across various contexts.6 Based on previous research, our research hypothesis is that women are more afraid of violent crime than men. However, adopting a sceptical approach, our null hypothesis states that there are no differences in fear of crime between men and women. To test this, we must contrast this statement with empirical data. For this example, we will use data from the Crime Survey for England and Wales (2007–08), which provides a representative sample of the adult population living in England and Wales. This dataset includes information on respondents’ fear of crime, making it suitable for addressing our research question. Let’s begin by loading the dataset. # load readr library and import the data using read_csv() function library(readr) csew_0708 &lt;- read_csv(&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/BCS0708.csv&quot;) The variables of interest in our analysis are tcviolent and sex. The variable tcviolent is an index of fear of violent crime, measured on a numerical scale where lower scores indicate less fear and higher scores indicate greater fear. To summarise this variable, we can use the summary() function. As shown below, the mean score is 0.05, with a minimum of -2.35 and a maximum of 3.81. summary(csew_0708$tcviolent) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -2.350 -0.672 -0.117 0.046 0.540 3.805 3242 In this dataset, sex is a binary variable—unfortunately, the survey instrument did not measure gender identification and is limited to responses recorded as ‘male’ or ‘female’. We can use the table() and prop.table() functions to summarise this variable, which respectively provide counts and proportions of the number of observations in our data that take distinct values for a given variable. 6369 (55%) respondents were recorded as female, whereas 5307 (45%) were recorded as male. table(csew_0708$sex) ## ## female male ## 6369 5307 prop.table(table(csew_0708$sex)) ## ## female male ## 0.5454779 0.4545221 5.3 Calculating mean differences in R Now, how can we examine whether women are more afraid of violent crime than men? To address this research question, we need to evaluate the association between the variables tcviolent and sex. This process builds on concepts introduced in Chapter 3. In Chapter 3, we learned how to describe numerical variables using measures of central tendency (such as the mean and the median) and measures of dispersion (such as the standard deviation and the range). Among these, the mean is particularly effective for summarising symmetric and normally distributed data. Given this, one straightforward strategy to assess whether women are more afraid of violent crime than men is to calculate the mean level of fear of violent crime only among women and compare it to the mean level of fear only among men. Adopting a sceptical approach—recalling the null hypothesis, which states that there are no differences in fear of crime between men and women—we would expect these two means to be roughly equal. If the means differ, this would provide evidence against the null hypothesis, suggesting that fear of violent crime varies between men and women and that the two variables (tcviolent and sex) are associated. In this context, the mean difference serves as the measure of association between a numerical dependent variable (tcviolent) and a binary independent variable (sex). To calculate the mean of a numerical variable for specific subgroups, we can use the filter() function from the dplyr package. The filter() function allows us to subset the data based on specified conditions. For example, we can use the filter() function to calculate the mean of fear of violent crime for women and the mean of fear of violent crime for men. Note that when using filter(), you need to use a double equals sign (==) to specify equality. # Install the &#39;dplyr&#39; package if you haven&#39;t already # install.packages(&quot;dplyr&quot;) # Load the dplyr package library(dplyr) # Subset the data for female respondents csew_0708_women &lt;- filter(csew_0708, sex == &quot;female&quot;) # Check the number of rows in the filtered dataset nrow(csew_0708_women) ## [1] 6369 As expected, the csew_0708_women dataset contains 6369 rows. This is the number of female respondents we had obtained before. # Subset the data for male respondents csew_0708_men &lt;- filter(csew_0708, sex == &quot;male&quot;) # Check the number of rows in the filtered dataset nrow(csew_0708_men) ## [1] 5307 Similarly, the csew_0708_men dataset contains 5307 rows, corresponding to the number of male respondents in the dataset. Now, let’s calculate the mean level of fear of violent crime (tcviolent) for each subgroup: # Calculate the mean of fear of violent crime for women mean_fear_women &lt;- mean(csew_0708_women$tcviolent, na.rm = TRUE) # Calculate the mean of fear of violent crime for men mean_fear_men &lt;- mean(csew_0708_men$tcviolent, na.rm = TRUE) # Display the mean of fear of violent crime for women mean_fear_women ## [1] 0.3281656 # Display the mean of fear of violent crime for men mean_fear_men ## [1] -0.2738322 The mean level of fear of violent crime among women is 0.33, while among men, it is -0.27. As anticipated, women report higher levels of fear of violent crime than men (since 0.33 \\(&gt;\\) -0.27), providing evidence that gender differences in fear of crime exist. To refine our analysis, we can calculate the mean difference between the fear of violent crime scores for men and women. The mean difference is simply the result of subtracting one group’s mean from the other. # Calculate the mean difference mean_difference &lt;- mean_fear_men - mean_fear_women # Display the mean difference mean_difference ## [1] -0.6019978 The mean difference is -0.6. This indicates that the average score of fear of violent crime among male respondents is 0.6 lower than the average score among female respondents. Note: the order of subtraction matters in interpreting the result, even though it does not change the numerical value. Subtracting the mean for women from the mean for men highlights that men have lower fear scores, while reversing the subtraction would emphasize that women have higher fear scores. It is crucial to align the direction of subtraction with the focus of the research question or the narrative you wish to convey. For example: # Calculate the mean difference using the alternative order mean_difference_alternative &lt;- mean_fear_women - mean_fear_men # Display the mean difference mean_difference_alternative ## [1] 0.6019978 In this case, the mean difference is 0.6. This indicates that the average score of fear of crime among female respondents is 0.6 higher than the average score among male respondents. If the null hypothesis were true (i.e., adopting a sceptical approach), we would expect the means for both groups to be approximately the same, resulting in a mean difference close to zero. A mean difference of -0.6 suggests that women tend to report higher levels of fear of violent crime than men in the Crime Survey for England and Wales, providing some evidence that allows to address our research question. 5.4 Visual exploration One simple strategy to depict the association between a numerical dependent variable and a binary independent variable (i.e., the mean difference) involves using data visualisation techniques. We already covered this in Chapter 3! For example, when we want to visualise the distribution of a numerical variable, we can produce a histogram, a density plot, or a boxplot. If we want to graphically represent the distribution of a numerical variable across two groups, we can produce a grouped histogram, a grouped density plot, or a grouped boxplot. This allows us to assess the association between a numerical dependent variable and a binary independent variable by examining their mean difference. # load the ggplot2 package library(ggplot2) # produce a grouped density plot ggplot(csew_0708, aes(x = tcviolent, fill = sex)) + geom_density(alpha = .3) This grouped density plot shows that the distribution of fear of violent crime (tcviolent) scores among female respondents is slightly shifted to the right compared to the distribution among male respondents. This suggests an association between gender and fear of violent crime—as we already knew—as women in this sample have a higher average score than men. The same pattern can be visualised with a grouped boxplot. # produce a grouped boxplot ggplot(csew_0708, aes(x = sex, y = tcviolent)) + geom_boxplot() 5.5 Using linear regression to calculate mean differences Calculating mean differences in R is straightforward, as demonstrated above. We first filter the dataset by the groups of interest, compute the mean of the dependent variable for each group, and then calculate the difference between the two group-specific means. While this step-by-step approach is effective, it can become time-consuming when repeated for multiple analyses. Fortunately, R offers a more efficient alternative: the lm() function. lm(dependent_variable ~ independent variable, data = dataset) The lm() function, short for linear model, streamlines the process by calculating the mean difference directly. The function’s structure is simple: Specify the dependent variable first. Follow it with a tilde (\\(\\sim\\)). Then, provide the independent variable and the dataset. When the dependent variable is numerical and the independent variable is binary, the lm() function automatically calculates the mean difference and saves time by performing all the necessary steps in one go. You can use it directly or save the output to an object for later use. For our example, the code would look like this: # Linear model calculating the difference in fear of crime by sex mean_difference_lm &lt;- lm(tcviolent ~ sex, data = csew_0708) In this case: tcviolent is the dependent variable (a numerical variable). sex is the independent variable (a binary variable). csew_0708 is the dataset being analysed. mean_difference_lm is the name we assigned to the object storing the model’s results. This single line of code computes the mean difference in fear of violent crime between men and women based on the dataset csew_0708, offering a more streamlined approach to the analysis. Now, let’s examine the output. # Display the results of the linear model mean_difference_lm ## ## Call: ## lm(formula = tcviolent ~ sex, data = csew_0708) ## ## Coefficients: ## (Intercept) sexmale ## 0.3282 -0.6020 The output has two parts: Call: This section restates the formula you provided to the function, confirming that tcviolent is the dependent variable, sex is the independent variable, and csew_0708 is the dataset. Coefficients: This section provides the results we’re most interested in, showing two estimates: (Intercept): 0.3282 sexmale: -0.6020 If you recall from above when we manually calculated everything, these numbers should look familiar! The average score of fear of violent crime among women (remember, we created the mean_fear_women object) was 0.3282—exactly what is reported as the Intercept in this output. And the mean difference (remember, we created the mean_difference object) was -0.602—exactly what is reported as the sexmale coefficient! This implies that male respondents have a fear score that is 0.3282 points lower than female respondents on average. Note on how to figure out which comparisons the model is making: As we discussed above, there are two possible comparisons. They are numerically equivalent (i.e., only the sign differs), but the interpretation changes. We can calculate mean_fear_men - mean_fear_women, which gives a mean difference of -0.602. Alternatively, we can calculate mean_fear_women - mean_fear_men, as we did when we created the mean_difference_alternative object, which gives a mean difference of 0.602. While these values are numerically the same, their interpretation focuses on different groups. How to determine which comparison lm() is performing The lm() function selects one category to be represented by the Intercept—this is known as the reference category. The reference category is the group being compared against, i.e., the right-hand side of the subtraction equation. The lm() function then calculates the difference between the other category and the reference category. This is visible in the output. For example, in the output above, the coefficient is labelled sexmale. This implies that female is the reference category, and the comparison being made is mean_fear_men - mean_fear_women. The coefficient sexmale: -0.602 therefore indicates that male respondents, on average, have a fear score -0.602 points lower than female respondents. How R determines the reference category Character Variables: If the independent variable is a character variable, lm() selects the first group alphabetically as the reference category. In this example, female comes before male, so female is set as the reference category. Factor Variables: If the independent variable is a factor, you can manually set the reference category. Logical Variables: If the independent variable is logical (i.e., TRUE or FALSE), FALSE is automatically set as the reference category. Changing the reference category. If we wanted to treat male as the reference category, we could do one of the following: # Create a logical variable that is TRUE if the respondent is female # and FALSE if the respondent is male csew_0708 &lt;- mutate(csew_0708, female_logical = sex == &quot;female&quot;) # Create a factor variable with &#39;male&#39; as the reference category csew_0708 &lt;- mutate(csew_0708, female_factor = factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;))) We can then estimate new regression models using the female_logical and female_factor variables: # Estimate a linear regression using &#39;female_logical&#39; as the independent variable lm(tcviolent ~ female_logical, data = csew_0708) ## ## Call: ## lm(formula = tcviolent ~ female_logical, data = csew_0708) ## ## Coefficients: ## (Intercept) female_logicalTRUE ## -0.2738 0.6020 # Estimate a linear regression using &#39;female_factor&#39; as the independent variable lm(tcviolent ~ female_factor, data = csew_0708) ## ## Call: ## lm(formula = tcviolent ~ female_factor, data = csew_0708) ## ## Coefficients: ## (Intercept) female_factorfemale ## -0.2738 0.6020 For both models, the outputs are identical. The Intercept now reflects the mean of fear of violent crime among male respondents (-0.27), as male is the reference category. The coefficient is also identical to the value stored in the mean_difference_alternative object (0.6). This coefficient is numerically equivalent to the previously estimated sexmale coefficient (-0.6), but the sign changes. With male as the reference group, the output reflects that female respondents have a fear score 0.6 points higher than male respondents on average. 5.5.1 Linear Regression In the previous section, we used the lm() function to estimate mean differences and learned that lm stands for linear model. This function provides a way to estimate relationships using a statistical model, specifically the most basic and widely used model: linear regression. At its core, linear regression characterises the relationship between a dependent variable and one (or more) independent variable(s) using a linear model: \\[ Y = \\underbrace{\\alpha}_{\\text{Intercept}} + \\underbrace{\\beta \\cdot}_{\\text{Slope}} \\! X + \\!\\!\\!\\!\\! \\underbrace{\\epsilon}_{\\text{Error Term}} \\] In this equation: \\(Y\\) is the dependent variable. \\(X\\) is the independent variable. By convention, the dependent variable (\\(Y\\)) is always displayed on the left-hand side of the equation, while the independent variable (\\(X\\)) is on the right-hand side. For example, in the context of our dataset, \\(Y\\) might represent scores of fear of violent crime, and \\(X\\) could represent sex. It is also standard practice to use Latin letters (\\(Y\\) and \\(X\\)) for observed variables in our dataset, whereas Greek letters (\\(\\alpha\\), \\(\\beta\\), and \\(\\epsilon\\)) represent unknown parameters that need to be estimated. The intercept (\\(\\alpha\\)) represents the average value of \\(Y\\) when \\(X = 0\\). The slope (\\(\\beta\\)) measures the average increase in \\(Y\\) when \\(X\\) increases by one unit. Together, \\(\\alpha\\) and \\(\\beta\\) are called regression coefficients. These coefficients are not directly observed in the data and must be estimated. Finally, the error term (\\(\\epsilon\\)) accounts for the variability in \\(Y\\) that is not explained by \\(X\\). We will elaborate on this concept in the sections that follow. Linear regression is a widely used statistical model in the social sciences. Over the coming weeks, we will extend several aspects of this model. Regression models serve two main purposes: prediction and theory testing. These models allow us to specify research questions and translate them into statistical representations, assuming the model approximates the data-generating process. In reality, we do not know the true data-generating process, and our statistical model may be incomplete. For example, factors beyond gender—such as prior victimisation (of oneself or family/friends), local crime rates, or individual personality traits—may also influence people’s fear of violent crime. While these factors are not included in our current model, that’s acceptable. As the saying goes, “all models are wrong, but some are useful.” Our primary goal is not to explain all variation in the dependent variable (e.g., fear of crime) but to address our research question. In this case, we aim to determine whether women are more afraid of violent crime than men by estimating the difference in average fear scores between the two groups. Over the next few weeks, we will expand our understanding of linear regression models in various ways: Independent variables can also be numerical (Chapter 6). Linear regression models have several assumptions that need to be satisfied (Chapter 6). Independent variables can be categorical with more than two groups (Chapter 7). Multiple independent variables can be included simultaneously (Chapter 7). For now, we are focusing on a basic scenario: a numerical dependent variable and a binary independent variable. In this case, the estimated slope coefficient corresponds to the difference-in-means estimator. 5.5.2 Linear Regression as a Difference-in-Means Estimator Let’s apply this to our example. The regression equation can be written as: \\[ \\widehat{tcviolent} = \\widehat{\\alpha} + \\widehat{\\beta} \\cdot sex \\] Here, tcviolent (a variable reflecting scores of fear of violent crime) is the dependent variable, and sex is the independent variable. We aim to estimate the parameters \\(\\alpha\\) (the intercept) and \\(\\beta\\) (the slope), which help us address the research question. Linear regression employs ordinary least squares (OLS)—a method we will study in more detail next week—to estimate \\(\\alpha\\) and \\(\\beta\\). The lm() function, introduced earlier, performs this estimation. Let’s revisit the regression output: # Display the results of the linear model mean_difference_lm ## ## Call: ## lm(formula = tcviolent ~ sex, data = csew_0708) ## ## Coefficients: ## (Intercept) sexmale ## 0.3282 -0.6020 From the output: The intercept is 0.3282, meaning \\(\\widehat{\\alpha} =\\) 0.3282. The slope is -0.602, meaning \\(\\widehat{\\beta} =\\) -0.602. Thus, we can rewrite the regression equation as: \\[ \\widehat{tcviolent} = 0.3282 - 0.6020 \\cdot sex \\] Now, how does this equation make sense in practice? As noted earlier, tcviolent is a numerical variable, ranging from -2.35 to 3.81. Since it is numerical, arithmetic operations are meaningful, making its inclusion in a regression equation straightforward. After all, the goal is to estimate expected scores of tcviolent under specific conditions. However, sex is not a numerical variable; it is a binary variable with two possible values: female and male. How can we incorporate such a variable into an equation? The trick lies in treating binary variables as a special type of numerical variable. Binary variables can only take two distinct values (e.g., TRUE or FALSE, yes or no, black or white). By assigning meaningful numeric values, such as 1 or 0, to the categories, they can be seamlessly included in equations. In this case, the variable sex is coded as follows: \\(sex = 0\\) if sex is female \\(sex = 1\\) if sex is male Conventionally, the group assigned a value of 0 is the reference group (or sometimes referred to as the control group), while the group assigned a value of 1 is called the comparison group (or sometimes the treatment group). By applying this coding to the linear regression model, we can interpret the results as follows: For \\(sex=0\\) (i.e., females): \\[ \\widehat{tcviolent} = 0.3282 - 0.6020 \\cdot 0 \\\\ \\widehat{tcviolent} = 0.3282 \\] For \\(sex = 1\\) (i.e., males): \\[ \\widehat{tcviolent} = 0.3282 - 0.6020 \\cdot 1 \\\\ \\widehat{tcviolent} = -0.2738 \\] These equations illustrate how linear regression estimates mean differences. The intercept (\\(\\alpha\\)) represents the mean outcome for the reference group (e.g., the average fear of violent crime score among females), while the slope coefficient (\\(\\beta\\)) reflects the difference in means between the two groups. Thus, in this example: The mean fear of violent crime score for females is \\(\\alpha = 0.3282\\). The mean fear of violent crime score for males is \\(\\alpha + \\beta = 0.3282 - 0.6020 = -0.2738\\). The slope coefficient (\\(\\beta = -0.6020\\)) quantifies the difference in means between the two groups. Linear regression provides a straightforward way to quantify and interpret these differences. Why do we include “hats” in the parameters (e.g., \\(\\widehat{Y}\\), \\(\\widehat{\\alpha}\\), \\(\\widehat{\\beta}\\))? The linear regression model is expressed as: \\[ Y = \\alpha + \\beta \\cdot X + \\epsilon \\] Here, \\(\\alpha\\) and \\(\\beta\\) are unknown parameters that need to be estimated. We can attempt to estimate (e.g., calculate) them. A common method for estimating these linear regression coefficients is the method of least squares. However, because we don’t know whether our estimates of \\(\\alpha\\) and \\(\\beta\\) perfectly match the unknown parameters, we need distinguish the estimates from the unknown values. That’s where the “hats” come in. \\(\\widehat{\\alpha}\\) and \\(\\widehat{\\beta}\\) represent the estimates (think guesstimates!) of \\(\\alpha\\) and \\(\\beta\\), respectively. The “hat” indicates that these are estimated values, not the true parameters. We usually expect our estimator to do a good job in estimating parameters. To the extent that \\(\\widehat{\\alpha} = \\alpha\\) and \\(\\widehat{\\beta} = \\beta\\) can be proved, then we would have an unbiased estimator. (but don’t worry, that’s not something we need to worry about! That’s a job for theoretical statisticians). Once we have estimated values of \\(\\alpha\\) and \\(\\beta\\), we can use them to predict the value of the dependent variable \\(Y\\) for a given value of the independent variable \\(X\\) (e.g., predict the value of fear of violent crime given respondents’ sex). This predicted value (or fitted value) of Y is also and estimated value, therefore we denote it as \\(\\widehat{Y}\\). As such, we can write the regression function: \\[ \\widehat{Y} = \\widehat{\\alpha} + \\widehat{\\beta} \\cdot x \\] In this equation, we did not include \\(\\epsilon\\). In most cases, the predicted value \\(\\widehat{Y}\\) is not equal to the observed value \\(Y\\). For instance, while \\(\\widehat{Y}=0.3282\\) for \\(X=0\\) (i.e., the average score of fear of violent crime among female respondents is \\(0.3282\\)), most female respondents probably have an observed score of fear of violent crime that is not exactly \\(0.3282\\). Similarly, while \\(\\widehat{Y}=-0.2738\\) for \\(X=1\\) (i.e., the average score of fear of violent crime among male respondents is \\(-0.2738\\)), most male respondents probably have an observed score of fear of violent crime that is not exactly \\(-0.2738\\). The difference between the observed value \\(Y\\) and its predicted value \\(\\widehat{Y}\\) (e.g., the difference between each individual score of fear of violent crime and the estimates above) is called the residual and is given by: \\[ \\widehat{\\epsilon} = Y - \\widehat{Y}. \\] The residual (\\(\\widehat{\\epsilon}\\)) is essentially the “error” in prediction. It represents the part of \\(Y\\) that is not explained by \\(X\\) using the regression model. The residual \\(\\widehat{\\epsilon}\\) is also the error term \\(\\epsilon\\) with a hat, as it represents an estimate of the error term. When we write the linear model focused on \\(Y\\), we have unknown parameters \\(\\alpha\\) and \\(\\beta\\) as well as an error term accounting for variation in \\(Y\\) not explained by \\(X\\). When we write the linear model focused on \\(\\widehat{Y}\\), we have parameter estimates \\(\\widehat{\\alpha}\\) and \\(\\widehat{\\beta}\\) and no error term. The distinction between the error term (\\(\\epsilon\\)) and the residual (\\(\\widehat{\\epsilon}\\)), as well as the role of \\(\\widehat{Y}\\) versus \\(Y\\), will become clearer as we explore these concepts further next week! 5.6 Effect size In this chapter, we learned that when we have a numerical dependent variable and a binary independent variable, we can assess their association by calculating the mean difference. Any difference between the mean scores of the dependent variable in the reference group and the mean scores of the dependent variable in the comparison group indicates that both variables are associated. In our case, because the average score of fear of violent crime among male respondents is -0.27, and among female respondents it is 0.33, the mean difference of -0.6 represents the association between gender and fear of violent crime. Is this enough, though? Does a mean difference of -0.6 imply a strong association? How large a difference would we need to observe to argue that there is a substantive difference? Let’s say, hypothetically, that the mean score of fear of violent crime among men was 0.32, and among women it was 0.33. In this hypothetical case, the mean difference would be \\(0.01\\). Would this mean difference be enough for us to conclude that there is an association? Technically, given that the mean difference is not \\(0\\), an association exists… but how strong is it? How can we assess the actual strength of an association? The first step is looking at the scaling of the dependent variable. In our case, fear of violent crime (tcviolent) was measured using an artificial metric. It ranges from -2.35 to 3.81, with a range of 6.16 points. In this context, we can subjectively assess whether our estimated mean difference of -0.6 is weak or strong. In other words, considering a scale ranging from -2.35 to 3.81, female respondents have an mean score that is 0.6 higher than male respondents. If that subjective assessment is not enough to have an intuitive understanding of the magnitude of the observed association, we can always look at a standardised measured of the effect size. You will find a number of standardised measures of effect size. They aim to give you a sense of how large these differences are by using a standardised metric. We are just going to use one of them, Cohen’s d, for this scenario. We can obtain this measure with the cohen.d() function from the effsize package, which you will have to install. # install the &#39;effsize&#39; package. Remember: you only have to do this once. # install.packages(&quot;effsize&quot;) # load the &#39;effsize&#39; package. You have to do this everytime. library(effsize) # compute the Cohen&#39;s d effect size cohen.d(csew_0708$tcviolent ~ csew_0708$sex) ## ## Cohen&#39;s d ## ## d estimate: 0.6281126 (medium) ## 95 percent confidence interval: ## lower upper ## 0.5843047 0.6719205 The output suggests that the Cohen’s d estimate is a medium effect size. Cohen proposed a set of rules of thumb to interpret the d statistic: an effect size (in absolute value) of 0.2 to 0.3 might be a “small” effect, around 0.5 a “medium” effect and 0.8 to infinity, a “large” effect. However, keep in mind these rules are not absolute. In some fields of research and in relation to some problems the rules of thumb may be slightly different. You need, in professional practice, to be alert to those nuances by being familiar with the rules that other researchers use in your particular area of work. How do we write our results up? We could say the following: On average, males have a lower score of fear of violent crime (mean = -0.27) than the female group (mean =0.33). Considering a normally distributed score ranging from -2.35 to 3.81, the mean difference of 0.60 suggests a moderate association between gender and fear of violent crime. For example, this association is represented by a medium-sized standardised effect (Cohen’s d=-0.63). This is what you would write in your “Findings” section. In your “Conclusions” you would need to discuss what the theoretical or practical implications of this finding are; connecting it to existing theoretical debates. 5.7 Lab Exercises Your turn! In the lab session, using the same data set (the Crime Survey for England and Wales 2007/08, i.e., the csew_0708 data.frame), answer the following questions. Note that not all questions necessarily require analysing data in R. After you finish, click on ‘Reveal answer!’ to check your answers. You want to use the CSEW data to study people’s perceptions about the level of anti-social behaviour in their neighbourhood. The variable tcarea measures that, with higher scores indicating higher levels of perceived anti-social behaviour. Based on social disorganisation theory, you suspect that citizens who live in urban or rural areas have different perceptions. The variable rural2 indicates the level of urbanisation of respondents’ area of residence. Based on social disorganisation theory, what is your research hypothesis? Reveal answer! Citizens who reside in urban areas tend to perceive more anti-social behaviour in their neighbourhood than residents of rural areas. Based on your research hypothesis, what are your dependent and independent variables? Reveal answer! Dependent variable: perceived levels of anti-social behaviour, i.e., tcarea. Independent variable: level of urbanisation of respondents’ area of residence, i.e., rural2. Is your dependent variable numerical or categorical? If categorical, is it binary, ordinal, or multinomial? What about the independent variable? Reveal answer! The dependent variable, perceived levels of anti-social behaviour in the neighbourhood (tcarea), is a numerical variable. The independent variable, level of urbanisation of respondents’ area of residence (rural2), is a binary variable. Using your dependent and independent variables, what is your null hypothesis? Reveal answer! Null hypothesis: there is no difference in the level of perceived anti-social behaviour in the neighbourhood between residents of urban and rural areas. How can you assess the association between your dependent and independent variables and test your null hypothesis? Reveal answer! Given that the dependent variable is numeric and the independent variable is binary, we can test the null hypothesis by estimating a mean difference. If the null hypothesis is true, we would expect the average level of perceived anti-social behaviour in the neihbourhood to be largely the same among both rural and urban residents. Describe your dependent variable using the appropriate descriptive statistics. Reveal answer! Given that tcarea is a numerical variable, we can use the summary() function to describe it. summary(csew_0708$tcarea) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -2.6735 -0.7943 -0.0942 0.0303 0.6420 4.1883 677 Perceived anti-social behaviour in the neighbourhood is measured using a scale ranging from -2.67 to 4.19. The mean is 0.03, with a median of -0.09. Describe your independent variable using the appropriate descriptive statistics. Reveal answer! Given that rural2 is a binary variable, we can use the table() and the prop.table() functions to describe it. table(csew_0708$rural2) ## ## rural urban ## 2974 8702 prop.table(table(csew_0708$rural2)) ## ## rural urban ## 0.2547105 0.7452895 2974 respondents (25.47%) live in rural areas, whereas 8702 respondents (74.53%) live in urban areas. What is the average score of perceived anti-social behaviour among respondents who live in rural areas? What about respondents who live in urban areas? What is the mean difference? Reveal answer! # Subset the data for respondents living in urban areas csew_0708_urban &lt;- filter(csew_0708, rural2 == &quot;urban&quot;) # Calculate the average perceived ASB in the neighbourhood among urban residents mean_urban &lt;- mean(csew_0708_urban$tcarea, na.rm = T) # Display results mean_urban ## [1] 0.1598183 # Subset the data for respondents living in rural areas csew_0708_rural &lt;- filter(csew_0708, rural2 == &quot;rural&quot;) # Calculate the average perceived ASB in the neighbourhood among rural residents mean_rural &lt;- mean(csew_0708_rural$tcarea, na.rm = T) # Display results mean_rural ## [1] -0.3403847 # Estimate the mean difference mean_urban - mean_rural ## [1] 0.500203 The average score of perceived anti-social behaviour among respondents who live in urban areas is 0.16, whereas the average score of perceived anti-social behaviour among respondents who live in rural areas is -0.34. The mean difference is 0.5 Let’s build a regression model. Replacing \\(Y\\) with the name of your dependent variable, and \\(X\\) with the name of your independent variable, write down the equation with the unknown parameters (i.e, \\(\\alpha\\) and \\(\\beta\\)) that you want to estimate. Note: this question does not involve any data analysis. Reveal answer! \\[ tcarea = \\alpha + \\beta \\cdot rural2 \\] Using the lm() function, estimate the parameters of your linear regression model. Rewrite the equation above replacing unknown parameters with the estimated parameters. Reveal answer! lm(tcarea ~ rural2, data = csew_0708) ## ## Call: ## lm(formula = tcarea ~ rural2, data = csew_0708) ## ## Coefficients: ## (Intercept) rural2urban ## -0.3404 0.5002 Based on the results of the linear regression model, we can rewrite the equation in the following way: \\[ tcarea = -0.34 + 0.50 \\cdot rural2 \\] What does the estimated intercept \\(\\widehat{\\alpha}\\) represent? What does the estimated slope coefficient \\(\\widehat{\\beta}\\) represent? Reveal answer! The estimated intercept \\(\\widehat{\\alpha}=-0.34\\) indicates the average score of perceived anti-social behaviour in the reference group (i.e., when \\(rural2=0\\)). In this case, it indicates that the average perceived anti-social behaviour among rural residents is 0. The estimated slope coefficient \\(\\widehat{\\beta}=0.50\\) indicates the mean difference between the comparison and the reference groups. In this case, it indicates that respondents who live in urban areas have, on average, scores of perceived anti-social behaviour in the neighbourhood 0.50 points higher than respondents who live in rural areas. What do you conclude about the association between your independent and dependent variables? Did you find support for your hypothesis? How strongly associated are the two variables? Reveal answer! cohen.d(csew_0708$tcarea ~ csew_0708$rural2) ## ## Cohen&#39;s d ## ## d estimate: -0.5072798 (medium) ## 95 percent confidence interval: ## lower upper ## -0.5504660 -0.4640936 As hypothesised by social disorganisation theory, on average, residents of urban areas are more exposed to anti-social behaviour in the neighbourhood (mean \\(=0.16\\)) than residents of rural areas (mean \\(=-0.34\\)). Considering a normally distributed score ranging from -2.67 to 4.19, the mean difference of 0.5 suggests a moderate association. For example, this association is represented by a medium-sized standardised effect (Cohen’s d \\(=-0.51\\)). Note: While this example examines the fear of crime between men and women, reflecting the focus of much prior research, we acknowledge that gender is not binary and includes transgender, non-binary, and gender-diverse individuals. Their experiences and perceptions of fear may differ and are equally important to consider in criminological research. The binary framing here is intended only to align with the specific scope of prior studies, not to imply that other gender identities are less significant.↩︎ "],["regression-ii-numerical-independent-variables.html", "Chapter 6 Regression II: numerical independent variables 6.1 Introduction 6.2 Motivating regression 6.3 Fitting a simple regression model 6.4 Residuals: R squared 6.5 Is this the same as last week? 6.6 Regression assumptions 6.7 Lab Exercises", " Chapter 6 Regression II: numerical independent variables 6.1 Introduction Last week, we began studying linear regression modelling as a powerful tool to assess the relationship between two variables. We learned that, in bivariate analysis, we always start with a research hypothesis derived from theory, and from that, we identify a dependent variable (also known as an outcome or response variable) and an independent variable (also known as an explanatory or predictor variable). We then learned that when the dependent variable is numerical and the independent variable is binary, we examine their association by calculating the difference between the average scores of the dependent variable across the two groups of the binary independent variable; that is, the mean difference represents the association in this scenario. Finally, we learned that the linear regression framework is a powerful method that allows us to efficiently estimate the mean difference. By defining a linear model given by \\(Y = \\alpha + \\beta \\cdot X\\), where \\(Y\\) represents the numerical dependent variable and \\(X\\) represents the binary independent variable, we can use the linear regression estimator to estimate values for \\(\\alpha\\) and \\(\\beta\\). In this model, \\(\\widehat{\\alpha}\\) represents the average score of the dependent variable among observations in the reference group, and \\(\\widehat{\\beta}\\) represents the mean difference. In this session, we will continue our journey with linear regression models. This form of analysis has been one of the primary techniques of data analysis in the social sciences for many years, and it belongs to a family of techniques known as generalised linear models. Regression is a flexible method that allows us to “explain” or “predict” a given outcome (\\(Y\\)) as a function of an independent variable (\\(X\\)). Building on last week’s material, this week we will study the scenario in which both the dependent variable and the independent variable are numerical. It is the same linear model as last week; in fact, we can always use linear regression models when the dependent variable is numerical. For example, next week, we will expand it even further and learn about models with categorical independent variables as well as models with multiple explanatory variables simultaneously—what we call multiple regression models. We will use a new dataset today, specifically the data used by Patrick Sharkey and his colleagues, to study the association between non-profit organisations and crime levels. In “Uneasy Peace”, Prof Sharkey argues that one factor contributing to the decline of crime from the 1990s onwards was the role played by non-profit community organisations in bringing peace and services to deteriorated neighbourhoods. Watch this video to gain a more theoretical background and learn about the research. In this session, we will use the replication data from one of the papers that Prof Sharkey published to study this question. This data is found in the Harvard Dataverse. If you are interested in the specific study analysing this data, you can find it here. # create an object with the URL address of the dataset urlfile &lt;- &quot;https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/46WIH0/ARS2VS&quot; # import the dataset into R communitycrime &lt;- read.table(urlfile, sep = &#39;\\t&#39;,header = T) As before, we create an object with the permanent URL address, and then we use a function to read the data into R. The data can be saved using an api, and it is in tab-separated format. We use the read.table function from base R for this. We pass two arguments to the function sep= '\\t', telling R this file is tab separated. The header = T function tells R that it is TRUE (T) that this file has a first row that acts as a header (this row has the name of the variables). There are many more variables here that we are going to need, so let’s do some filtering and selection. We will focus on 2012, the most recent year in the dataset, and just a few select variables. # load the dplyr package for data cleaning library(dplyr) # create a new object, &#39;df&#39;, that only includes # observations from 2012 df &lt;- filter(communitycrime, year == &quot;2012&quot;) # select just some variables from the dataset df &lt;- select(df, place_name, state_name, viol_r, black, lesshs, unemployed, fborn, incarceration, log_incarceraton, swornftime_r, log_viol_r, largest50) So now we have a more manageable data set that we can use for this session. The file includes a sample of 264 US cities (see variable place_name) across 44 of states (variable state_name). As ever, we always start understanding what our unit of analysis is (i.e., what each row represents). In this case, our unit of analysis are cities in the US: each row of the data frame df represents a different city. Then we have information (variables) on those cities, such as their level of violence and their unemployment rate. You can check the names of each column (i.e., each variable) included in the dataset by using the names() function. # print the names of all columns in the dataset &#39;df&#39; names(df) ## [1] &quot;place_name&quot; &quot;state_name&quot; &quot;viol_r&quot; &quot;black&quot; ## [5] &quot;lesshs&quot; &quot;unemployed&quot; &quot;fborn&quot; &quot;incarceration&quot; ## [9] &quot;log_incarceraton&quot; &quot;swornftime_r&quot; &quot;log_viol_r&quot; &quot;largest50&quot; The variables we have extracted contain information on the demographic composition of those cities (per cent black population, per cent without a high school degree, per cent unemployed, per cent foreign-born), and some criminal justice ones (incarceration rate and the rate of sworn full-time police officers). We also have measures of the violence rate and a binary indicator that tells us if the city is one of the 50 largest in the country. We will examine the relationship between unemployment and violence. Based on Sharkey’s book, we would expect that cities with a larger percentage of unemployed residents would also have larger violence rates. Therefore, violence rate is our dependent variable, and unemployment percentage is our independent variable. They are both numerical variables. Let’s start examining the following scatterplot. On the Y-axis, we have the distribution of our dependent variable, violence rate, ranging from 0 to just over 2000. On the X-axis, we have the distribution of our independent variable, unemployment percentage, ranging from 0 to 14. Each dot in the scatterplot represents one of the 264 cities in our dataset. # load the &#39;ggplot2&#39; package library(ggplot2) # plot a scatterplot ggplot(df, aes(x = unemployed, y = viol_r)) + geom_point() What do you think when looking at this scatterplot? Is there a relationship between violence and unemployment? Does it look like cities with a high score on the X-axis (unemployment) also have a high score on the Y-axis (violent crime)? It may be a bit hard to see, but we think there is certainly a trend. 6.2 Motivating regression Now, imagine that we play a game. Imagine we have all the names of the cities in a hat, and we randomly take one of the names from the hat. You’re sitting in the audience, and you have to guess the level of violence (viol_r) for that city. Imagine paying £150 to the student who gets the closest to the right value. What would you guess if you only had one guess and knew (as we do) how violent crime is distributed across all cities? # plot a density plot with the distribution of violence rate ggplot(df, aes(x = viol_r)) + geom_density() + geom_vline(xintercept = mean(df$viol_r), linetype = &quot;dashed&quot;, size = 1, color=&quot;red&quot;) + ggtitle(&quot;Density estimate and mean of violent crime rate&quot;) summary(df$log_viol_r) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.831 5.532 6.149 6.061 6.595 7.634 If we only had one shot, we would go for the mean (or perhaps the median, given the skew). Most cities have values clustered around those values, which is another way of saying they are bound to be not too far from them. It would be silly to say 15, for example, or 1500, since very few cities have such low or high levels of violence (as measured by viol_r). Imagine, however, that now, when we take the name of the city from the hat, you are also told how much unemployment there is in that city—so the value of the unemployed variable for the city that has been selected (for example, 11). Imagine that you also have the scatterplot we produced earlier in front of you. Would you still go for a value close to 500 (the mean) as your best guess for the value of the selected city? We certainly would not go with the overall mean or median as our prediction any more. If somebody told us that 10% of the population residing in the city was unemployed, we would be more inclined to guess the average level of violence for the cities with that level of unemployment (the conditional mean) rather than the overall mean across all the cities. Wouldn’t you? If we plot the conditional means, we can see that the mean viol_r for cities that report an unemployment rate of 11% is around 1000. So you may be better off guessing that. Linear regression tackles this problem using a slightly different approach. Rather than focusing on the conditional mean (smoothed or not), it draws a straight line that tries to capture the trend in the data. If we focus on the regions of the scatterplot that are less sparse, we see that this is an upward trend, suggesting that as the level of unemployment increases, so does the level of violent crime. Simple linear regression draws a single straight line of predicted values as the model for the data. This line would be a model, a simplification of the real world like any other model (e.g., a toy pistol, an architectural drawing, a subway map) that assumes that there is approximately a linear relationship between \\(X\\) and \\(Y\\). Let’s draw the regression line: # produce a scatterplot and a regression line # summarising the relationship between unemployment # and violence rate ggplot(data = df, aes(x = unemployed, y = viol_r)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, size = 1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The geom_smooth function asks for a geom with the regression line, method = lm asks for the linear regression line, se = FALSE asks for just the line to be printed, and the other arguments specify the colour and thickness of the line. What that line is doing is giving you guesses (predictions) for the values of violent crime based on the information we have about the level of unemployment. It gives you one possible guess for the value of violence for every possible value of unemployment and links them all together in a straight line. Another way to think about this line is as the best possible summary of the cloud of points represented in the scatterplot (if we can assume that a straight line would do a good job of this). If we were to tell you to draw a straight line that best represents this pattern of points, the regression line would be the one that best does it (if certain assumptions are met). The linear model is then a model that takes the form of the equation of a straight line through the data. The line does not go through all the points. In fact, you can see it is a slightly less accurate representation than the (smoothed) conditional means: As De Veaux et al. (2012: 179) highlight: “like all models of the real world, the line will be wrong, wrong in the sense that it can’t match reality exactly. But it can help us understand how the variables are associated”. A map is never a perfect representation of the world; the same happens with statistical models. Yet, as with maps, models can be helpful. 6.3 Fitting a simple regression model In order to draw a regression line, we need to know two things: (1) We need to know where the line begins: what is the value of \\(Y\\) (our dependent variable) when \\(X\\) (our independent variable) is 0 so that we have a point from which to start drawing the line. The technical name for this point is the intercept. (2) And we need to know what is the slope of that line, that is, how inclined the line is, the angle of the line. If you recall from elementary algebra (and you may not), the equation for any straight line is: \\(Y = m \\cdot X + c\\) In statistics, we use a slightly different notation, although the equation remains the same: \\(Y = \\alpha + \\beta \\cdot X\\) We need the origin of the line (\\(\\alpha\\)) and the slope of the line (\\(\\beta\\)). How does R get the intercept and the slope for the red line? How does R know where to draw this line? We need to estimate these parameters (or regression coefficients) from the data. How? We don’t have time to get into these more mathematical details now. You should study the required reading to understand this (required means it is required, it is not optional)7. For now, suffice to say that for linear regression models like the one we cover here when drawing the line, R tries to minimise the distance from every point in the scatterplot to the regression line using a method called least squares estimation. In order to fit the model, we use the lm() function using the formula specification (Y ~ X)—exactly as we did last week. Typically, you want to store your regression model into an object. For example, let’s fit a linear model regression violence rates on percentage of unemployment and store the results under an object named fit_1: # run a regression model where violence rate is # the dependent variable and per cent unemployed # is the independent variable fit_1 &lt;- lm(viol_r ~ unemployed, data = df) In your R Studio global environment space, you will see a new object called fit_1 with 12 elements on it. We can get a sense for what this object is and includes using the functions we introduced in previous weeks: # check the class of the object &quot;fit_1&quot; class(fit_1) ## [1] &quot;lm&quot; # check the attributes of the object &quot;fit_1&quot; attributes(fit_1) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; ## ## $class ## [1] &quot;lm&quot; R is telling us that this is an object of class lm and includes several attributes. One of the beauties of R is that you are producing all the results from running the model, putting them in an object, and then giving you the opportunity to use them later on. If you want to see the basic results from running the model, you can simply print the name of the object. # print results of the regression model &quot;fit_1&quot; fit_1 ## ## Call: ## lm(formula = viol_r ~ unemployed, data = df) ## ## Coefficients: ## (Intercept) unemployed ## -213.7 121.7 This is exactly the same as we did last week. The output of the lm() function in R provides us with two numbers under “Coefficients:”. The value of -213.7 is the estimated value for the Intercept (i.e., \\(\\widehat{\\alpha}= -213.7\\)). This indicates the predicted score of \\(Y\\) when \\(X\\) equals zero. In this case, it indicates that a hypothetical city with 0% of its population under unemployment would have an expected violence rate of -213.7. We then need the \\(\\widehat{\\beta}\\) regression coefficient for our independent variable, the value that will shape the slope in this scenario. This value is 121.7. This estimated regression coefficient for our independent variable has a convenient interpretation. When the value is positive, it tells us that every one unit increase in \\(X\\) is associated with a \\(\\beta\\) increase on \\(Y\\). If the coefficient is negative, then it represents a decrease in \\(Y\\). Here, we can read it as “every one unit increase in the percentage of people unemployed is associated with a 121.7 unit increase in violence rate.” Knowing these two parameters not only allows us to draw the line, but we can also solve for any given value of \\(X\\). Let’s go back to our guess-the-violence game. Imagine if we tell you the unemployment percentage is 4. What would be your best bet now? We can go back to our regression line equation and insert the estimated parameters: \\(Y = \\alpha + \\beta \\cdot X\\) \\(Y = -213.7 + 121.7 \\cdot 4\\) \\(Y = 273.1\\) Or, if you don’t want to do the calculation yourself, you can use the predict function (differences are due to rounding error): # First, you name your stored model, and then you identify the new data # (which has to be in a data frame format and with a variable name matching # the one in the original data set) predict(fit_1, data.frame(unemployed = c(4))) ## 1 ## 272.9868 This is the expected value of \\(Y\\), violence rate, when \\(X\\), percentage of unemployment, is 4% of the population according to our model (according to our simplification of the real world, our simplification of the whole cloud of points into just one straight line). Look back at the scatterplot we produced earlier with the green line. Does it look like the red line when X is 4 corresponds to a value of Y of 273.1? 6.4 Residuals: R squared In the output, when we ran the model above, we saw something called the residuals. The residuals (in regression) are the differences between the observed values of Y for each case minus the predicted or expected value of Y, in other words, the distances between each point in the dataset and the regression line (see the visual example below). @http://www.shodor.org/ You see, we have our line, our predicted value, and then we have the black dots, which are our observed values. The distance between them is essentially the amount by which we were wrong, and all these distances between observed and predicted values are our residuals. Least square estimation, the “machine” we use to build the regression line, essentially aims to reduce the squared average of all these distances: that’s how it draws the line. Why do we have residuals? Well, think about it. The fact that the line is not a perfect representation of the cloud of points makes sense, doesn’t it? You cannot predict perfectly what the value of Y is for every city just by looking ONLY at unemployment! This line only uses information regarding unemployment. This means there’s bound to be some difference between our predicted level of violence, given our knowledge of unemployment (the regression line) and the actual level of violence (the actual location of the points in the scatterplot). Other things that matter are not being taken into account by our model to predict the values of Y. There are other things that surely matter regarding understanding violence. And then, of course, we have measurement errors and other forms of noise. We can re-write our equation like this if we want to represent each value of Y (rather than the predicted value of Y) then: \\(y = b_0 + b_1x + e(residuals)\\) The residuals capture how much variation is unexplained - how much we still have to learn if we want to understand variation in Y. A good model tries to maximise explained variation and reduce the magnitude of the residuals. We can use information from the residuals to produce a measure of effect size - how good our model is in predicting variation in our dependent variables. Remember our game where we try to guess violence (Y)? If we had no information about X, our best bet for Y would be the mean of Y. The regression line aims to improve that prediction. By knowing X’s values, we can build a regression line that aims to get us closer to the actual values of Y. The distance between the mean (our best guess without any other information) and the observed value of Y is the total variation. The residual is the difference between our predicted value of Y and the observed value of Y. This is what we cannot explain (i.e., variation in Y that is unexplained). The difference between the mean value of Y and the expected value of Y (the value given by our regression line) is how much better we are doing with our prediction by using information about X (i.e., in our previous example, it would be variation in Y that can be explained by knowing about unemployment). How much closer the regression line gets us to the observed values? We can then contrast these two different sources of variation (explained and unexplained) to produce a single measure of how good our model is. The formula is as follows: \\[ R^2 = \\frac{\\text{Explained Variation}}{\\text{Total Variation}} \\] This can also be written as: \\[ R^2 = \\frac{\\sum (\\hat{Y}_i - \\bar{Y})^2}{\\sum (Y_i - \\bar{Y})^2} \\] Where: - \\(Y_i\\): observed values of the dependent variable. - \\(\\hat{Y}_i\\): predicted values from the regression line. - \\(\\bar{Y}\\): mean of the observed \\(Y\\) values. - \\(\\sum (\\hat{Y}_i - \\bar{Y})^2\\): explained variation (the squared differences between the predicted values and the mean of \\(Y\\)). - \\(\\sum (Y_i - \\bar{Y})^2\\): total variation (the squared differences between the observed values and the mean of \\(Y\\)). All this formula is doing is taking a ratio of the explained variation (the squared differences between the regression line and the mean of Y for each observation) by the total variation (the squared differences of the observed values of Y for each observation from the mean of Y). This gives us a measure of the percentage of variation in Y that is “explained” by X. Then, we can take this value as a measure of the strength of our model. If you look at the R output, you will see that the \\(R^2\\) for our model was .29 (look at the multiple R square value in the output). We can say that our model explains 29% of the variance in the fear of violent crime measure. #As an aside, and to continue emphasising your appreciation of the object-oriented nature of R, #when we run the summary() function, we are simply generating a list object of the class summary.lm. attributes(summary(fit_1)) ## $names ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; ## ## $class ## [1] &quot;summary.lm&quot; #This means that we can access its elements if so we wish. #So, for example, to obtain just the R Squared, we could ask for: summary(fit_1)$r.squared ## [1] 0.2991119 Knowing how to interpret this is important. \\(R^2\\) ranges from 0 to 1. The greater it is, the more powerful our model is; the more explaining we are doing, the better we can account for variation in our outcome \\(Y\\) with our input. In other words, the stronger the relationship is between \\(Y\\) and \\(X\\). As with all the other measures of effect size, interpretation is a matter of judgement. You are advised to see what other researchers report in relation to the particular outcome that you may be exploring. This is a reasonable explanation of how to interpret R-Squared. Weisburd and Britt (2009: 437) suggest that in criminal justice, you rarely see values for \\(R^2\\) greater than .40. Thus if your \\(R^2\\) is larger than .40, you can assume you have a powerful model. When, on the other hand, \\(R^2\\) is lower than .15 or .2, the model is likely to be viewed as relatively weak. Our observed r squared here is not too bad. There is considerable room for improvement to develop a better model to explain violence8. In any case, many people would argue that \\(R^2\\) is a bit overrated. You need to be aware of what it measures and the context in which you use it. Read here for some additional detail. 6.5 Is this the same as last week? Yes! Last week, we used linear regression modelling to assess the association between a binary independent variable and a numerical dependent variable. We learned that the estimated intercept (\\(\\alpha\\)) represents the average score of the dependent variable in the reference group (\\(\\bar{Y} \\mid x=0\\)), and that the slope coefficient (\\(\\beta\\)) represents the mean difference (\\((\\bar{Y} \\mid x=1) - (\\bar{Y} \\mid x=0)\\)). Now, we’re taking a deeper dive into linear regression modelling and understanding that fitting a regression model is essentially drawing a line—the line that best represents the linear relationship between an independent variable \\(X\\) and a dependent variable \\(Y\\). In fact, everything we learned today applies for what we studied last week. A binary independent variable is just a special case of linear regression models. Let’s have a look at another independent variable: whether the city is one of the largest 50 cities in the United States or not. The name of the variable is largest50 # print frequency table of variable largest50 table(df$largest50) ## ## 0 1 ## 216 48 48 observations have a score of 1, indicating that they are one of the 50 largest cities in the United States, whereas 216 observations have a score of 0, indicating that they not. Sharkey also hypothesised that violent crime is more common in large cities. So, we can treat violence rate (viol_r) as our numerical dependent variable and whether they are one of the largest 50 cities in the country (largest50) as our new binary independent variable. We can test this association by fitting a linear regression model that estimates parameters \\(\\alpha\\) (the intercept) and \\(\\beta\\) (the slope coefficient, in this case representing the mean difference). But, if today we learned that linear regression models are essentially all about drawing a line, what if we start producing a scatterplot? # produce a scatterplot between violence rate and largest 50 ggplot(data = df, aes(x = largest50, y = viol_r)) + geom_point() This scatterplot does not seem very useful… This is because, as we already knew, our independent variable in this example is binary, which means it can only take two values: 0 or 1. Therefore, all the dots in the scatterplot are stacked on 0 or on 1. But this scatterplot is not entirely useless! If we quickly check, for instance, what the average violence rate among smaller cities is and what the average violence rate among larger cities is, we can try to eyeball those means in the plot # load the dplyr package library(dplyr) # produce grouped means using group_by df %&gt;% group_by(largest50) %&gt;% summarise(average_violence = mean(viol_r)) ## # A tibble: 2 × 2 ## largest50 average_violence ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 486. ## 2 1 824. The average violence rate in smaller cities is 486, whereas the average violence rate is larger cities is 824. Although this scatterplot does not look particularly useful at first, we can spot those two means in each distribution of “stacked dots”. But isn’t linear regression modelling essentially just about drawing a line? What if simply draw a line connecting the two means? That’s exactly what linear regression does! It draws a line. This line, because the independent variable is binary, only goes from 0 (a group representing smaller cities) to 1 (a group representing larger cities). If we fit a linear regression model: # fit a linear regression model lm(viol_r ~ largest50, data = df) ## ## Call: ## lm(formula = viol_r ~ largest50, data = df) ## ## Coefficients: ## (Intercept) largest50 ## 485.6 338.0 We can see that the estimated Intercept is \\(\\widehat{\\alpha}=485.6\\). This is the expected score of the dependent variable \\(Y\\) (i.e., violence rates) in the hypothetical scenario of \\(x=0\\)—but it just so happens that, in this case, \\(x=0\\) represents the group of smaller cities, so the intercept has a much more meaningful interpretation! Similarly, we can see that the slope coefficient is \\(\\widehat{\\beta} = 338\\). As we learned today, we could simply interpret this number by stating that every unit increase in the variable largest50 is associated with a 338-unit increase in violence rates. However, the variable largest50 cannot increase by one unit in the conventional sense, as it is binary; or rather, it can only increase by one unit. The only possible change in this variable is between \\(x = 0\\) (representing the group of smaller cities) and \\(x = 1\\) (representing the group of larger cities)—it’s the line going from the mean when X equals 0 all the way to the mean when X equals 1. Therefore, the idea of a “one-unit increase” for a binary variable necessarily implies the difference between the estimated scores of the two groups. This is why we interpret the slope coefficient (\\(\\beta\\)) for binary variables as the mean difference—this is a special case of linear regression models! 6.6 Regression assumptions Like other models and statistical tests, the regression model also makes assumptions of its own. In fact, there are so many that we could spend an entire class discussing them. Gelman and Hill (2007) point out that the most important regression assumptions by decreasing order of importance are: Validity. The data should be appropriate for the question that you are trying to answer: “Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to all cases to which it will be applied… Data used in empirical research rarely meet all (if any) of these criteria precisely. However, keeping these goals in mind can help you be precise about the types of questions you can and cannot answer reliably.” Additivity and linearity. These are the most important mathematical assumptions of the model. You can include interaction effects in your models if the additivity assumption is violated. This is something we will discuss in more detail in future sessions. Similarly, we will discuss problems with non-linearities. If the relationship is non-linear (e.g., it is curvilinear), predicted values will be wrong in a biased manner, meaning that predicted values will systematically miss the true pattern of the mean of y (as related to the x-variables). Independence of errors. Regression assumes that the errors from the prediction line (or hyperplane for multiple regression) are independent. Suppose there is a dependency between the observations (you are assessing change across the same units, working with spatial units, or with units that are somehow grouped, such as students from the same class). In that case, you may have to use more appropriate models (e.g., multilevel models, spatial regression, etc.). Equal variances of errors. When the variance of the residuals is unequal, you may need different estimation methods. This is, nonetheless, considered a minor issue. There is a small effect on the validity of t-test and F-test results, but generally, regression inferences are robust with regard to the variance issue. Normality of errors. The residuals should be normally distributed. Gelman and Hill (2007: 46) discuss this as the least important of the assumptions and, in fact, “do not recommend diagnostics of the normality of the regression residuals”. If the errors do not have a normal distribution, it usually is not particularly serious. Regression inferences tend to be robust with respect to normality (or nonnormality of the errors). In practice, the residuals may appear to be nonnormal when the wrong regression equation has been used. So, we will show you how to inspect the normality of the residuals, not because this is a problem in itself, but because it may give you further evidence that there is some other problem with the model you are applying to your data. So, these are the assumptions of linear regression. In this section, we can go through very quickly how to test some of them using visuals. While finding that some of the assumptions are violated does not necessarily mean that you have to scrap your model, it is important to use these diagnostics to illustrate that you have considered what the possible issues with your model are and if you find any serious issues that you address them. In R, we can use the plot() function on our output lm object to look through some diagnostics. This gives us 4 plots, so to show them all, we’ll use the code par(mfrow = c(2, 2)) to split our plot window into 4 panes (remember to set back, run par(mfrow = c(1, 1))). Let’s return to fit_1, our model. par(mfrow = c(2, 2)) plot(fit_1) The 4 plots we get are: Residuals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns, is an indication of a linear relationship, which is good. Normal Q-Q. Used to examine whether the residuals are normally distributed. It’s good if residual points follow the straight dashed line. Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). A horizontal line with equally spread points is a good indication of homoscedasticity. Residuals vs Leverage. Used to identify influential cases, that is, extreme values that might influence the regression results when included or excluded from the analysis. In our example, we have issues with all four of them. This is probably due to the fact that our dependent variable is not normally distributed—check the histogram in the beginning of the section again—indicating that we probably have a non-linear relationship between unemployment and violence rates. We will revisit this in future sessions. For the purposes of this module, it is enough that you understand that these assumptions of regression exist, what they mean, and how you might test for them. For some of them, for example, additivity, we will discuss some ways to address this in future weeks in sections focused on interaction effects. For others, it is just important to keep in mind when these might be violated and raise these as possible limitations in your ability to rely on the conclusions you draw from your results. 6.7 Lab Exercises Your turn! In the lab session, using the same data set, answer the following questions. Note that not all questions necessarily require analysing data in R. After you finish, click on ‘Reveal answer!’ to check your answers. We start revisiting the communitycrime dataset (which we loaded in the beginning of the section) to select new variables: # select new variables from the raw dataset df_homework &lt;- communitycrime %&gt;% filter(year == 2012) %&gt;% select(place_name, state_name, burglary_r, fborn, largest100) Using data from 264 cities in the United States in 2012, you want to assess whether the percentage or residents who are foreign born is associated with burglary rates. Your variables of interest are burglary_r, reflecting the burglary rate of the cities in the United States; fborn, indicating the percentage of foreign-born residents; and largest100, indicating whether the city is one of the 100 largest cities in the US. Based on your research hypothesis, what are your dependent and independent variables? Reveal answer! Dependent variable: burglary rates, i.e., burglary_r. Independent variable: percentage of foreign-born population, i.e., fborn. Is your dependent variable numerical or categorical? If categorical, is it binary, ordinal, or multinomial? What about the independent variable? Reveal answer! The dependent variable, burglary rates (burglary_r), is a numerical variable. The independent variable, percentage of foreign born (fborn), is also a numerical variable. Using your dependent and independent variables, what is your null hypothesis? Reveal answer! Null hypothesis: there is no association between the percentage of foreign-born residents and the burglary rate in cities in the United States. Examine the relationship between your dependent and your independent variable using the appropriate visualisation strategies Reveal answer! We can produce a scatterplot to assess the relationship # produce scatterplot ggplot(df_homework, aes(x = fborn, y = burglary_r)) + geom_point() Based on the scatterplot, we could expect a negative relationship: a higher concentration of foreign-born residents appears to be associated with fewer burglaries. We can check the regression line. # produce scatterplot ggplot(df_homework, aes(x = fborn, y = burglary_r)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, size = 1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Let’s build a regression model. Replacing \\(Y\\) with the name of your dependent variable, and \\(X\\) with the name of your independent variable, write down the equation with the unknown parameters (i.e, \\(\\alpha\\) and \\(\\beta\\)) that you want to estimate. Note: this question does not involve any data analysis. Reveal answer! \\[ burglary\\_r = \\alpha + \\beta \\cdot fborn \\] Using the lm() function, estimate the parameters of your linear regression model. Rewrite the equation above replacing unknown parameters with the estimated parameters. Reveal answer! # fit linear regression model lm(burglary_r ~ fborn, data = df_homework) ## ## Call: ## lm(formula = burglary_r ~ fborn, data = df_homework) ## ## Coefficients: ## (Intercept) fborn ## 1148.12 -14.67 Based on the results of the linear regression model, we can rewrite the equation in the following way: \\[ burglary\\_r = 1148.12 - 14.67 \\cdot fborn \\] This indicates that the expected level of burglary rates in a hypothetical city with 0% foreign-born residents would be 1148.12. Every one-unit increase in the percentage of foreign-born residents is associated with a decrease of 14.67 units of burglary rates. Based on your regression model, what is the expected burglary rate in a city with 1% foreign-born residents? What about a city with 10%? And 50%? Reveal answer! We can use the regression model to make those predictions. We can calculate manually or use R as a calculator. # fit regression model regression_model &lt;- lm(burglary_r ~ fborn, data = df_homework) # predict burglary scores when foreign born is 1 predict(regression_model, data.frame(fborn = c(1))) ## 1 ## 1133.445 # predict burglary scores when foreign born is 10 predict(regression_model, data.frame(fborn = c(10))) ## 1 ## 1001.37 # predict burglary scores when foreign born is 50 predict(regression_model, data.frame(fborn = c(50))) ## 1 ## 414.3705 The predicted burglary rate in a city with 1% of foreign-born residents is 1133.45. In a city with 10% of foreign-born residents, it is 1101.37. In a city with 50% foreign-born residents, it is 414.37. What is the \\(R^2\\) of your regression model? Reveal answer! # extract R Squared summary(regression_model)$r.squared ## [1] 0.1319607 The \\(R^2\\) is 0.1319, suggesting that 13.19% of the variance in burglary rates is explained by this regression model. This is a fine chapter too if you struggle with the explanations in the required reading. Many universities, like the University of Manchester, have full access to Springer ebooks. You can also have a look at these notes.↩︎ This is a reasonable explanation of how to interpret R-Squared.↩︎ "],["regression-iii.html", "Chapter 7 Regression III 7.1 Fitting regression with categorical predictors 7.2 Motivating multiple regression 7.3 Fitting and interpreting a multiple regression model 7.4 Presenting your regression results. 7.5 Rescaling input variables to assist interpretation 7.6 Testing conditional hypothesis: interactions 7.7 Multicollinearity", " Chapter 7 Regression III Following the introduction of regression models in previous weeks, we will develop these models further in this session. How do we interpret categorical variables used in regression? What are interaction effects, and why we might need them? These are some of the questions we will address this week. 7.1 Fitting regression with categorical predictors In previous weeks, we explained regression using a numeric input. It turns out we can also use regression with categorical explanatory variables. It is quite straightforward to run it. We will use the same dataset from previous weeks. We will use the replication data from one of the papers that Prof Sharkey published to study what contributed to the decline of crime from the 90s. This data is found in the Harvard Dataverse. If you are interested in the specific study analysing this data, you can find it here. urlfile &lt;- &quot;https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/46WIH0/ARS2VS&quot; communitycrime &lt;- read.table(urlfile, sep = &#39;\\t&#39;,header = T) As before, we create an object with the permanent URL address, and then we use a function to read the data into R. We use the read.table function from base R for this. We pass two arguments to the function sep= '\\t', telling R this file is tab separated. The header = T function tells R that it is TRUE (T) that this file has a first row that acts as a header (this row has the name of the variables). There are many more variables here than we will need, so let’s do some filtering and selection. We will focus on 2012, the most recent year in the dataset, and just a few select variables. library(dplyr) df &lt;- filter(communitycrime, year == &quot;2012&quot;) df &lt;- select(df, place_name, state_name, viol_r, black, lesshs, unemployed, fborn, incarceration, log_incarceraton, swornftime_r, log_viol_r, largest50) So now we have a more manageable data set that we can use for this session. The file includes a sample of 264 US cities (see place_name) across 44 states. See previous weeks for code on how to figure out what these states are. We have one categorical variable in the dataset, largest50, identifying whether the city is one of the 50 largest in the country. table(df$largest50) ## ## 0 1 ## 216 48 class(df$largest50) ## [1] &quot;numeric&quot; This variable is, however, stored in a numeric vector. We may want to change this to reflect the fact it is categorical. df$largest50 &lt;- as.factor(df$largest50) class(df$largest50) ## [1] &quot;factor&quot; Let’s rename the levels. In previous sessions, we illustrated how to do that with base R functions. Here, we introduce a new package, forcats, worth considering when doing any work with factor variables. You can read more about it here. library(forcats) df$largest50 &lt;- fct_recode(df$largest50, Yes = &quot;1&quot;, No = &quot;0&quot;) table(df$largest50) ## ## No Yes ## 216 48 We can explore if particularly large cities have higher rates of violence (remember, a rate controls for population size, so if this were to be significant, it would be telling us that it’s not just because there are more people in them!). This is how you would express the model: fit_2 &lt;- lm(log_viol_r ~ largest50, data=df) Notice that there is nothing different in how we ask for the model compared to last week. And see below the regression line: Although in the plot, we still see a line, what we are really estimating here is the average of log_viol_r for each of the two categories. Let’s have a look at the results: summary(fit_2) ## ## Call: ## lm(formula = log_viol_r ~ largest50, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.11616 -0.48286 0.02965 0.51523 1.49789 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.94763 0.04771 124.661 &lt; 2e-16 *** ## largest50Yes 0.62114 0.11189 5.551 6.94e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7012 on 262 degrees of freedom ## Multiple R-squared: 0.1052, Adjusted R-squared: 0.1018 ## F-statistic: 30.82 on 1 and 262 DF, p-value: 6.943e-08 As you can see, the output does not look too different. But notice that in the printout, you see how the row with the coefficient and other values for our input variable largest50 we see that R is printing largest50Yes. What does this mean? When you only have one predictor, the value of the intercept is the mean value of what we call the reference category, and the coefficient for the slope tells you how much higher (if it is positive) or how much lower (if it is negative) is the mean value for the other category in your factor. The reference category is the one for which R does not print the level next to the name of the variable for which it gives you the regression coefficient. Here we see that the named level is “Yes” (largest50Yes). That’s telling you that the reference category here is “No”. Therefore, the Y-intercept, in this case, is the mean value of violence for cities that are not the largest in the country, whereas the coefficient for the slope tells you how much higher the mean value is for the largest cities in the country. Don’t believe me? #Compute the mean for the smaller cities mean(df$log_viol_r[df$largest50 == &quot;No&quot;], na.rm=TRUE) ## [1] 5.947628 #Compute the difference between the two means mean(df$log_viol_r[df$largest50 == &quot;Yes&quot;], na.rm=TRUE) - mean(df$log_viol_r[df$largest50 == &quot;No&quot;], na.rm=TRUE) ## [1] 0.6211428 So, to reiterate, the coefficient for a single binary predictor in a linear regression model often represents the difference in the means of the outcome variable between the two groups defined by the binary predictor (the two groups). With categorical variables encoded as factors, you always have a situation like this: a reference category and then as many additional coefficients as there are additional levels in your categorical variable. Each of these additional categories is input into the model as “dummy” variables. Here, our categorical variable has two levels. Thus, we have only one dummy variable. There will always be one fewer dummy variable than the number of levels. The level with no dummy variable, females in this example, is known as the reference category or the baseline. It turns out the regression table is printing out a test of statistical significance for every input in the model. If we look at the table above, the p value associated with it is near 0. This is indeed considerably lower than the conventional significance level of 0.05. So, we could conclude that the probability of obtaining this value if the null hypothesis is true is very low. However, the observed r squared is also kind of poor. Read this to understand a bit more about this phenomenon of low p, but also low r-squared. If, rather than a binary explanatory variable, you had a factor with five levels. Then, if you were to run a regression model, this would result in a model with 4 dummy variables. The coefficient of each of these dummies would tell you how much higher or lower (if the sign were negative) the level of violence for each of the levels for which you have a dummy compared to your reference category or baseline is. One thing that is important to keep in mind is that R, by default, will use the first level in your factor as the baseline category. levels(df$largest50) ## [1] &quot;No&quot; &quot;Yes&quot; In our case, you can see “No” listed first. Keep in mind for your assignment that levels in factors are often alphabetically listed, not in a particularly meaningful or useful way. If you want to change this, you may need to reorder the levels. See here for instructions on how to do this. 7.2 Motivating multiple regression So, we saw above and in previous weeks that our models with just one predictor are not terribly powerful. Partly, that’s due to the fact that they are not properly specified; they do not include a solid set of good predictor variables that can help us explain variation in our response variable. We can build better models by expanding the number of predictors (although keep in mind you should also aim to build models as parsimonious as possible). Another reason why it is important to think about additional variables in your model is to control for spurious correlations (although here, you may also want to use your common sense when selecting your variables!). You must have heard before that correlation does not equal causation. Just because two things are associated, we cannot assume that one is the cause for the other. Typically, we see how the pilots switch on the secure belt button when there is turbulence. These two things are associated - they tend to come together. But the pilots are not causing the turbulences by pressing a switch! The world is full of spurious correlations, associations between two variables that should not be taken too seriously. You can explore a few here. It’s funny. Looking only at covariation between pairs of variables can be misleading. It may lead you to conclude that a relationship is more important than it really is. This is no trivial matter, but one of the most important ones we confront in research and policy. It’s not an exaggeration to say that most quantitative explanatory research is about trying to control for the presence of confounders, variables that may explain away observed associations. Think about any criminology question: Does marriage reduce crime? Or is it that people who get married are different from those who don’t (and are those pre-existing differences that are associated with less crime)? Do gangs lead to more crime? Or is it that young people who join gangs are more likely to be offenders to start with? Are the police being racist when they stop and search more members of ethnic minorities? Or are there other factors (i.e., offending, area of residence, time spent in the street) that, once controlled, would mean no ethnic dis-proportionality in stop and searches? Does a particular program reduce crime? Or is the observed change due to something else? These things also matter for policy. Wilson and Kelling, for example, argued that signs of incivility (or antisocial behaviour) in a community lead to more serious forms of crime later on as people withdraw to the safety of their homes when they see those signs of incivilities and this leads to a reduction in informal mechanisms of social control. All the policies to tackle antisocial behaviour in this country are very much informed by this model and were heavily influenced by the broken windows theory. But is the model right? Sampson and Raudenbush argue that this is not entirely correct. They argued and tried to show that other confounding (poverty, collective efficacy) factors explain the association between signs of incivility and more serious crime. In other words, the reason why you see antisocial behaviour in the same communities that you see crime is because other structural factors explain both of those outcomes. They also argue that perceptions of antisocial behaviour are not just produced by observed antisocial behaviour but also by stereotypes about social class and race. If you believe them, then the policy implications are that only tackling antisocial behaviour won’t help you to reduce crime (as Wilson and Kelling have argued). So, as you can see, this stuff matters for policy, not just for theory. Multiple regression is one way of checking the relevance of competing explanations. You could set up a model where you try to predict crime levels with an indicator of broken windows and an indicator of structural disadvantage. If, after controlling for structural disadvantage, you see that the regression coefficient for broken windows is still significant, you may be onto something, particularly if the estimated effect is still large. If, on the other hand, the regression coefficient of your broken windows variable is no longer significant, then you may be tempted to think that perhaps Sampson and Raudenbush were onto something. 7.3 Fitting and interpreting a multiple regression model It could not be any easier to fit a multiple regression model. You simply modify the formula in the lm() function by adding terms for the additional inputs. fit_3 &lt;- lm(log_viol_r ~ unemployed + largest50, data=df) summary(fit_3) ## ## Call: ## lm(formula = log_viol_r ~ unemployed + largest50, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7338 -0.4047 -0.0189 0.4531 1.5726 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.54871 0.14101 32.258 &lt; 2e-16 *** ## unemployed 0.22625 0.02186 10.351 &lt; 2e-16 *** ## largest50Yes 0.53495 0.09476 5.645 4.29e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5915 on 261 degrees of freedom ## Multiple R-squared: 0.3657, Adjusted R-squared: 0.3608 ## F-statistic: 75.23 on 2 and 261 DF, p-value: &lt; 2.2e-16 With more than one input, you need to ask yourself whether all of the regression coefficients are zero. We are assuming the residuals are normally distributed, though with large samples, the F statistics approximate the F distribution. You see the F test printed at the bottom of the summary output and the associated p-value, which, in this case, is way below the conventional .05 that we use to declare statistical significance and reject the null hypothesis. At least one of our inputs must be related to our response variable. Notice that the table printed also reports a t-test and p-value for each of the predictors. These test whether each of these predictors is associated with the response variable when adjusting for the other variables in the model. They report the “partial effect of adding that variable to the model” (James et al. 2014: 77). In this case, we can see that both variables seem to be significantly associated with the response variable. If we look at the r squared, we can see that it is higher than before. This quantity, r squared, will always increase as a consequence of adding new variables, even if the new variables added are weakly related to the response variable. However, the increase we are observing suggests that adding these two variables results in a substantial improvement to our previous model. We see that the coefficients for the predictors change very little; it goes down a bit for unemployed, and it goes down somehow more for largest50. But their interpretation now changes. A common interpretation is that now the regression for each variable tells you about changes in Y related to that variable when the other variables in the model are held constant. So, for example, you could say the coefficient for unemployed represents the increase in the log of the violence rate for every one-unit increase in the measure of unemployment when holding all other variables in the model constant (in this case, that refers to holding constant largest50). But this terminology can be a bit misleading. Other interpretations are also possible and are more generalizable. Gelman and Hill (2007: p. 34) emphasise what they call the predictive interpretation which considers how “the outcome variable differs, on average, when comparing two groups of units that differ by 1 in the relevant predictor while being identical in all the other predictors”. So if you’re regressing y on u and v, the coefficient of u is the average difference in y per difference in u, comparing pairs of items that differ in u but are identical in v. So, for example, in this case, we could say that comparing respondents who have the same level of unemployment but who differ in whether they are one of the largest cities or not, the model predicts an expected difference of .53 in their violent crime measure. And cities with the same size category but that differ by 1% point in unemployment, we would expect to see a difference of 0.23 in their violent crime measure. So, we are interpreting the regression slopes as comparisons of observation that differ in one predictor while being at the same levels of the other predictors. As you can see, interpreting regression coefficients can be kind of tricky9. The relationship between the response y and any one explanatory variable can change greatly depending on what other explanatory variables are present in the model. For example, if you contrast this model with the one we run with only largest50 as a predictor, you will notice that the intercept has changed. You can no longer read the intercept as the mean value of violence for smaller cities. Adding predictors to the model changes their meaning. Now, the intercept index is the value of violence for smaller cities that, in addition, score 0 in unemployed. In this case, you don’t have cases that meet this condition (equal to zero in all your predictors). More often than not, there is not much value in bothering to interpret the intercept because it does not represent a real observation in your sample. Something you need to be particularly careful about is to interpret the coefficients in a causal manner. Unless your data comes from an experiment, this is unlikely to be helpful. With observational data, regression coefficients should not be read as indexing causal relations. This sort of textbook warning is, however, often neglectfully ignored by professional researchers. Often authors carefully draw sharp distinctions between causal and correlational claims when discussing their data analysis, but then interpret the correlational patterns in a totally causal way in their conclusion section. This is what is called the causation creep. Beware. Don’t do this tempting as it may be. Comparing the simple models with this more complex model, we could say that adjusting for largest50 does not change much the impact of unemployed in violence. Almost the same can be said about the effect of largest50 when holding unemployed fixed. 7.4 Presenting your regression results. Clearly communicating your results is incredibly important. We have seen the tabular results produced by R. If you want to use them in a paper, you may need to tidy up those results. There are a number of packages (textreg, stargazer) that automatise that process. They take your lm objects and produce tables that you can put straight away in your reports or papers. One popular trend in presenting results is the coefficient plot as an alternative to the table of regression coefficients. There are various ways of producing coefficient plots with R for a variety of models. See here for example. We are going to use instead the plot_model() function of the sjPlot package, which makes it easier to produce these sorts of plots. You can find a more detailed tutorial about this function here. See below for an example: library(sjPlot) Let’s try a more complex example: fit_4 &lt;- lm(log_viol_r ~ unemployed + largest50 + black + fborn + log_incarceraton, data=df) plot_model(fit_4) You can further customise this: plot_model(fit_4, title=&quot;Violence across cities&quot;) What you see plotted here are the point estimates (the circles), the confidence intervals around those estimates (the longer the line, the less precise the estimate), and the colours representing whether the effect is negative (red) or positive (blue). There are other packages that also provide similar functionality, like the dotwhisker package, that you may want to explore; see more details here. The sjPlot package also allows you to produce HTML tables for a more professional presentation of your regression tables. For this, we use the tab_model() function. This kind of tabulation may be particularly helpful for your final assignment. tab_model(fit_4)   log viol r Predictors Estimates CI p (Intercept) 5.65 4.19 – 7.12 &lt;0.001 unemployed 0.17 0.12 – 0.22 &lt;0.001 largest50 [Yes] 0.46 0.29 – 0.63 &lt;0.001 black 0.02 0.01 – 0.02 &lt;0.001 fborn -0.01 -0.02 – -0.00 0.001 log incarceraton -0.13 -0.36 – 0.10 0.251 Observations 263 R2 / R2 adjusted 0.508 / 0.499 As before, you can further customise this table. Let’s change, for example, the name displayed for the dependent variable. tab_model(fit_4, dv.labels = &quot;Violence rate (log)&quot;)   Violence rate (log) Predictors Estimates CI p (Intercept) 5.65 4.19 – 7.12 &lt;0.001 unemployed 0.17 0.12 – 0.22 &lt;0.001 largest50 [Yes] 0.46 0.29 – 0.63 &lt;0.001 black 0.02 0.01 – 0.02 &lt;0.001 fborn -0.01 -0.02 – -0.00 0.001 log incarceraton -0.13 -0.36 – 0.10 0.251 Observations 263 R2 / R2 adjusted 0.508 / 0.499 Or you could change the labels for the independent variables: tab_model(fit_4, pred.labels = c(&quot;(Intercept)&quot;, &quot;Percent unemployment&quot;, &quot;Largest cities (Yes)&quot;,&quot;Percent black&quot;, &quot;Percent foreign-born&quot;, &quot;Incarceration rate (log)&quot;), dv.labels = &quot;Violence rate (log)&quot;)   Violence rate (log) Predictors Estimates CI p (Intercept) 5.65 4.19 – 7.12 &lt;0.001 Percent unemployment 0.17 0.12 – 0.22 &lt;0.001 Largest cities (Yes) 0.46 0.29 – 0.63 &lt;0.001 Percent black 0.02 0.01 – 0.02 &lt;0.001 Percent foreign-born -0.01 -0.02 – -0.00 0.001 Incarceration rate (log) -0.13 -0.36 – 0.10 0.251 Observations 263 R2 / R2 adjusted 0.508 / 0.499 Visual displays of the effects of the variables in the model are particularly helpful. The effects package allows us to produce plots to visualise these relationships (when adjusting for the other variables in the model). Here’s an example going back to our model fit_3, which contained unemployment and the dummy for large cities as predictor variables: library(effects) plot(allEffects(fit_3), ask=FALSE) Notice that the line has a confidence interval drawn around it (to reflect the likely impact of sampling variation) and that the predicted means for smaller and largest cities (when controlling for unemployment) also have a confidence interval. 7.5 Rescaling input variables to assist interpretation The interpretation of regression coefficients is sensitive to the scale of measurement of the predictors. This means one cannot compare the magnitude of the coefficients to compare the relevance of variables to predict the response variable. Let’s look at the more recent model. How can we tell which predictors have a stronger effect? summary(fit_4) ## ## Call: ## lm(formula = log_viol_r ~ unemployed + largest50 + black + fborn + ## log_incarceraton, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4546 -0.3523 -0.0009 0.3827 1.5771 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.654459 0.743725 7.603 5.44e-13 *** ## unemployed 0.169895 0.024496 6.936 3.27e-11 *** ## largest50Yes 0.457226 0.085311 5.360 1.86e-07 *** ## black 0.015003 0.002678 5.602 5.44e-08 *** ## fborn -0.010414 0.003101 -3.358 0.000904 *** ## log_incarceraton -0.133946 0.116461 -1.150 0.251158 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.523 on 257 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.5082, Adjusted R-squared: 0.4987 ## F-statistic: 53.12 on 5 and 257 DF, p-value: &lt; 2.2e-16 We just cannot. One way of dealing with this is by rescaling the input variables. A common method involves subtracting the mean and dividing it by the standard deviation of each numerical input. The coefficients in these models are the expected difference in the response variable, comparing units that differ by one standard deviation in the predictor while adjusting for other predictors in the model. Instead, Gelman (2008) has proposed dividing each numeric variable by two times its standard deviation so that the generic comparison is with inputs equal to plus/minus one standard deviation. As Gelman explains, the resulting coefficients are comparable to untransformed binary predictors. The implementation of this approach in the arm package subtracts the mean of each binary input while subtracting the mean and dividing it by two standard deviations for every numeric input. The way we would obtain these rescaled inputs uses the standardize() function of the arm package, which takes the name of the stored fit model as an argument. library(arm) standardize(fit_4) ## ## Call: ## lm(formula = log_viol_r ~ z.unemployed + c.largest50 + z.black + ## z.fborn + z.log_incarceraton, data = df) ## ## Coefficients: ## (Intercept) z.unemployed c.largest50 z.black ## 6.06080 0.56923 0.45723 0.49188 ## z.fborn z.log_incarceraton ## -0.24890 -0.08133 Notice the main change affects the numerical predictors. The unstandardised coefficients are influenced by the degree of variability in your predictors, which means they will typically be larger for your binary inputs. With unstandardised coefficients, you are comparing the complete change in one variable (whether one is a large city or not) with one-unit changes in your numerical variable, which may not amount to much change. So, by putting in a comparable scale, you avoid this problem. Standardising in the way described here will help you to make fairer comparisons. These standardised coefficients are comparable in a way that the unstandardised coefficients are not. We can now see what inputs have a comparatively stronger effect. It is very important to realise, though, that one should not compare standardised coefficients across different models. 7.6 Testing conditional hypothesis: interactions In the social sciences, there is a great interest in what are called conditional hypotheses or interactions. Many of our theories do not assume simply additive effects but multiplicative effects. For example, Wikstrom and his colleagues (2011) suggest that the threat of punishment only affects the probability of involvement in crime for those who have a propensity to offend but is largely irrelevant for people who do not have this propensity. Or you may think that a particular crime prevention programme may work in some environments but not in others. The interest in this kind of conditional hypothesis is growing. One of the assumptions of the regression model is that the relationship between the response variable and your predictors is additive. That is if you have two predictors, x1 and x2. Regression assumes that the effect of x1 on y is the same at all levels of x2. If that is not the case, you are then violating one of the assumptions of regression. This is, in fact, one of the most important assumptions of regression (see previous week for details), even if researchers often overlook it. One way of extending our model to accommodate for interaction effects is to add additional terms to our model, a third predictor, x3, where x3 is simply the product of multiplying x1 by x2. Notice we keep a term for each of the main effects (the original predictors) as well as a new term for the interaction effect. “Analysts should include all constitutive terms when specifying multiplicative interaction models except in very rare circumstances” (Brambor et al., 2006: 66). How do we do this in R? One way is to use the following notation in the formula argument. Notice how we have added a third term, unemployed:largest50, which is asking R to test the conditional hypothesis that the size of the cities may have a different impact on the violent crime rate. fit_5 &lt;- lm(log_viol_r ~ unemployed + largest50 + unemployed:largest50 , data=df) # which is equivalent to: # fit_5 &lt;- lm(log_viol_r ~ unemployed * largest50, data=BCS0708) summary(fit_5) ## ## Call: ## lm(formula = log_viol_r ~ unemployed + largest50 + unemployed:largest50, ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.71844 -0.41330 -0.02989 0.45583 1.59625 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.49267 0.15636 28.734 &lt;2e-16 *** ## unemployed 0.23532 0.02443 9.630 &lt;2e-16 *** ## largest50Yes 0.83064 0.36794 2.258 0.0248 * ## unemployed:largest50Yes -0.04557 0.05479 -0.832 0.4063 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5919 on 260 degrees of freedom ## Multiple R-squared: 0.3673, Adjusted R-squared: 0.36 ## F-statistic: 50.32 on 3 and 260 DF, p-value: &lt; 2.2e-16 You see here that, essentially, you have only two inputs (the size of the city and unemployment) but several regression coefficients. Gelman and Hill (2007) suggest reserving the term input for the variables encoding the information and using the term predictor to refer to each of the terms in the model. So here, we have two inputs and four predictors (one for the constant term, one for unemployment, another for the largest 50 dummy, and a final one for the interaction effect). In this case, the test for the interaction effect is non-significant, which suggests there isn’t such an interaction. The R squared barely changes. Let’s visualise the results with the effects package: plot(allEffects(fit_5), ask=FALSE) Notice that essentially, what we are doing is running two regression lines and testing whether the slope is different for the two groups. The intercept is different; we know that largest cities are more violent, but what we are testing here is whether violence goes up in a steeper fashion (and in the same direction) for one or the other group as unemployment goes up. We see that’s not the case here. The estimated lines are almost parallel. A word of warning: the moment you introduce an interaction effect, the meaning of the coefficients for the other predictors changes (what it is often referred to as the “main effects” as opposed to the interaction effect). You cannot retain the interpretation we introduced earlier. Now, for example, the coefficient for the largest50 variable relates to the marginal effect of this variable when unemployment equals zero. The typical table of results helps you to understand whether the effects are significant but offers little of interest that will help you to meaningfully interpret what the effects are. For this, it is better if you use some of the graphical displays we have covered. Essentially, the regression coefficients that get printed are interpretable only for certain groups. So now: The intercept still represents the predicted score of violence for the smaller cities and has a score of 0 in unemployment (as before). The coefficient of the largest50Yes predictor now can be thought of as the difference between the predicted score of violence for small cities that have a score of 0 in unemployment and largest cities that have a score of 0 in unemployment. The coefficient of unemployed now becomes the comparison of mean violence for small cities who differ by one point in unemployment. The coefficient for the interaction term represents the difference in the slope for unemployed comparing smaller and largest cities, the difference in the slope of the two lines that we visualised above. Models with interaction terms are too often misinterpreted. We strongly recommend you read this piece by Brambor et al (2005) to understand some of the issues involved. When discussing logistic regression we will return to this and will consider tricks to ease the interpretation. Equally, John Fox (2003) piece on the effects package goes into much more detail than we can explain here - the logic and some of the options that are available when producing plots to show interactions with this package. You may also want to look at the newer interactions package here. 7.7 Multicollinearity The regression model may experience problems when there is strong collinearity—when the predictors are related to each other. In these scenarios, the standard errors for the coefficients will be large, and the confidence intervals will be broader. Thus, it will be more difficult to detect a significant relationship even if there is one. To diagnose multicollinearity, the first step may be to observe the correlation between the predictors in the model. Let’s first subset the data to obtain only the relevant predictors and our main outcome. For this, I will introduce another form of selecting columns in a data frame using the dplyr package. library(MASS) library(dplyr, quietly=TRUE, warn.conflict=FALSE) data &lt;- Boston Boston_R &lt;- dplyr::select(data, crim, lstat, dis, chas, medv) Then, we can obtain the correlations using the cor() function. c_1 &lt;- cor(Boston_R) c_1 ## crim lstat dis chas medv ## crim 1.00000000 0.4556215 -0.37967009 -0.05589158 -0.3883046 ## lstat 0.45562148 1.0000000 -0.49699583 -0.05392930 -0.7376627 ## dis -0.37967009 -0.4969958 1.00000000 -0.09917578 0.2499287 ## chas -0.05589158 -0.0539293 -0.09917578 1.00000000 0.1752602 ## medv -0.38830461 -0.7376627 0.24992873 0.17526018 1.0000000 And then, we can visualise these correlations with the corrplot package. library(corrplot, quietly = TRUE, warn.conflicts = FALSE) ## Warning: package &#39;corrplot&#39; was built under R version 4.3.3 ## corrplot 0.95 loaded corrplot(c_1, order = &quot;hclust&quot;, tl.pos=&quot;d&quot;) We can see some non-trivial correlations between some of the predictors, particularly between median property value and per cent lower status (r=-73). Correlations among pairs of variables will only give you a first impression of the problem. What we are really concerned about is what happens once you throw all the variables in the model. Not all problems with multicollinearity will be detected by the correlation matrix. The recommended reading describes the variance inflation factor as a tool to diagnose multicollinearity. Let’s look at the one we obtain with the first model (pre-transformations) that we run with the Boston dataset: fit_B1 &lt;- lm(crim ~ medv + lstat + dis + chas, data = Boston) vif(fit_B1) ## medv lstat dis chas ## 2.358517 2.850675 1.399747 1.057865 Typically, a VIF larger than 5 or 10 indicates serious problems with collinearity. Fox (2008) recommends using the square root of the variance inflation factor: sqrt(vif(fit_B1)) ## medv lstat dis chas ## 1.535746 1.688394 1.183109 1.028525 Typically, it is assumed that you need a value greater than 2 for the square root of the variance inflation factor before collinearity seriously impairs the precision of the estimation. It does not look as if we would have to worry much on the basis of these results. When you have a set of related inputs or regressors (either because you have a multicategory factor and various dummy variables or because you have polynomial regressors), you cannot use the variance inflation factor. There is, however, a similar statistic that you can use in these contexts: the generalised variance inflation factor. We could use this measure for our final model for the Boston data. We invoke this statistic using the same code as before: fit_B3 &lt;- lm(log(crim) ~ poly(medv, 2) + log(lstat) + log(dis) + chas, data = Boston) vif(fit_B3) ## GVIF Df GVIF^(1/(2*Df)) ## poly(medv, 2) 4.439803 2 1.451580 ## log(lstat) 4.151409 1 2.037501 ## log(dis) 1.770096 1 1.330450 ## chas 1.058015 1 1.028599 Now that we are using the transformed variables, it looks as if we have more of an issue with the coefficient for lstat. The confidence interval for this coefficient is about 2 times larger than it would be without collinearity, which goes a long way towards explaining why it is no longer significant. The problem with collinear inputs is that they do not add much new to the model. Non-collinearity is not an assumption of the regression model. And everything is related to everything else, to some extent at least. But if a predictor is strongly related to some other input, then we are simply adding redundant information to the model. If you are developing a risk assessment tool for probation, you don’t want to add loads of predictors that are highly correlated (for this simply requires probation to collect redundant information). Also, in these situations is hard to tell apart the relative importance of the collinear predictors (if you are interested in explanation rather than prediction). It can be difficult to separate their effects. A number of solutions to multicollinearity have been proposed, from dropping the culprits to creating composite variables that summarise variation among the correlated inputs to more complex techniques. Still, all these are well beyond the aims of this course. It is very important, however, to remember that there are some circumstances in which multicollinearity can be safely ignored. Professor Paul Allison discusses this issue in his blog, and you may want to check the full discussion. Essentially, Prof. Allison argues that one can ignore multicollinearity when: “The variables with high VIFs are control variables, and the variables of interest do not have high VIFs… In this case, the coefficients of the variables of interest are not affected, and the performance of the control variables as controls is not impaired.” “The high VIFs are caused by the inclusion of powers or products of other variables. If you specify a regression model with both x and x2, there’s a good chance that those two variables will be highly correlated. Similarly, if your model has x, z, and xz, both x and z are likely to be highly correlated with their product. This is not something to be concerned about, however, because the p-value for xz is not affected by the multicollinearity…” “The variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories. If the proportion of cases in the reference category is small, the indicator variables will necessarily have high VIFs, even if the categorical variable is not associated with other variables in the regression model.” We recommend reading chapter 13 “Woes of regression coefficients” of an old book Mostseller and Tukey (1977) Data Analysis and Regression. Reading: Addison-Wesley Publishing.↩︎ "],["studying-relationships-between-two-factors.html", "Chapter 8 Studying relationships between two factors 8.1 Cross-tabulations 8.2 Expected frequencies and Chi-Square 8.3 Odds and odd ratios 8.4 Logistic regression 8.5 Fitting logistic regression 8.6 Assessing model fit: confusion matrix 8.7 Interactions 8.8 Further resources", " Chapter 8 Studying relationships between two factors 8.1 Cross-tabulations In earlier sessions, we covered how to run frequency distributions using the table() function. Cross tabulations, also called contingency tables, are essentially crossed frequency distributions, where you plot the frequency distributions of more than one variable simultaneously. This semester, we are only going to explore two-way cross-tabulations, that is, contingency tables where we plot the frequency distribution of two variables at the same time. Frequency distributions are a useful way of exploring categorical variables that do not have too many categories. By extension, cross-tabulations are a useful way of exploring relationships between two categorical variables that do not have too many levels or categories. As we learned during the first week, we can get results from R in various ways. You can produce basic tables with some of the core functions in R. However, I suggest you install and load the package gmodels to produce the sort of cross-tabs we will use. This package allows you to produce cross-tabulations in a format similar to the one used by commercial statistical packages SPSS and SAS. Since some of you may have previous experience with SPSS, we will use the SPSS format. Cross-tabs with this package are more useful for our purposes than the default you may get with the core R table() function. We will begin the session by loading again the BCS 2007/2008 data from previous weeks. BCS0708&lt;-read.csv(&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/BCS0708.csv&quot;) We will start by producing a cross-tabulation of victimisation (“bcsvictim”), a categorical unordered variable, by whether the presence of rubbish in the streets is a problem in the area of residence (“rubbcomm”), another categorical ordered variable. The broken windows theory would argue that we should see a relationship. We will use the following code: library(gmodels) with(BCS0708, CrossTable(rubbcomm, bcsvictim, prop.chisq = FALSE, format = c(&quot;SPSS&quot;))) ## ## Cell Contents ## |-------------------------| ## | Count | ## | Row Percent | ## | Column Percent | ## | Total Percent | ## |-------------------------| ## ## Total Observations in Table: 11065 ## ## | bcsvictim ## rubbcomm | not a victim of crime | victim of crime | Row Total | ## ------------------|-----------------------|-----------------------|-----------------------| ## fairly common | 876 | 368 | 1244 | ## | 70.418% | 29.582% | 11.243% | ## | 9.950% | 16.276% | | ## | 7.917% | 3.326% | | ## ------------------|-----------------------|-----------------------|-----------------------| ## not at all common | 4614 | 849 | 5463 | ## | 84.459% | 15.541% | 49.372% | ## | 52.408% | 37.550% | | ## | 41.699% | 7.673% | | ## ------------------|-----------------------|-----------------------|-----------------------| ## not very common | 3173 | 981 | 4154 | ## | 76.384% | 23.616% | 37.542% | ## | 36.040% | 43.388% | | ## | 28.676% | 8.866% | | ## ------------------|-----------------------|-----------------------|-----------------------| ## very common | 141 | 63 | 204 | ## | 69.118% | 30.882% | 1.844% | ## | 1.602% | 2.786% | | ## | 1.274% | 0.569% | | ## ------------------|-----------------------|-----------------------|-----------------------| ## Column Total | 8804 | 2261 | 11065 | ## | 79.566% | 20.434% | | ## ------------------|-----------------------|-----------------------|-----------------------| ## ## #In CrossTable(), we are using the name of the variable defining the rows as our first argument, and as our second argument, the name of the variable defining the columns. We are also telling R that we don&#39;t yet want any test of statistical significance and that we want the table to look like it would in SPSS. The cells for the central two columns represent the total number of cases in each category, the row percentages, the column percentages, and the total percentages. So you have, for example, 63 people in the category “rubbish is very common” who were victims of a crime; this represents 30.88% of all the people in the “rubbish is very common” category (your row per cent), 2.79% of all the people in the “victim of a crime” category (your column per cent), and 0.57% of all the people in the sample. Let’s check the level of measurement of the “rubbcomm” variable with the class() function: class(BCS0708$rubbcomm) ## [1] &quot;character&quot; It is categorical, we know, but note that R considers this “character” rather than “factor”, which is what we would like. To make sure that R knows this is a factor, we can convert it with the as.factor() function. PAY ATTENTION: we are not recoding, so we use as.factor() with a dot (as.dot.factor), and we are not using the as_factor() from the haven package, which we would use to recode if this were a *.dta file (it’s not!). BCS0708$rubbcomm &lt;- as.factor(BCS0708$rubbcomm) In the table above, we notice that although “rubbcomm” is an ordinal variable, the order in which it is printed does not make logical sense. We can check the order of the encoding using the levels() function. levels(BCS0708$rubbcomm) ## [1] &quot;fairly common&quot; &quot;not at all common&quot; &quot;not very common&quot; ## [4] &quot;very common&quot; As we can see, the order makes little sense. We should reorder the factor levels to make them follow a logical order. There are multiple ways to do this, some of which we have already seen. This is one possible way of doing it. BCS0708$rubbcomm &lt;- factor(BCS0708$rubbcomm, levels = c(&quot;not at all common&quot;, &quot;not very common&quot;, &quot;fairly common&quot;, &quot;very common&quot;)) You are only interested in the proportions or percentages that allow you to make meaningful comparisons. Although you can do cross-tabs for variables in which a priori you don’t think of one of them as the one doing the explaining (your independent variable) and another to be explained (your dependent variable), most often, you will already be thinking of them in this way. Here, we think of victimisation as the outcome we want to explain and “rubbish in the area” as the factor that may help us explain variation in victimisation. If you have a dependent variable, you need to request only the percentages that allow you to make comparisons across your independent variable (how common rubbish is) for the outcome of interest (victimisation). In this case, with our outcome (victimisation) defining the columns, we would request and compare the row percentages only. On the other hand, if our outcome were the variable defining the rows, we would be interested in the column percentages instead. Pay very close attention to this. It is a very common mistake to interpret a cross tab the wrong way if you don’t do as explained here. To reiterate, there are two rules for producing and reading cross tabs the right way. The first rule for reading cross-tabulations is that if your dependent variable is defining the rows, then you ask for the column percentages. If, on the other hand, you decided that you preferred to have your dependent variable define the columns (as seen here), then you would need to ask for the row percentages. Make sure you remember this. To avoid confusion when looking at the table, you could also modify the code to only ask for the relevant percentages. In this case, we will ask for the row percentages. We can control what gets printed in the main console using the different options of the CrossTable() function. By default, this function prints all the percentages, but most of them are not terribly useful for our purposes here. So, we are going to modify the default options by asking R not to print the column or the total percentages. with(BCS0708, CrossTable(rubbcomm, bcsvictim, prop.chisq=FALSE, prop.c=FALSE, prop.t=FALSE, format=c(&quot;SPSS&quot;))) ## ## Cell Contents ## |-------------------------| ## | Count | ## | Row Percent | ## |-------------------------| ## ## Total Observations in Table: 11065 ## ## | bcsvictim ## rubbcomm | not a victim of crime | victim of crime | Row Total | ## ------------------|-----------------------|-----------------------|-----------------------| ## not at all common | 4614 | 849 | 5463 | ## | 84.459% | 15.541% | 49.372% | ## ------------------|-----------------------|-----------------------|-----------------------| ## not very common | 3173 | 981 | 4154 | ## | 76.384% | 23.616% | 37.542% | ## ------------------|-----------------------|-----------------------|-----------------------| ## fairly common | 876 | 368 | 1244 | ## | 70.418% | 29.582% | 11.243% | ## ------------------|-----------------------|-----------------------|-----------------------| ## very common | 141 | 63 | 204 | ## | 69.118% | 30.882% | 1.844% | ## ------------------|-----------------------|-----------------------|-----------------------| ## Column Total | 8804 | 2261 | 11065 | ## ------------------|-----------------------|-----------------------|-----------------------| ## ## Much less cluttered. Now, we only see the counts and the row percentages. Marginal frequencies appear along the right and the bottom. Row marginals show the total number of cases in each row: 204 people perceive rubbish as very common in the area they’re living in, whereas 1244 perceive rubbish as fairly common in their area, etc. Column marginals indicate the total number of cases in each column: 8804 non-victims and 2261 victims. In the central cells, we see the total number for each combination of categories and now only the row percentage. So, the total in each of those cells is expressed as the percentage of cases in that row. So, for example, 63 people who perceive rubbish as very common in their area who are victims of a crime represent 30.88%% of all people in that row (n=204). If we had asked for the column percentages, the 63 people who live in areas where rubbish is very common and are victims would be divided by the 2261 victims in the study. Changing the denominator when computing the percentage changes the meaning of the percentage. This can sound a bit confusing now. But as long as you remember the first rule we gave you before, you should be fine: if your dependent defines the rows, ask for the column percentages; if your dependent defines the columns, ask for the row percentages. There are always students who get this wrong in the assignments and lose points as a result. Don’t let it be you. The second rule for reading cross-tabulations the right way is this: You make the comparisons across the right percentages (see first rule) in the direction where they do not add up to a hundred. Another way of saying this is that you compare the percentages for each level of your dependent variable across the levels of your independent variable. In this case, we would, for example, compare “What percentage of people who perceive rubbish as common in their area are victims of crime?”. We focus on the second column here (being a victim of a crime) because, typically, that’s what we want to study; this is our outcome of interest (e.g., victimisation). We can see rubbish seems to matter a bit. For example, 30.88% of people who live in areas where rubbish is very common have been victimised. By contrast, only 15.54% of people who live in areas where rubbish is not at all common have been victimised in the previous year. 8.2 Expected frequencies and Chi-Square So far, we have only described our sample. Can we infer that the differences we observed in this sample can be generalised to the population from which this sample was drawn? Every time you draw a sample from the same population, the results will be slightly different, and we will have a different combination of people in these cells. To assess that possibility, we carry out a test of statistical significance. This test allows us to examine the null hypothesis. So, our hypothesis is that there is a relationship between the two variables, while our null hypothesis is that there is no relationship. In this case, the null hypothesis states that in the population from which this sample was drawn, victimisation and how common rubbish is in your area are independent events; that is, they are not related to each other. The materials you read as preparation explained the test appropriate for this case is the chi-square test. You should know that what this test does is to contrast the squared average difference between the observed frequencies and the expected frequencies (divided by the expected frequencies). We can see the expected frequencies for each cell modifying the options of the CrossTable function in the following manner: with(BCS0708, CrossTable(rubbcomm, bcsvictim, expected = TRUE, prop.c = FALSE, prop.t = FALSE, format = c(&quot;SPSS&quot;))) ## ## Cell Contents ## |-------------------------| ## | Count | ## | Expected Values | ## | Chi-square contribution | ## | Row Percent | ## |-------------------------| ## ## Total Observations in Table: 11065 ## ## | bcsvictim ## rubbcomm | not a victim of crime | victim of crime | Row Total | ## ------------------|-----------------------|-----------------------|-----------------------| ## not at all common | 4614 | 849 | 5463 | ## | 4346.701 | 1116.299 | | ## | 16.437 | 64.005 | | ## | 84.459% | 15.541% | 49.372% | ## ------------------|-----------------------|-----------------------|-----------------------| ## not very common | 3173 | 981 | 4154 | ## | 3305.180 | 848.820 | | ## | 5.286 | 20.583 | | ## | 76.384% | 23.616% | 37.542% | ## ------------------|-----------------------|-----------------------|-----------------------| ## fairly common | 876 | 368 | 1244 | ## | 989.804 | 254.196 | | ## | 13.085 | 50.950 | | ## | 70.418% | 29.582% | 11.243% | ## ------------------|-----------------------|-----------------------|-----------------------| ## very common | 141 | 63 | 204 | ## | 162.315 | 41.685 | | ## | 2.799 | 10.899 | | ## | 69.118% | 30.882% | 1.844% | ## ------------------|-----------------------|-----------------------|-----------------------| ## Column Total | 8804 | 2261 | 11065 | ## ------------------|-----------------------|-----------------------|-----------------------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 184.0443 d.f. = 3 p = 1.180409e-39 ## ## ## ## Minimum expected frequency: 41.68495 For example, although 63 people lived in areas where rubbish was very common and experienced victimisation in the past year, under the null hypothesis of no relationship, we should expect this value to be 41.69. Thus, there are more people in this cell than we would expect under the null hypothesis. The Chi-Square test compares these expected frequencies with the ones we actually observe in each of the cells; then averages the differences across the cells and produces a standardised value that we look at a Chi-Square distribution to see how probable/improbable it is. If this absolute value is large, it will have a small p-value associated with it, and we will be in a position to reject the null hypothesis. We conclude that observing such a large chi-square is improbable if the null hypothesis is true. In practice, we don’t actually do any of this. We just run the Chi-Square in our software and look at the p-value. But it is helpful to know what the test is actually doing. Asking for the expected frequencies with CrossTable() automatically prints the Chi-Square test results. In this case, you get a Chi-Square of 184.04, with 3 degrees of freedom. The probability associated with this particular value is nearly zero (1.180e-39). This value is considerably lower than the standard alpha level of .05. So these results would lead us to conclude that there is a statistically significant relationship between these two variables. We can reject the null hypothesis that these two variables are independent in the population from which this sample was drawn. In other words, this significant Chi-Square test means that we can assume that there was indeed a relationship between our indicator of broken windows (perceived disorder, here rubbish) and victimisation in the population of England and Wales in 2007/2008. Notice that R is telling us that the minimum expected frequency is 41.68. Why? Find the answer in the appendix. 8.3 Odds and odd ratios When you have two dichotomous nominal-level variables, which can take only two possible levels, odds ratios are one of the more commonly used measures to indicate the strength of an association. Odds ratios and relative risk are commonly used in public health and criminological research. If you have knowledge of betting, you may already know a thing or two about odds. They are the statistical equivalent of a tongue twister, so don’t worry too much if you need to keep looking at this handout whenever you want to interpret them. We are going to look at the relationship between victimisation and living in a rural/urban setting: with(BCS0708, CrossTable(rural2, bcsvictim, prop.c = FALSE, prop.t = FALSE, expected = TRUE, format = c(&quot;SPSS&quot;))) ## ## Cell Contents ## |-------------------------| ## | Count | ## | Expected Values | ## | Chi-square contribution | ## | Row Percent | ## |-------------------------| ## ## Total Observations in Table: 11676 ## ## | bcsvictim ## rural2 | not a victim of crime | victim of crime | Row Total | ## -------------|-----------------------|-----------------------|-----------------------| ## rural | 2561 | 413 | 2974 | ## | 2373.393 | 600.607 | | ## | 14.830 | 58.602 | | ## | 86.113% | 13.887% | 25.471% | ## -------------|-----------------------|-----------------------|-----------------------| ## urban | 6757 | 1945 | 8702 | ## | 6944.607 | 1757.393 | | ## | 5.068 | 20.028 | | ## | 77.649% | 22.351% | 74.529% | ## -------------|-----------------------|-----------------------|-----------------------| ## Column Total | 9318 | 2358 | 11676 | ## -------------|-----------------------|-----------------------|-----------------------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 98.52709 d.f. = 1 p = 3.206093e-23 ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ------------------------------------------------------------ ## Chi^2 = 98.00261 d.f. = 1 p = 4.178318e-23 ## ## ## Minimum expected frequency: 600.6074 So, we can see that 22% of urban dwellers, compared to 14% of those living in rural areas, have experienced victimisation in the previous year. Living in an urban environment seems to constitute a risk factor or is associated with victimisation. The Chi-Square we obtained has a low p-value, suggesting this association is statistically significant. That is, we can possibly infer that there is an association in the population from which the sample was drawn. But how large is this relationship? This is where odds ratios are handy. Before we get to them, I will discuss a simple tip on layout. Risk ratios and odds ratios are commonly used in the public health tradition. In this tradition, researchers place the disease/condition defining the columns and the treatment or risk factor defining the rows, and they do so in such a way that the first cell corresponds to the intersection of the outcome and the risk factor. And the software that computes odds ratios tends to assume this is how your table is set up. So, whenever you are after the relative risks or odds ratios (that is, whenever you work with a 2X2 table), you should have the table shown like this as well. It will help interpretation: Outcome: Yes Outcome: No Risk factor: Yes Risk factor: No Our table was set up in such a way that the rows are defined by our “risk factor” and the columns by our outcome. But, the first cell represents the intersection of the non-presence of the risk factor and the absence of the outcome. The easiest way to sort this out is to change the order of the levels in our categorical variable identifying the outcome (“bcsvictim”). If we ask R to print the levels of the bcsvictim variable, we will see that they are as follows: print(levels(BCS0708$bcsvictim)) ## NULL print(levels(BCS0708$rural2)) ## NULL You are seeing NULL because these variables are currently character vectors. So we must 1) change them both into factor variables and then 2) reverse the order of the levels. So, “victim of crime” becomes the first level (appears first in the printout), and “urban” becomes the first level as well. There are various ways of doing that with add-on packages; this is an easy way using base R: BCS0708$bcsvictimR &lt;- factor(BCS0708$bcsvictim, levels = c(&#39;victim of crime&#39;, &#39;not a victim of crime&#39;)) print(levels(BCS0708$bcsvictimR)) ## [1] &quot;victim of crime&quot; &quot;not a victim of crime&quot; BCS0708$urban &lt;- factor(BCS0708$rural2, levels = c(&#39;urban&#39;,&#39;rural&#39;)) print(levels(BCS0708$urban)) ## [1] &quot;urban&quot; &quot;rural&quot; We can now rerun the previous cross-tabulation (with the newly created reordered factors), and the table will look as below: with(BCS0708, CrossTable(urban, bcsvictimR, prop.c = FALSE, prop.t = FALSE, expected = TRUE, format = c(&quot;SPSS&quot;))) ## ## Cell Contents ## |-------------------------| ## | Count | ## | Expected Values | ## | Chi-square contribution | ## | Row Percent | ## |-------------------------| ## ## Total Observations in Table: 11676 ## ## | bcsvictimR ## urban | victim of crime | not a victim of crime | Row Total | ## -------------|-----------------------|-----------------------|-----------------------| ## urban | 1945 | 6757 | 8702 | ## | 1757.393 | 6944.607 | | ## | 20.028 | 5.068 | | ## | 22.351% | 77.649% | 74.529% | ## -------------|-----------------------|-----------------------|-----------------------| ## rural | 413 | 2561 | 2974 | ## | 600.607 | 2373.393 | | ## | 58.602 | 14.830 | | ## | 13.887% | 86.113% | 25.471% | ## -------------|-----------------------|-----------------------|-----------------------| ## Column Total | 2358 | 9318 | 11676 | ## -------------|-----------------------|-----------------------|-----------------------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 98.52709 d.f. = 1 p = 3.206093e-23 ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ------------------------------------------------------------ ## Chi^2 = 98.00261 d.f. = 1 p = 4.178318e-23 ## ## ## Minimum expected frequency: 600.6074 To understand odds ratios and relative risk, it is important to understand risks and odds first. The risk is simply the probability of the “outcome” we are concerned about (i.e., victimisation). So, the risk of victimisation for urban dwellers is simply the number of victimised urban dwellers (1945) divided by the total number of urban dwellers (8702). This is .2235. Similarly, we can look at the risk of victimisation for people living in rural areas: the number of victimised countryside residents (413) divided by the total number of residents in rural areas (2974). This is .1388. The relative risk is simply the ratio of these two risks. In this case, this yields 1.60. This suggests that urban dwellers are 1.60 times more likely to be victimised than people who live in rural areas. The odds, on the other hand, contrast the number of individuals with the outcome with those without the outcome for each of the rows. Notice the difference compared to risk. So, the odds of victimisation for urban dwellers equal the number of victimised urban dwellers (1945) by the number of non-victimised urban dwellers (6757). This is .2878. There are .2878 victimised urban dwellers for every non-victimised urban dweller. The odds of victimisation for residents of rural areas equal the number of victimised rural residents (413) by the number of non-victimised rural residents (2561). This is .1612. There are .1612 victimised rural residents for every non-victimised rural resident. The odds ratio is the ratio of these two odds. So, the odds ratio of victimisation for urban dwellers to rural residents is the odds of victimisation for urban dwellers (.2878) divided by the odds of victimisation for rural residents (.1612). This yields 1.78. This means that the odds of victimisation are almost 1.78 times higher for urban dwellers than for residents in rural areas. You can use R to obtain the odds ratios directly. We can use the vcdExtra package (which you may need to install and load). We first create an object with the table and then ask for the ordinary odds ratio using the following code: library(vcd) mytable.1&lt;-table(BCS0708$urban, BCS0708$bcsvictimR) oddsratio(mytable.1, stratum = NULL, log = FALSE) ## odds ratios for and ## ## [1] 1.784947 The oddsratio function here asks for the odds ratio for the table data in the object called mytable.1. The log=FALSE requests an ordinary odds ratio, and the stratum option clarifies that your data are not stratified. What would happen if we used the original variable instead of the recoded one (“bcsvictimR”)? mytable.2&lt;-table(BCS0708$urban, BCS0708$bcsvictim) print(mytable.2) ## ## not a victim of crime victim of crime ## urban 6757 1945 ## rural 2561 413 oddsratio(mytable.2, stratum = NULL, log = FALSE) ## odds ratios for and ## ## [1] 0.5602409 What’s going on? Why do we have a different value here? If you look at the cross-tab, you should be able to understand. R is now computing the odds ratio for “not being a victim of a crime” (since this is what defines the first column). When an odds ratio is below 1, it indicates that the odds in the first row (“urban”) are lower than in the second row (“rural”). Living in urban areas (as contrasted with living in rural areas) reduces the likelihood of non-victimisation. How do you interpret odds ratios? Something important to keep in mind is that odds ratios (and relative risk) are non-negative values; that is, they cannot take negative values (i.e., -1, -3, etc.). They can go from 1 to infinite but only from 1 to zero. If the value is greater than 1, that means that, as in this case, living in an urban area increases the chances of the event, such as being a victim in this example. If the value were to be between 0 and 1, that would mean that the factor in question reduces the chances of the event you are trying to predict. Finally, a value of 1 means that there is no relationship between the two variables: it would mean that the odds or the risks are the same for the two groups. Whenever the odds ratio is between 0 and 1, that means that the odds of whatever it is defining the first column (in the more recent table we printed not being a victim) is lower for the first row (in this case, living in an urban area) than in the second row (in this case living in a rural area). To read an odds ratio between 0 and 1, you first need to take its inverse, so in this case, 1/0.5602 will give you 1.78. What this is telling you is that the odds of not being a victim of crime are 1.78 times less for ‘rural dweller’ than ‘urban dweller’, which is the same as saying that the odds of being a victim of assault are 1.78 more for ‘urban dweller’ than ‘rural dweller’. That is, odds ratios are reciprocal. You may be confused by now. Look at this video; it may help to see an oral presentation of these ideas with a different example. Repeated practice will make it easier to understand. The fact that the interpretation of these quantities is contingent on the way we have laid out our table makes it particularly advisable to hand calculate them as explained above in relation to the outcome you are interested in until you are comfortable with them. This may help you to see more clearly what you are getting and how to interpret it. When looking at the R results for odds ratios, always remember that you are aware of the reference categories (what defines the first column and the first row) when reading and interpreting odds ratios. The R function we introduced will always give you the odds ratio for the event defined in the first column and will contrast the odds for the group defined by the first row with the odds defined by the second row. If the odds ratio is between 0 and 1, that would mean the odds are lower for the first row; if the odds are greater than 1, that would mean the odds are higher for the second row. It is also very important you distinguish the relative risk (a ratio of probabilities) from the odds ratio (a ratio of odds). Be very careful with the language you use when interpreting these quantities. Odds ratios are ratios of odds, not probability ratios. You cannot say that urban dwellers are 1.78 more likely to be victimised. All you can say with an odds ratio is that the odds of being victimised are 1.78 times higher for urban dwellers than for residents in rural areas. Odds and probabilities are different things. It is also very important that you interpret these quantities carefully. You will often see media reports announcing things such as that chocolate consumption will double your risk of some terrible disease. What that means is that the percentage of cases of individuals who take chocolate and present the condition is twice as large as those who do not take chocolate and present the condition. But you also need to know what those percentages are to put it in the right context. If those probabilities are very low to start with, well, does it really matter? increased risk 8.4 Logistic regression In previous sessions, we covered linear regression models, which can be used to model variation in a numerical response variable. Here, we introduce logistic regression, a technique you may use when your outcome or response (or dependent) variable is categorical and has two possible levels. In criminology, very often, you will be interested in binary outcomes (e.g., victim/no victim, arrested/not arrested, etc.) and want to use a number of predictor variables to study these outcomes. It is, then, helpful to understand how to use these models. Logistic regression is part of a broader family of models called generalised linear models. You should read the Wikipedia entry for this concept here. With logistic regression, we model the probability of belonging to one of the levels in the binary outcome. For any combination of values for our predictor variables, the model estimates the probability of presenting the outcome of interest. We use maximum likelihood to fit this model. Although the mathematics behind the method are important, this handout does not explain them. In this introductory module, we only provide an introduction to the technique. To illustrate logistic regression, we will use the Arrests data from the effects package. You can obtain details about this dataset and the variables included by using help(Arrests, package=\"effects\"). If you don’t have that package, you must install and load it. library(effects) data(Arrests, package=&quot;effects&quot;) This data includes information on police treatment of individuals arrested in Toronto for possession of marijuana. We will model variation on released, a factor with two levels indicating whether the arrestee was released with a summons. In this case, the police could: Release the arrestee with a summons - like a parking ticket Bring to the police station, held for bail, etc. - harsher treatment table(Arrests$released) ## ## No Yes ## 892 4334 Most marijuana-possession arrestees are released with a summons. Let’s see if we can develop an understanding of the factors that affect this outcome. In particular, let’s assume our research goal is to investigate whether race is associated with harsher treatment. For this, we may run a logistic regression model. 8.5 Fitting logistic regression It is fairly straightforward to run a logistic model. Before you fit it, though, it is convenient to check what you are modelling. Remember that R orders the levels in a factor alphabetically (unless they have been reordered by the authors of the data frame). What that means is that when you run logistic regression, you will be predicting probabilities associated with the category in a higher alphabetical order. levels(Arrests$released) ## [1] &quot;No&quot; &quot;Yes&quot; #Prints the levels, and you can see #that &quot;Yes&quot; comes after &quot;No&quot; in alphabetical order contrasts(Arrests$released) ## Yes ## No 0 ## Yes 1 #This function shows you the contrasts #associated with a factor. #You can see that 1 is associated with &quot;Yes&quot;. #This is what our model will be predicting: #the probability of a &quot;Yes&quot;. If, by any chance, the level of interest is not the one that R will select, we will need to reorder the factor levels. In this particular analysis, our goal is to check whether being Black predicts harsher treatment. So, let’s reorder the factors so that the model is oriented towards predicting this harsher treatment. This will simply change the sign of the coefficients, which may enhance interpretation. #Reverse the order Arrests$harsher &lt;- relevel(Arrests$released, &quot;Yes&quot;) #Rename the levels so that it is clear we now mean yes to harsher treatment levels(Arrests$harsher) &lt;- c(&quot;No&quot;,&quot;Yes&quot;) #Check that it matches in reverse the original variable table(Arrests$harsher) ## ## No Yes ## 4334 892 #We will also reverse the order of the #&quot;colour&quot; variable so that the dummy #uses Whites as the baseline Arrests$colour &lt;- relevel(Arrests$colour, &quot;White&quot;) We use the glm() function to fit the model, specifying as an argument that we will be using a logit model (family=\"binomial\"). As stated, we are going to run a model oriented primarily to assess to what degree race/ethnicity seems to matter even when we adjust for other factors (e.g., sex, employment, and previous police contacts (checks: number of police data records of previous arrests, previous convictions, parole status, etc. - 6 in all) on which the arrestee’s name appeared; a numeric vector)). fitl_1 &lt;- glm(harsher ~ checks + colour + sex + employed, data=Arrests, family = &quot;binomial&quot;) summary(fitl_1) ## ## Call: ## glm(formula = harsher ~ checks + colour + sex + employed, family = &quot;binomial&quot;, ## data = Arrests) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.90346 0.15999 -11.898 &lt; 2e-16 *** ## checks 0.35796 0.02580 13.875 &lt; 2e-16 *** ## colourBlack 0.49608 0.08264 6.003 1.94e-09 *** ## sexMale 0.04215 0.14965 0.282 0.778 ## employedYes -0.77973 0.08386 -9.298 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4776.3 on 5225 degrees of freedom ## Residual deviance: 4330.7 on 5221 degrees of freedom ## AIC: 4340.7 ## ## Number of Fisher Scoring iterations: 5 The table, as you will see, is similar to the one you get when running linear regression, but there are some differences we will discuss. The first thing you see in the output printed in the console is the model we run. Then, we see something called the deviance residuals, which are a measure of model fit. This part of the output shows the distribution of the deviance residuals for individual cases used in the model. Later, we discuss how to use summaries of the deviance statistics to assess model fit. Below the table of coefficients, at the very bottom, are fit indices (including the null and deviance residuals and the Akaike information criterion (AIC)). We will discuss this later. The part in the middle of the output shows the coefficients, their standard errors, the statistics, and the associated p-values. The z statistic (sometimes called a Wald z-statistic) is a test of statistical significance assessing whether each input in the model is associated with the dependent variable. If we focus on the table of coefficients, we can see that all the inputs but sex were significant in the model. The estimates that get printed when you run logistic regression give you the change in the log odds of the outcome for a one-unit increase in the predictor. Here, we see that for every one unit increase in the number of previous police contacts (checks), the log odds of receiving harsher treatment (versus being released) increases by 0.36, adjusting for the other variables in the model. The reminder variables are categorical predictors with two levels. So, we see the coefficient for the dummy variables indicating the contrast with their respective baseline or reference category. For example, being Black increases the log odds of receiving harsher treatment by 0.49, whereas being employed decreases the log odds of being released by 0.77. As mentioned above, the coefficient for gender was not significant. What are log odds? We will discuss this in a bit. We can also use the confint() function to obtain confidence intervals for the estimated coefficients. confint(fitl_1) ## 2.5 % 97.5 % ## (Intercept) -2.2241433 -1.5962775 ## checks 0.3075891 0.4087441 ## colourBlack 0.3334415 0.6574559 ## sexMale -0.2445467 0.3429244 ## employedYes -0.9436356 -0.6148518 So what does that actually mean? Interpreting the log odds scale is something some people do not find very intuitive. So, using odd ratios when interpreting logistic regression is common. To do this, all we need to do is to exponentiate the coefficients. To get the exponentiated coefficients, you tell R that you want to exponentiate (exp()), that the object you want to exponentiate is called coefficients, and it is part of the model you just run. exp(coef(fitl_1)) ## (Intercept) checks colourBlack sexMale employedYes ## 0.1490516 1.4304108 1.6422658 1.0430528 0.4585312 You can use the same logic as for the confidence intervals. exp(cbind(OR = coef(fitl_1), confint(fitl_1))) ## OR 2.5 % 97.5 % ## (Intercept) 0.1490516 0.1081600 0.2026495 ## checks 1.4304108 1.3601419 1.5049266 ## colourBlack 1.6422658 1.3957633 1.9298763 ## sexMale 1.0430528 0.7830594 1.4090622 ## employedYes 0.4585312 0.3892103 0.5407210 #This will print both the OR and their 95% CI. Now, we can use the interpretation of odd ratios we introduced in a previous session. When the odd ratio is greater than 1, it indicates that the odds of receiving harsher treatment increases when the independent variable increases. We can say, for example, that previous police contacts increase the odds of harsher treatment by 43%, whereas being black increases the odds of harsher treatment by 64% (while adjusting for the other variables in the model). Employment has an odd ratio of 0.45. When the odd ratio is between 0 and 1, it indicates a negative relationship. So employment reduces the odds of harsher treatment by 1/0.46, that is by a factor of 2.18. For more details on interpreting odd ratios in logistic regression, you may want to read this. Some people do not like odd ratios. For other ways of interpreting logistic regression coefficients, you may want to consult chapter 5 of the book by Gelman and Hill (2007). You can read more about how to interpret odd ratios in logistic regression here. See the appendix for an alternative way of getting the same results with less typing. As with linear regression, the interpretation of regression coefficients is sensitive to the scale of measurement of the predictors. This means one cannot compare the magnitude of the coefficients to compare the relevance of variables to predict the response variable. The same applies to the odd ratios. Tempting and common as this might be unless the predictors use the same metric (or maybe if they are all categorical), there is little point in comparing the magnitude of the odd ratios in logistic regression. Like the unstandardised logistic regression coefficients, odd ratios are not a measure of effect size that allows comparisons across inputs (Menard, 2012). We can also use forest plots in much the same way as we did for linear regression. One way of doing this is using the plot.model() function of the sjPlot package. library(sjPlot) plot_model(fitl_1) Equally, we can produce effect plots using the effects package: library(effects) plot(allEffects(fitl_1), ask=FALSE) Effect plots in this context are particularly helpful because they summarise the results using probabilities, which is what you see plotted on the y-axis. We don’t have to print them all. When we are primarily concerned with one of them, as in this case, that’s the one we want to emphasise when presenting and discussing our results. There isn’t much point in discussing the results for the variables we defined as control (given our research goal). So, in this case, we would ask for the plot for our input measuring race/ethnicity: plot(effect(&quot;colour&quot;, fitl_1), multiline = FALSE, ylab = &quot;Probability(harsher)&quot;) We can use the predict() function to generate the predicted probability that the arrestees will be released, given what we know about their inputs in the model given the values of the predictors. By default, R will compute the probabilities for the dataset to which we fitted the model. Here, we have printed only the first ten probabilities, but the way we use the predict() function here will generate a predicted probability for each case in the dataset. fitl_1_prob &lt;- predict(fitl_1, type = &quot;response&quot;) #If you want to add this to your data frame, #you could designate your object as Arrests$fitl_1_prob fitl_1_prob[1:10] ## 1 2 3 4 5 6 7 ## 0.17262268 0.25519856 0.17262268 0.14344103 0.13833944 0.10091376 0.13455033 ## 8 9 10 ## 0.08905504 0.32891111 0.17262268 It is important to understand that with this type of models, we usually generate two types of predictions. On the one hand, we are producing a continuous-valued prediction in the form of a probability, but we can also generate a predicted class for each case. In many applied settings, the latter will be relevant. A discrete category prediction may be required to make a decision. Imagine a probation officer evaluating the future risk of a client. She/He would want to know whether the case is high-risk or not. 8.6 Assessing model fit: confusion matrix If we are interested in “qualitative” prediction, we also need to consider other measures of fit. In many applied settings, such as in applied predictive modelling, this can be the case. Imagine you are developing a tool to be used to forecast the probability of repeat victimisation in cases of domestic violence. This type of prediction may then be used to determine the type of police response to cases defined as high-risk. Clearly, you want to make sure the classification you make is as accurate as possible. In these contexts, it is common to start from a classification table or confusion matrix. A confusion matrix is simply a cross-tabulation of the observed outcome in relation to the predicted outcome. We saw earlier how the predict() function generated a set of predicted probabilities for each of the subjects in the study. To produce a classification table in this context, we must define a cut-off point, a particular probability that we will use to classify cases. Anybody above that cut-off we will define as belonging to the level of interest and anybody below we will define as not. We could, for example, say that anybody with a probability larger than .5 should be predicted to receive harsher treatment. The confusion matrix typically follows this layout: confusion The diagonal entries correspond to observations that are classified correctly according to our model and our cut-off point, whereas the off-diagonal entries are misclassifications. False negatives are observations that were classified as zeros but turned out to be ones (the outcome of interest). False positives are observations that were classified as ones (the outcome of interest) but turned out to be zeros. There are various ways of producing a confusion matrix in R. The most basic one is to ask for the cross-tabulation of the predicted classes (determined by the cut-off criterion) versus the observed classes. #First, we define the classes according to the cut-off fitl_1_pred_class &lt;- fitl_1_prob &gt; .5 #This creates a logical vector that returns TRUE #when the condition is met (the subject is predicted to be released) and #FALSE when the condition is not met (harsher treatment was delivered) fitl_1_pred_class[1:10] ## 1 2 3 4 5 6 7 8 9 10 ## FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #Let&#39;s make this into a factor with the same levels as the original variable harsher_pred &lt;- as.factor(fitl_1_pred_class) levels(harsher_pred) &lt;- c(&quot;No&quot;,&quot;Yes&quot;) table(harsher_pred) ## harsher_pred ## No Yes ## 5113 113 #Then we can produce the cross-tab tab0 &lt;- table(harsher_pred, Arrests$harsher) tab0 ## ## harsher_pred No Yes ## No 4278 835 ## Yes 56 57 We can derive various useful measures from classification tables. Two important ones are the sensitivity and the specificity. The model’s sensitivity is the rate at which the event of interest (e.g., receiving harsher treatment) is predicted correctly for all cases having the event. Sensitivity = number of cases with the event and predicted to have the event/number of samples actually presenting the event In this case, this amounts to 57 divided by 835 plus 57. The sensitivity is sometimes also considered the true positive rate since it measures the accuracy in the event population. On the other hand, the specificity is defined as: Specificity = number of cases without the events and predicted as non-events/number of cases without the event In this case, this amounts to 4278 divided by 4278 plus 56. The false positive rate is defined as one minus the specificity. We can generate these measures automatically from the table we produced. However, for this sort of thing, I prefer to use the confusionMatrix() function from the caret package. It produces a very detailed set of calibration measures that help indicate how well the model is classifying. library(caret) confusionMatrix(data=harsher_pred, reference=Arrests$harsher, positive=&quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 4278 835 ## Yes 56 57 ## ## Accuracy : 0.8295 ## 95% CI : (0.819, 0.8396) ## No Information Rate : 0.8293 ## P-Value [Acc &gt; NIR] : 0.4943 ## ## Kappa : 0.078 ## ## Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002 ## ## Sensitivity : 0.06390 ## Specificity : 0.98708 ## Pos Pred Value : 0.50442 ## Neg Pred Value : 0.83669 ## Prevalence : 0.17069 ## Detection Rate : 0.01091 ## Detection Prevalence : 0.02162 ## Balanced Accuracy : 0.52549 ## ## &#39;Positive&#39; Class : Yes ## #The data argument specifies the vector #with the predictions and the reference #argument the vector with the observed #outcome or event. The positive argument #identifies the level of interest in the factor. We can see first the accuracy. The overall accuracy rate gives us the agreement between the observed and predicted classes. However, the overall accuracy is often not the most useful measure. Kappa is also a measure that is often used with values ranging between 0.30 and 0.50 considered to indicate reasonable agreement. However, for many applications, it will be of interest to focus on the sensitivity and the specificity as defined above. In this case, we can see that our sensitivity, or the true positive rate, is very poor. And so is the Kappa. Clearly, the model has problems predicting harsh treatment with the select cut-off. One of the problems with taking this approach is that the choice of the cut-off point can be arbitrary, and yet this cut-off point will impact the sensitivity and specificity of the model. There is a trade-off between sensitivity and specificity. Given a fixed accuracy, more of one will result in less of the other. So if we use a different cut-off point, say .25, the classification table would look like this: precision&lt;-function(c) { tab1 &lt;- table(fitl_1_prob&gt;c, Arrests$harsher) out &lt;- diag(tab1)/apply(tab1, 2, sum) names(out) &lt;- c(&#39;specificity&#39;, &#39;sensitivity&#39;) list(tab1, out) } precision(.25) ## [[1]] ## ## No Yes ## FALSE 3627 496 ## TRUE 707 396 ## ## [[2]] ## specificity sensitivity ## 0.8368713 0.4439462 Here, we are predicting, according to our model, that anybody with a probability above .25 receives harsher treatment. Our sensitivity goes up significantly, but our specificity goes down. You can see that the cut-off point will affect how many false positives and false negatives we have. With this cut-off point, we will be identifying many more cases as presenting the outcome of interest when, in fact, they won’t present it (707 as opposed to 56 when using a cut-off of .5). On the other hand, we have improved the sensitivity, and now we are correctly identifying as positives 396 cases as opposed to just 57 cases). The overall accuracy is still the same, but we have shifted the balance between sensitivity and specificity. Potential trade-offs here may be appropriate when there are different penalties or costs associated with each type of error. For example, if you are trying to predict a homicide as part of an intervention or prevention program, you may give more importance to not making a false negative error. That is, you want to identify as many potential homicide victims as possible, even if that means that you will identify as victims individuals that, in the end, won’t be (false positives). On the other hand, if you have limited resources to attend to all the cases that you will predict as positives, you also need to factor this into the equation. You don’t want to use a cut-off point that will lead you to identify more cases as potential homicide victims that you can possibly work with. Similarly, the criminal justice system is essentially built around the idea of avoiding false positives - that is, convicting people who are innocent. You will have heard many phrases like “innocent until proven guilty” or “It is far better that 10 guilty men go free than one innocent man is wrongfully convicted”. This approach would incline us to err on the side of false negatives and avoid false positives (higher sensitivity, lower specificity). We may want to see what happens to sensitivity and specificity for different cut-off points. For this, we can look at receiver operating characteristics or simply ROC curves. This is essentially a tool for evaluating the sensitivity/specificity trade-off. The ROC curve can be used to investigate alternate cut-offs for class probabilities. See the appendix for more info on the ROC curve. There are also other ways to assess a model’s fit, such as deviance and pseudo r squared. For those interested, you can read more in the appendix. 8.7 Interactions The data we have been using were obtained by the author of the effects package from Michael Friendly, another prominent contributor to the development of R packages. The data are related to a series of stories revealed by the Toronto Star and further analysed by Professor Friendly as seen here. In this further analysis, Friendly proposes a slightly more complex model than the one we have specified so far. This model adds three new predictors (citizenship, age, and year in which the case was processed) and also allows for interactions between race (colour) and year, as well as race and age. fitl_2 &lt;- glm(harsher ~ employed + citizen + checks + colour * year + colour * age, family = binomial, data = Arrests) #Notice this different way of including interactions summary(fitl_2) ## ## Call: ## glm(formula = harsher ~ employed + citizen + checks + colour * ## year + colour * age, family = binomial, data = Arrests) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -134.187783 69.287702 -1.937 0.052785 . ## employedYes -0.747475 0.084601 -8.835 &lt; 0.0000000000000002 *** ## citizenYes -0.620159 0.105164 -5.897 0.0000000037 *** ## checks 0.364718 0.025949 14.055 &lt; 0.0000000000000002 *** ## colourBlack 361.668318 115.180289 3.140 0.001689 ** ## year 0.066318 0.034664 1.913 0.055722 . ## age 0.009347 0.005495 1.701 0.088979 . ## colourBlack:year -0.180225 0.057604 -3.129 0.001756 ** ## colourBlack:age -0.038134 0.010161 -3.753 0.000175 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4776.3 on 5225 degrees of freedom ## Residual deviance: 4275.0 on 5217 degrees of freedom ## AIC: 4293 ## ## Number of Fisher Scoring iterations: 5 What we see here is that the two interactions included are significant. To assist in the interpretation of interactions, it is helpful to look at effect plots. plot(effect(&quot;colour:year&quot;, fitl_2)) First, we see that up to 2000, there is strong evidence for differential treatment of blacks and whites. However, we also see evidence to support Police claims of the effect of training to reduce racial effects. plot(effect(&quot;colour:age&quot;, fitl_2)) On the other hand, we see a significant interaction between race and age. Young blacks are treated more harshly than young whites. However, older blacks were treated less harshly than older whites. In a previous session, we discussed the difficulties of interpreting regression coefficients in models with interactions. Centring and standardising in the way discussed earlier can actually be of help for this purpose. 8.8 Further resources These are a set of useful external resources that may aid your comprehension (and I have partly relied upon myself, so credit to them!): The UCLA guide to using logistic regression with R. A helpful list of resources for general linear models with R. "],["statistical-inference.html", "Chapter 9 Statistical Inference", " Chapter 9 Statistical Inference "],["wrapping-up.html", "Chapter 10 Wrapping up", " Chapter 10 Wrapping up It is the final week, so let me start with some general reflections to wrap up the unit. I know some of you may think, why the hell am I learning code and stats? You may think the contents of this course may ultimately not be of direct benefit to you. But we have designed this experience in such a way that you also acquire a set of skills and tools to learn how to confront tough problems (even those you may think you hate). Life will throw many of those kind of problems at your door. Having survived this module, we believe may have also teach you a thing or two about the importance of task persistence; overcoming your frustration in order to obtain results; dealing with your own insecurites and the understandable emotions they generate; being more organised about your work; collaborate with others in the same situation; the importance of attention to detail; etc. Hopefully we may have also helped you to develop a greater sense of curiosity and to achieve the inmense satisfaction one gets when one cracks a tough problem that may have had us screaming at the computer screen for what may have felt like hours. We know a course like this may get you out of your comfort zone. But we firmly believe that it is worth it. And that you will get something out of it, even if now may not be terribly obvious to you. And with that, let’s move to this final session. We are not going to cover new techniques today. What there may be a few tips about presentation that you may find helpful. The document in Blackboard about how to write your analytical report is quite detailed already. But there a few additional tips you may find helpful. ##Tips for the assignment You should all now be familiar with the assignment you have to do. In the Blackboard page for this course unit you can obtain the essay questions and additional guidance we have prepared. This guidance aims to answer most of the questions about what you need to do and how. It is very detailed and it has been built with the feedback from the many students that have taken this module before. That guidance aims to provide you a helping hand during the process. Critically you have never written anything like this before so it is normal to have questions. It may help you to read any article in any of the top journals in our field such as Criminology, the Journal of Research in Crime and Delinquency, or if you like “hard drugs” the Journal of Quantitative Criminology to get an idea of the templates that criminologists follow when writing a research report. Looking at good templates such as the ones you will encounter in those journals will give you good ideas about how to tell your story. Keep in mind those journal articles are subject to a more generous word count than you will have here, so you will have to be even more succint and parsimonious. If you are spoiled by choice and hesitant about which paper to choose, have a look at this one by Chris Melde and Finn Esbensen from 2009. For any additional questions you can follow the sequential 5 “Bs” approach designed to enhance autonomy that primary schools follow these days: brain, book, board, buddy, boss. Start with the first and move to the next one if you get stuck. Brain: “Do you already know the answer or is this something you can decide for yourself?” Book: “Can I look back through my learning materials and find the answer there?” Board: “Is the answer in the discussion board?” Buddy: “Can one of my peers answer this question?” (if unconvinced by their answer you can move to the next B) Boss: “If I have tried all the other Bs, then I can put up my hand but continue working” (and remember here you should first try to ask in the discussion board or the labs, and failing that make an appointment through office hours) A scientific report, like the one you are tasked to write, needs to be clear, well justified, and very efficient in the use of space. It will be helpful you finish ahead of time and dedicate some time simply to edit your final essay. This is, in fact, a general piece of advice for any essay you write. But in this case perhaps matters more. You will have done loads of things as part of your analysis, but will have very limited word count to tell the story of your analysis. Every word and every sentence must be needed. There is always a shorter and more parsimonious way of saying something! And, please, use the spelling editors of whatever app you use to write your essays. It is not cool to send essays with typos and grammar errors that any app can automatically detect these days. Here I am just going to cover a few things you can do to make this presentation a bit more succint and parsimonious. ##Summarising your variables The paper by Melde and Esbensen (2009) cited above provides an example of how you may want to describe in the text your variables. As noted you may not have as much space to get into so much detail, but the core idea you need to take home is that the reader needs to know what variables you are using and how they are measured. You will also need to provide a succint summary of the descriptive measures for your variables. Look at this paper by Andromachi Tseloni and Christina Zarafonitou (2008). Below I reproduce a table from the pre-print version of this article with a summary of the variables they are using in their analysis: This is a very succint way of describing the distribution of your variables when all are categorical. If you happen to have quantitative variables as well you could, for example, add another column with the mean and the standard deviation (the latter enclosed in brackets) -which would only have this information for your quantitative variables. Notice that this table has minimum amount of ink and only lines separating cells when absolutely necessary. If you are uncertain how to modify the default tables your word editor uses, this file gives you some tips in how to do with with MS Word. It will be fairly similar with other apps. Notice as well that information about the total sample size is provided. Remember what we cover in the end of section 4.9! As we have said we want you to look at whether your variables are associated before you run your multiple regression model. We do this so that we can evaluate if you have learnt how to choose the right tests and measures of effect size. Above I am reproducing an example of how you could do this. The example assumes you have a categorical dependent variable with four different categories (non-gang, gang only at T1, etc.). You can see how in this table the categories define the column (there is also one for the total sample, if you have something like this you are essentially providing also the summary descriptives for the whole). You can see that the rows define the levels for two categorical variables (gender and race/ethnicity) and two quantitative variables (age and a score for crime). For the categorical variables we have the appropriate percentages (as discussed in week 7) and for the quantitative variables we have the means and standard deviations. Remember how you set this table may differ depending the nature of your variables, but hopefully you get the idea. You can find a way to summarise the information. You could add a column to a table like this with the p values for your statistical tests (somehow indicating what that test was: chi square, F, etc.). Then you could discuss the more relevant of this in the narrative discussing your table. ##Some final words So, this is the end. At least for now. What we have done this semester is an introduction to the field of data analysis. But there is so much more to it. If you are planning a scientific career or want to work as an analyst in the private or public sector, the journey is just beginning. You could take some choices in the final year to continue this journey. There are some options within our degree (like Crime Mapping or the Quantitative Seconday Analysis Short Dissertation pathway, one of the possible modalities as the Short Dissertation). You could also take options in other degrees within the School of Social Science, our new home, that also have a focus on quant analysis (we can advise on these). After that you may want to think about our MRes in Criminology (Social Statistics), which focuses on quant analysis as part of the criminology track. You can see details here. But there are many free courses offered as massive online open courses that you could take to consolidate or further expand what you have learnt this semester. Check out Coursera or edX platforms for these. Many of those courses make you pay for a certificate (and if you want to get feedback), but you can watch the videos and use the materials for free in almost all of them. You may also want to have a look at datacamp. Most of their courses require a paid subscription, but there are many courses for free (particularly those created by the community). Good luck with the essays!!! And do not hesitate to get in touch if you have any queries. "],["appendix.html", "Chapter 11 Appendix 11.1 Logistic regression", " Chapter 11 Appendix 11.0.1 Expected frequencies Notice that R is telling us that the minimum expected frequency is 41.68. Why? For the Chi-squared test to work, it assumes the cell counts are sufficiently large. Precisely what constitutes ‘sufficiently large’ is a matter of some debate. One rule of thumb is that all expected cell counts should be above 5. If we have small cells, one alternative is to rely on the Fisher’s Exact Test rather than on the Chi-Square. We don’t have to request it here. Our cells are large enough for Chi Square to work fine. But if we needed, we could obtain the Fisher’s Exact Test with the following code: BCS0708&lt;-read.csv(&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/BCS0708.csv&quot;) library(gmodels) BCS0708$rubbcomm &lt;- as.factor(BCS0708$rubbcomm) BCS0708$rubbcomm &lt;- factor(BCS0708$rubbcomm, levels = c(&quot;not at all common&quot;, &quot;not very common&quot;, &quot;fairly common&quot;, &quot;very common&quot;)) fisher.test(BCS0708$rubbcomm, BCS0708$bcsvictim, simulate.p.value=TRUE) ## ## Fisher&#39;s Exact Test for Count Data with simulated p-value (based on ## 2000 replicates) ## ## data: BCS0708$rubbcomm and BCS0708$bcsvictim ## p-value = 0.0004998 ## alternative hypothesis: two.sided The p-value is still considerably lower than our alpha level of .05. So, we can still conclude that the relationship we observe can be generalised to the population. Remember that we didn’t need the Fisher test. However, as suggested above, there may be times when you need them. 11.1 Logistic regression 11.1.1 Fitting logistic regression: alternative Another way of fitting a logistic regression and getting the odds ratio with less typing is to use the Logit() function in the lessR package (you will need to install it if you do not have it). library(effects) data(Arrests, package=&quot;effects&quot;) Arrests$harsher &lt;- relevel(Arrests$released, &quot;Yes&quot;) #Rename the levels so that it is clear we now mean yes to harsher treatment levels(Arrests$harsher) &lt;- c(&quot;No&quot;,&quot;Yes&quot;) #Check that it matches in reverse the original variable Arrests$colour &lt;- relevel(Arrests$colour, &quot;White&quot;) library(lessR, quietly= TRUE) Logit(harsher ~ checks + colour + sex + employed, data=Arrests, brief=TRUE) ## ## &gt;&gt;&gt; Note: colour is not a numeric variable. ## Indicator variables are created and analyzed. ## ## &gt;&gt;&gt; Note: sex is not a numeric variable. ## Indicator variables are created and analyzed. ## ## &gt;&gt;&gt; Note: employed is not a numeric variable. ## Indicator variables are created and analyzed. ## ## Response Variable: harsher ## Predictor Variable 1: checks ## Predictor Variable 2: colourBlack ## Predictor Variable 3: sexMale ## Predictor Variable 4: employedYes ## ## Number of cases (rows) of data: 5226 ## Number of cases retained for analysis: 5226 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of harsher for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) -1.9035 0.1600 -11.898 0.000 -2.2170 -1.5899 ## checks 0.3580 0.0258 13.875 0.000 0.3074 0.4085 ## colourBlack 0.4961 0.0826 6.003 0.000 0.3341 0.6580 ## sexMale 0.0422 0.1496 0.282 0.778 -0.2511 0.3355 ## employedYes -0.7797 0.0839 -9.298 0.000 -0.9441 -0.6154 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 0.1491 0.1089 0.2039 ## checks 1.4304 1.3599 1.5046 ## colourBlack 1.6423 1.3967 1.9310 ## sexMale 1.0431 0.7779 1.3986 ## employedYes 0.4585 0.3890 0.5404 ## ## ## -- Model Fit ## ## Null deviance: 4776.258 on 5225 degrees of freedom ## Residual deviance: 4330.699 on 5221 degrees of freedom ## ## AIC: 4340.699 ## ## Number of iterations to convergence: 5 ## ## ## Collinearity ## ## Tolerance VIF ## checks 0.908 1.101 ## colourBlack 0.963 1.038 ## sexMale 0.982 1.019 ## employedYes 0.931 1.074 11.1.2 Assessing model fit: deviance and pseudo r squared As you may remember, when looking at linear models, we could use an F test to check the overall fit of the model, and we could evaluate R squared. When running logistic regression, we cannot obtain the R squared (although there is a collection of pseudo-R^2 measures that have been produced). In linear regression, things are a bit simpler. As Menard (2010: 43) explains: “there is only one reasonable residual variation criterion for quantitative variables in OLS, the familiar error sum of squares… but there are several possible residual variation criteria (entropy, squared error, qualitative difference) for binary variables. Another hindrance is the existence of numerous mathematical equivalents to R^2 in OLS, which are not necessarily mathematically (same formula) or conceptually (same meaning in the context of the model) equivalent to R^2 in logistic regression… Moreover, in logistic regression, we must choose whether we are more interested in qualitative prediction (whether predicitons are correct or incorrect), quantitative prediction (how close predictions are to being correct), or both, because different measures of explained variation are appropriate for these two different types of prediction” A common starting point for assessing model fit is to look at the log-likelihood statistic and the deviance (also referred to as -2LL). The log-likelihood aims to provide a measure of how much-unexplained variation there is after you fit the mode. Large values indicate poor fit. The deviance, on the other hand, is simply the log-likelihood multiplied by -2 and is generally abbreviated as -2LL. The deviance will be a positive value, and larger values indicate worse prediction of the response variable. It is analogous to the error sum of squares in linear regression. In the same way that OLS linear regression tries to minimise the error sum of squares, maximum likelihood logistic regression tries to minimise the -2LL. The difference between the -2LL for the model with no predictors and the -2LL for the model with all the predictors is the closer we get in logistic regression to the regression sum of squares. This difference is often called model chi-squared, and it provides a test of the null hypothesis that all the regression coefficients equal zero. It is, thus, equivalent to the F test in OLS regression. fitl_1 &lt;- glm(harsher ~ checks + colour + sex + employed, data=Arrests, family = &quot;binomial&quot;) summary(fitl_1) ## ## Call: ## glm(formula = harsher ~ checks + colour + sex + employed, family = &quot;binomial&quot;, ## data = Arrests) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.90346 0.15999 -11.898 &lt; 0.0000000000000002 *** ## checks 0.35796 0.02580 13.875 &lt; 0.0000000000000002 *** ## colourBlack 0.49608 0.08264 6.003 0.00000000194 *** ## sexMale 0.04215 0.14965 0.282 0.778 ## employedYes -0.77973 0.08386 -9.298 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4776.3 on 5225 degrees of freedom ## Residual deviance: 4330.7 on 5221 degrees of freedom ## AIC: 4340.7 ## ## Number of Fisher Scoring iterations: 5 In our example, we saw that some measures of fit were printed below the table with the coefficients. The null deviance is the deviance of the model with no predictors, and the residual deviance is simply the deviance of this model. You clearly want the residual deviance to be smaller than the null deviance. The difference between the null and the residual deviance is what we call the model chi-squared. In this case, this is 4776.3 minus 4330.7. We can ask R to do this for us. First, notice that the object we created has all the information we need already stored. names(fitl_1) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;fitted.values&quot; ## [4] &quot;effects&quot; &quot;R&quot; &quot;rank&quot; ## [7] &quot;qr&quot; &quot;family&quot; &quot;linear.predictors&quot; ## [10] &quot;deviance&quot; &quot;aic&quot; &quot;null.deviance&quot; ## [13] &quot;iter&quot; &quot;weights&quot; &quot;prior.weights&quot; ## [16] &quot;df.residual&quot; &quot;df.null&quot; &quot;y&quot; ## [19] &quot;converged&quot; &quot;boundary&quot; &quot;model&quot; ## [22] &quot;call&quot; &quot;formula&quot; &quot;terms&quot; ## [25] &quot;data&quot; &quot;offset&quot; &quot;control&quot; ## [28] &quot;method&quot; &quot;contrasts&quot; &quot;xlevels&quot; So we can use this stored information in our calculations. with(fitl_1, null.deviance - deviance) ## [1] 445.5594 Is 445.6 small? How much smaller is enough? This value has a chi-square distribution, and its significance can be easily computed. For this computation, we need to know the degrees of freedom for the model (which equal the number of predictors in the model) and can be obtained like this: with(fitl_1, df.null - df.residual) ## [1] 4 Finally, the p-value can be obtained using the following code to invoke the Chi-Square distribution: #When doing it yourself, this is all you really need #(we present the code in a separate fashion above so that you understand better what the one here does) with(fitl_1, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)) ## [1] 3.961177e-95 We can see that the model chi-square is highly significant. Our model as a whole fits significantly better than a model with no predictors. Menard (2010) recommends also looking at the likelihood ratio R^2, which can be calculated as the difference between the null deviance and the residual deviance divided by the null deviance. #Likelihood ratio R2 with(fitl_1, (null.deviance - deviance)/null.deviance) ## [1] 0.09328629 Some authors refer to this as the Hosmer/Lemeshow R^2. It indicates how much the inclusion of the independent variables in the model reduces variation, as measured by the null deviance. It varies between 0 (when our prediction is catastrophically useless) and 1 (when we predict with total accuracy). There are many other pseudo R^2 measures that have been proposed, but Menard based on research on the properties of various of these measures recommends the likelihood ratio R^2 because: It is the one with a closer conceptual link to R^2 in OLS regression. It does not appear to be sensitive to the base rate (the proportion of cases that have the attribute of interest) of the phenomenon being studied and, therefore, will work even in cases with unbalanced probabilities. It varies between 0 and 1 And it can be used in other generalised linear models (models for categorical outcomes with more than two levels, which we don’t cover here) 11.1.3 Assessing model fit: ROC curves We may want to see what happens to sensitivity and specificity for different cut-off points. For this, we can look at receiver operating characteristics or simply ROC curves. This is essentially a tool for evaluating the sensitivity/specificity trade-off. The ROC curve can be used to investigate alternate cut-offs for class probabilities. We can use the pROC package for this. We start by creating an object that contains the relevant information with the roc() function from the pROC package. fitl_1_prob &lt;- predict(fitl_1, type = &quot;response&quot;) library(pROC) rocCURVE &lt;- roc(response = Arrests$harsher, predictor = fitl_1_prob) Once we have the object with the information, we can plot the ROC curve. plot(rocCURVE, legacy.axes = TRUE) #By default, the x-axis goes backwards; we can use the specified option legacy.axes=TRUE, to get 1-spec on the x-axis moving from 0 to 1. We can see the trajectory of the curve is at first steep, suggesting that sensitivity increases at a greater pace than the decrease in specificity. However, we then reach a point at which specificity decreases at a greater rate than the sensitivity increases. If you want to select a cut-off that gives you the optimal cut-off point, you can use the coords() function of the pROC package. You can pass arguments to this function so that it returns the best sum of sensitivity and specificity. alt_cutoff1 &lt;- coords(rocCURVE, x = &quot;best&quot;, best.method = &quot;closest.topleft&quot;) #The x argument, in this case, is selecting the best cut-off using the &quot;closest topleft&quot; method #(which identifies the point closest to the top-left part of the plot with perfect sensitivity or specificity). Another option is to use the &quot;youden&quot; method in the best.method argument. alt_cutoff1 ## threshold specificity sensitivity ## 1 0.1696539 0.6305953 0.7163677 Here, we can see that with a cut-off point of .16 we get a specificity of .63 and a sensitivity of .71. Notice how this is close to the base rate of harsher treatment in the sample (17% of individuals actually received harsher treatment). For a more informed discussion of cut-off points and costs of errors in applied predictive problems in criminal justice, I recommend reading Berk (2012). Often, the selection of cut-off may be motivated by practical considerations (e.g., selecting individuals for treatment in a situation where resources to do so are limited). The ROC curve can also be used to develop a quantitative assessment of the model. The perfect model is one where the curve reaches the top left corner of the plot. This would imply 100% sensitivity and specificity. On the other hand, a useless model would be one with a curve alongside the diagonal line splitting the plot in two, from the bottom right corner to the top right corner. You can also look at the area under the curve (AUC) and use it to compare models. An AUC of .5 corresponds to the situation where our predictors have no predictive utility. For a fuller discussion of how to compare these curves and the AUC, I recommend reading Chapter 11 of Kuhn and Johnson (2014). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

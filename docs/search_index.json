[["index.html", "Modelling Criminological Data CRIM20452 Preface", " Modelling Criminological Data CRIM20452 2026-02-13 Preface This study material is designed to introduce Criminology students at the University of Manchester to the use of data science in crime research and practice. This is an improved version originally developed by Juanjo Medina and Reka Solymosi, and is currently being updated and maintained by Ana Nicoriciu - anamaria.nicoriciu@manchester.ac.uk and Sam Langton - samuel.langton@manchester.ac.uk. This lab note is a work in progress. If you have any suggestions or find any errors, please don’t hesitate to contact us by submitting an issue to the GitHub repository. We appreciate your feedback and will use it to improve the material for future students. Note for students * While this material briefly covers some concepts, students are expected to engage in weekly reading, attend lab sessions, and participate in lectures for a comprehensive course experience. These notes are not intended to be a stand-alone reference or textbook but rather a set of exercises to gain hands-on practice with the concepts introduced during the course. * This material is designed for Criminology students at the University of Manchester. They are meant to introduce students to descriptive statistics and the key concepts needed to understand quantitative data analysis in crime research. * The handouts utilise various datasets, including data from the UK Data Service, such as the Crime Survey for England and Wales, which is available under an Open Government Licence. These datasets are designed to serve as a learning resource and should not be used for research or the production of summary statistics. "],["a-first-lesson-about-r.html", "Chapter 1 A first lesson about R 1.1 Install R &amp; RStudio 1.2 Open up and explore RStudio 1.3 Customising the RStudio look 1.4 Getting organised: R Projects 1.5 Functions: Talk to your computer 1.6 More on packages 1.7 Objects: creating an object 1.8 More on objects 1.9 Naming conventions for objects in R 1.10 R object types: vectors 1.11 R object types: Data frame 1.12 Exploring data 1.13 R data types: Factors 1.14 How to import data 1.15 How to use ‘comment’ 1.16 How to Quit RStudio 1.17 Summary", " Chapter 1 A first lesson about R 1.1 Install R &amp; RStudio We recommend using your laptops for this course. If you have not already, then please download and install R and RStudio onto your laptops. - click here for instructions using Windows or - here for instructions using a Mac. If you are using a Mac it would be convenient that you use the most up-to-date version of OS or at least one compatible with the most recent version of R. Read this if you want to check how to do that. You can access the download links for both R and RStudio here: https://posit.co/download/rstudio-desktop/ If you prefer, you can always use any of the PCs in the computer cluster. All of them already have the software installed. 1.2 Open up and explore RStudio In this session, we will focus on developing basic familiarity with R Studio. You can use R without using R Studio, but R Studio is an app that makes it easier to work with R. R Studio automatically runs R in the background. We will be interacting with R via R Studio in this course unit. When you first open R Studio, you will see (as in the image above) that there are 3 main panes. The bigger one to your left is the console. If you read the text in the console, you will see that R Studio is opening R and can see what version of R you are running. Depending on whether you are using the cluster machines or your own installation, this may vary, but don’t worry too much about it. The view in R Studio is structured so that you have 4 open panes in a regular session. Click on the File drop-down menu, select New File, then R Script. You will now see the 4 window areas on display. You can shift between different views and panels in each of these areas. You can also use your mouse to re-size the different windows if that is convenient. Look, for example, at the bottom right area. Within this area you can see that there are different tabs, which are associated with different views. You can see in the tabs in this section that there are different views available: Files, Plots, Packages, Help, and Viewer. The Files allow you to see the files in the physical directory that is currently set up as your working environment. You can think of it like a window in Windows Explorer that lets you see the content of a folder. In the plots panel, you will see any data visualisations or graphical displays of data that you produce. We haven’t yet produced any, so it is empty at the moment. If you click on packages, you will see the packages that are currently available in your installation. What is a “package” in this context? Packages are modules that expand what R can do. There are thousands of them. A few come pre-installed when you do a basic R installation. Others you pick and install yourself. This term, we will introduce some important packages we recommend you install when prompted. The other really useful panel in this part of the screen is the Help viewer. Here, you can access the documentation for the various packages that make up R. Learning how to use this documentation will be essential if you want to get the most from R. In the diagonally opposite corner, the top left, you should now have an open script window. The script is where you write your programming code - the instructions you send to your computer. A script is nothing but a text file that you can write in. Unlike other programs for data analysis you may have used in the past (Excel, SPSS), you need to interact with R by writing down instructions and asking R to evaluate those instructions. R is an interpreted programming language: you write instructions (code) that the R engine has to interpret in order to do something. All the instructions we write can and should be saved in a script, which you can later return to and continue working on. One of the key advantages of doing data analysis this way is that you are producing a written record of every step you take in the analysis. The challenge, though, is that you need to learn this language in order to be able to use it. That will be the main focus of this course, teaching you to write R code for data analysis purposes. As with any language, the more you practice it, the easier it will become. More often than not, you will be cutting and pasting chunks of code we will give you. But we will also expect you to develop a basic understanding of what these bits of code do. It is a bit like cooking. At first, you will just follow recipes as they are given to you, but as you become more comfortable in your “kitchen”, you will feel more comfortable experimenting. The advantage of doing the analysis this way is that once you have written your instructions and saved them in a file, you will be able to share them with others and run them every time you want in a matter of seconds. This creates a reproducible record of your analysis: something that your collaborators or someone else anywhere (including your future self, the one that will have forgotten how to do the stuff) could run and get the same results as you did at some point earlier. This makes science more transparent, and transparency brings many advantages. For example, it makes your research more trustworthy. Don’t underestimate how critical this is. Reproducibility is becoming a key criterion for assessing good quality research. And tools like R allow us to enhance it. You may want to read more about reproducible research here. 1.3 Customising the RStudio look RStudio allows you to customise the way it looks. For example, working with white backgrounds is not generally a good idea if you care about your eyesight. If you don’t want to end up with dry eyes, not only is it good for you to follow the 20-20-20 rule (every 20 minutes, look for 20 seconds at an object located 20 feet away from you), but it may also be a good idea to use more eye-friendly screen displays. Click in the Tools menu and select Global options. This will open up a pop-up window with various options. Select Appearance. In this section, you can change the font type and size, as well as the kind of theme background that R will use in the various windows. I suffer from poor sight, so I often increase the font type. I also use the Tomorrow Night Bright theme to prevent my eyes from going too dry from the effort of reading a lightened screen, but you may prefer a different one. You can preview them and then click apply to select the one you like. This will not change your results or analysis. This is just something you may want to do in order to make things look better and healthier for you. 1.4 Getting organised: R Projects Whenever you do analysis, you will be working with a variety of files. You may have an Excel data set (or some other type of data set file, like CSV, for example), a Microsoft Word file where you are writing down the essay with your results, but also a script with all the programming code you have been using. R needs to know where all these files are on your computer. Often, you will get error messages because you are expecting R to find one of these files in the wrong location. It is absolutely critical that you understand how your computer organises and stores files. Please watch the video below to understand the basics of file management and file paths: Windows users MAC users The best way to avoid problems with file management in R is using what RStudio calls R Projects. Technically, a RStudio project is just a directory (a folder) with the name of the project and a few files and folders created by RStudio for internal purposes. This is where you should hold your scripts, your data, and your reports. You can manage this folder with your own operating system manager (e.g., Windows Explorer) or through the RStudio file manager (which you access in the bottom right corner of the Windows set in RStudio). When a project is reopened, RStudio opens every file and data view that were open when the project was closed last time around. Let’s learn how to create a project. Go to the File drop-down menu and select New Project. That will open a dialogue box where you are asked to specify what kind of directory you want to create. Select a new working directory in this dialogue box. Now, you get another dialogue box where you have to specify what kind of project you want to create. Select the first option New Project. Finally, you get to select a name for your project (in the image below, I use the code for this course unit, but you can use any sensible name you prefer), and you will need to specify the folder/directory in which to place this directory. Preferably select a folder that you created specifically for this course, and to avoid problems later, not your desktop. With simple projects, a single script file and a data file are all you may have. But with more complex projects, things can rapidly become messy. So, you may want to create subdirectories within this project folder. I typically use the following structure in my own work to put all files of a certain type in the same subdirectory: Scripts and code: Here, I put all the text files with my analytic code, including Rmarkdown files, which is something we will introduce much later in the semester. Source data: Here, I put the original data. I tend not to touch this once I have obtained the original data. Documentation: This is the subdirectory where I place all the data documentation (e.g., codebooks, questionnaires, etc.) Modified data: All analyses involve transformations and changes to the original data files. You don’t want to mess up the original data files, so you should create new data files as soon as you start changing your source data. I go so far as to place them in a different subdirectory. Literature: Analysis is all about answering research questions. There is always a literature about these questions. I place the relevant literature for the analytic project I am conducting in this subdirectory. Reports and write-up: Here is where I file all the reports and data visualisations that are associated with my analysis. You can create these subdirectories using Windows Explorer or the Files window in R Studio. 1.5 Functions: Talk to your computer So far, we have covered an introduction to the main interface you will be using and talked about RStudio projects. In this unit, you will be using this interface and creating files within your RStudio projects to produce analysis based on programming code that you will need to write using the R language. Let’s write some very simple code using R to talk to your computer. First, open a new SCRIPT within the project you just created. Type the following instructions in the script window. After you are done, click in the top right corner where it says Run (if you prefer quick shortcuts, you can select the text and then press Ctrl + Enter): print(&quot;I love stats&quot;) ## [1] &quot;I love stats&quot; Congratulations!!! You just run your first line of R code!  In these handouts, you will see grey boxes with bits of code. You can cut and paste this code into your script window and run the code from it to reproduce our results. As we go along, we will be covering new bits of code. Sometimes, in these lab notes, you will see the results of running the code, which is what you see printed in your console or in your plot viewer. The results will appear enclosed in a box as above. The R language uses functions to tell the computer what to do. In the R language, functions are the verbs. You can think of functions as predefined commands that somebody has already programmed into R and tell R what to do. Here, you learnt your first R function: print. All this function does is ask R to print whatever you want in the main console (see the window in the bottom left corner). In R, you can pass a number of arguments to any function. These arguments control what the function will do in each case. The arguments appear between brackets. Here, we passed the text “I love stats” as an argument. Once you execute the program by clicking on Run, the R engine sends this to your machine’s CPU in the form of binary code, and this produces a result. In this case, we see that the result is printed on the main console. Every R function admits different kinds of arguments. Learning R involves not only learning different functions but also learning the valid arguments you can pass to each function. As indicated above, the window in the bottom left corner is the main console. You will see that the words “I love stats” appear printed there. If, rather than using R Studio, you were working directly from R, that’s all you would get: the main console where you can write code interactively (rather than all the different windows you see in R Studio). You can write your code directly in the main console and execute it line by line in an interactive fashion. However, we will be running code from scripts so that you get used to the idea of properly documenting all the steps you take. 1.6 More on packages Before, we described packages as elements that add the functionality of R. Most packages introduce new functions that allow you to ask R to do different things. Anybody can write a package, so consequently, R packages vary in quality and complexity. You can find packages in different places, as well, from official repositories (which means they have passed a minimum of quality control), something called GitHub (a webpage where software developers post work in progress), to personal webpages (danger, danger!). In early 2017, we passed the 10,000 packages mark just in the main official package repository, so the number of things that can be done with R grows exponentially every day as people keep adding new packages. When you install R, you only install a set of basic packages, not the full 10,000-plus. So, if you want to use any of these added packages that are not part of the basic installation, you first need to install them. You can see what packages are available for your local installation by looking at the packages tab in the bottom right corner panel. Click there and check. We are going to install a package that is not there so that you can see how the installation is done. If you just installed R on your laptop, you will see a shortish list of packages that constitute the basic installation of R. If you are using one of the machines in the computer cluster, this list is a bit longer because we asked IT to install some of the most commonly used packages. But knowing how to install packages is essential since you will want to do it very often. We will install a package called “cowsay” to demonstrate the process. In the Packages panel, there is an Install menu that opens a dialogue box and allows you to install packages. Instead, we are going to use code to do this. Just cut and paste the code below into your script and then run it: install.packages(&quot;cowsay&quot;) Here, we are introducing a new function, “install.packages” and what we have passed as an argument is the name of the package that we want to install. This is how we install a package that is available in the official CRAN repository. Given that you are connecting to an online repository, you will need an internet connection every time you want to install a package. CRAN is an official repository that has a collection of R packages that meet a minimum set of quality criteria. It’s a fairly safe place to get packages from. If we wanted to install a package from somewhere else, we would have to adapt the code. Later this semester, you will see how we install packages from GitHub. This line of code (as it is currently written) will install this package in a personal library that will be located in your P: drive if you are using a cluster machine. If you are using a Windows machine, this code will place this package within a personal library in your Documents folder. Once you install a package, it will remain in the machine/location where you installed it until you physically delete it. How do you find out what a package does? You look at the relevant documentation. In the Packages window, scroll down until you find the new package we installed. Here, you will see the name of the package (cowsay), the source of the package (i.e., where got it from, CRAN). and the package version. The version I have for cowsay is 1.2.2. Yours may be older or newer. It doesn’t matter much at this point. Click in the name cowsay. You will see that R Studio has now brought you to the Help tab. Here is where you find the help files for this package, including all the available documentation. Every beginner in R will find these help files a bit confusing. But after a while, their format and structure will begin to make sense to you. Click where it says User guides, package vignettes, and other documentation. Documentation in R has become much better since people started to write vignettes for their packages. They are little tutorials that explain with examples what each package does. Click on the cowsay::cowsay_tutorial that you see listed here. You will find a page that gives you a detailed tutorial on this package. You don’t need to read it now, but remember that this is one way to find help when using R. Let’s try to use some of the functions of this package. We will use the “say” function: say(&quot;I love stats&quot;) You will get an error message telling you that this function could not be found. What happened?? This will be the first of many error messages you will get. An error message is the computer’s way of telling you that your instructions are somehow incomplete or problematic and, thus, are unable to do what you ask. It is frustrating to get these messages, and a critical skill for you this semester will be to overcome that frustration and try to understand why the computer cannot do what you ask. These labs are all about finding out the source of the error and solving it. There is nothing wrong with getting errors. The problem is if you give up and let your frustration get the best of you! So why are we getting this error? Installing a package is only the first step. The next step, when you want to use it in a given session, is to load it. Think of it as a pair of shoes. You buy them once, but you have to take them from your closet and put them on when you want to use them. Same with packages: you only install once, but you need to load it from your library every time you want to use it -within a given session (once loaded, it will remain loaded until you finish your session by closing RStudio). To see what packages you currently have loaded in your session, you use the search() function (you do not need to pass it any arguments in this case). search() ## [1] &quot;.GlobalEnv&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [4] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ## [7] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; If you run this code, you will see that cowsay is not in the list of loaded packages. Therefore, your computer cannot use any of the functions associated with it until you load it. To load a package, we use the library function. So, if we want to load the new package we installed on our machine, we would need to use the following code: library(&quot;cowsay&quot;) Run the search function again. You will see this package is listed now. So now we can try using the function “say” again. say(&quot;I love stats&quot;) ## ## ______________ ## &lt; I love stats &gt; ## -------------- ## \\ ## \\ ## ## ^__^ ## (oo)\\ ________ ## (__)\\ )\\ /\\ ## ||------w| ## || || You get a random animal in the console repeating the text we passed as an argument. If we like a different animal, we could pass a new argument on to the “say” function. So, if we want to have a cow rather than a random animal, then we would pass the following arguments on to our function. say(&quot;I love stats&quot;, &quot;cow&quot;) ## ## ______________ ## &lt; I love stats &gt; ## -------------- ## \\ ## \\ ## ## ^__^ ## (oo)\\ ________ ## (__)\\ )\\ /\\ ## ||------w| ## || || This is an important feature of arguments in functions. We said how different functions admit different arguments. Here, by specifying cow, the function prints that particular animal. But why is it that when we didn’t specify a particular kind of animal, we still got a result? That happened because functions always have default arguments that are necessary for them to run, and that you do not have to make explicit. Default arguments are implicit and do not have to be typed. The say function has a default argument, random, which will print a random character or animal. It is only when you want to change the default that you need to make an alternative animal explicit. Remember, you only have to install a package that has not been installed ONCE. But if you want to use it in a given session, you will have to load it within that session using the library function. Once you load it within a session, the package will remain loaded until you terminate your session (for example, by closing R Studio). Do not forget this! 1.7 Objects: creating an object We have seen how the first argument that the “say” function takes is the text that we want to convert into speech for our given animal. We could write the text directly into the function (as we did above), but now we are going to do something different. We are going to create an object to store the text. An object? What do I mean? In the same way that everything you do in R you do with functions (your verbs), everything that exists in R is an object. You can think of objects as boxes where you put stuff. In this case, we are going to create an object called my_text, and inside this object, we are going to store the text “I love stats”. How do you do this? We will use the code below: my_text &lt;- &quot;I love stats.&quot; This bit of code is simply telling R we are creating a new object with the assigned name (“my_text”). We are creating a box with such a name, and inside this box, we are placing a bit of text (“I love stats”). The arrow &lt;- you see is the assignment operator. This is an important part of the R language that tells R what we are including inside the object in question. Run the code. Look now at the Environment window in the top right corner. We see that this object is now listed there. You can think of the Environment as a warehouse where you put stuff - your different objects. Is there a limit to this environment? Yes, your RAM. R works on your RAM, so you need to be aware that if you use very large objects, you will need loads of RAM. But that won’t be a problem you will encounter in this course unit. Once we put things into these boxes or objects, we can use them as arguments for our functions. See the example below: say(my_text, &quot;cow&quot;) ## ## _______________ ## &lt; I love stats. &gt; ## --------------- ## \\ ## \\ ## ## ^__^ ## (oo)\\ ________ ## (__)\\ )\\ /\\ ## ||------w| ## || || 1.8 More on objects Now that we have covered some of the preliminaries, we can move on to the data. In Excel, you are used to seeing your data in spreadsheet format. If you need a recap, you should review some of the materials from previous modules. R is considerably more flexible than Excel. Most of the work we do here will use data sets or data frames as they are called in R. But as you have seen earlier, you can have objects other than data frames in R. These objects can relate to external files or simple textual information (“I love stats”). This flexibility is a big asset because, among other things, it allows us to break down data frames or the results from doing analysis on them into their constitutive parts (this will become clearer as we go along). As we have seen earlier, to create an object, you have to give it a name and then use the assignment operator (the &lt;- symbol) to assign it some value. For example, if we want to create an object that we name “x”, and we want it to represent the numerical value of 5, we write: x &lt;- 5 We are simply telling R to create a numeric object, called x, with one element (5), of length 1. It is numeric because we are putting a number inside this object. The length is 1 because it only has one element in it, the number 5. You can see the content of the object x in the main console either by using the print function we used earlier or by auto-printing, that is, just typing the name of the object and running that as code: x ## [1] 5 When writing expressions in R, you must understand that R is case sensitive. This could drive you nuts if you are not careful. More often than not, if you write an expression asking R to do something and R returns an error message, chances are that you used lowercase when uppercase was needed (or vice versa). So, always check for the right spelling. For example, see what happens if I use a capital ‘X’: X ## Error: object &#39;X&#39; not found You will get the following message: \"Error in eval(expr, envir, enclos): object 'X' not found\". R is telling us that X does not exist. There isn’t an object X (upper case), but there is an object x (lower case). Remember, computers are very literal. They are like dogs. You can tell a dog to “sit”, and if it has been trained, it will sit. But if you tell a dog, “Would you be so kind as to relax a bit and lie down on the sofa?” it won’t have a clue what you are saying and will stare at you like you have gone mad. Error messages are computers’ ways of telling us, “I really want to help you, but I don’t really understand what you mean” (never take them personally; computers don’t hate you). When you get an error message or implausible results, you want to look back at your code to figure out what the problem is. This process is called debugging. There are some proper systematic ways to write code that facilitate debugging, but we won’t get into that here. R is very good with automatic error handling at the levels we’ll be using it at. Very often, the solution will simply involve correcting the spelling or checking you’ve used the correct placement and number of brackets or commas (you will find this out later, the hard way). A handy tip is to cut and paste the error message into Google and find a solution. If anybody had given me a penny for every time I had to do that myself, I would be Bill Gates by now. You’re probably not the first person to make your mistake, after all, and someone on the internet has surely already found a solution to your issue. People make mistakes all the time. It’s how we learn. Don’t get frustrated, don’t get stuck. Instead, look for a solution. These days, we have Google. We didn’t back in the day. Now, you have the answer to your frustration within quick reach. Use it to your advantage. 1.9 Naming conventions for objects in R You may have noticed the various names I have used to designate objects (x, my_text, etc.). You can use almost any names you want for your objects. Objects in R can have names of any length consisting of letters, numbers, underscores (“_“) or the period (”.”) and should begin with a letter. In addition, when naming objects, you need to remember: Some names are forbidden. These include words such as FALSE and TRUE, logical operators, and programming words like Inf, for, else, break, function, and words for special entities like NA and NaN. You want to use names that do not correspond to a specific function. We have seen, for example, that there is a function called print(); you don’t want to call an object “print” to avoid conflicts. To avoid this, use nouns instead of verbs when naming your variables and data. You don’t want them to be too long (or you will regret it every time you need to use that object in your analysis: your fingers will bleed from typing). You want to make them as intuitive to interpret as possible. You want to follow consistent naming conventions. R users are terrible about this. However, we could make it better if we all aim to follow similar conventions. In these handouts, you will see that I follow the underscore_separated convention. See here for details. It is also important to remember that R will always treat numbers as numbers. This sounds straightforward, but actually, it is important to note. We can name our variables almost anything. EXCEPT they cannot be numbers. Numbers are protected by R. 1 will always mean 1. If you want, give it a try. Try to create a variable called 12 and assign it the value “twelve”. As we did in the sections above, we can assign something meaning by using the “&lt;-” characters. 12 &lt;- &quot;twelve&quot; ## Error in 12 &lt;- &quot;twelve&quot;: invalid (do_set) left-hand side to assignment You get an error! 1.10 R object types: vectors In R, there are different kinds of objects. We will start with vectors. What is a vector? A vector is simply a set of elements of the same class. Typically, these classes are character (i.e., text), numeric, integer, or logical (i.e., True or False). Vectors are the basic data structure in R. Typically, you will use the c() function (c stands for concatenate) to create vectors. The code below exemplifies how to create vectors of different classes (numeric, logical, character, etc.). Notice how the listed elements (to simplify, there are two elements in each vector below) are separated by commas ,: my_1st_vector &lt;- c(0.5, 0.6) #creates a numeric vector with two elements my_2nd_vector &lt;- c(1L, 2L) #creates an integer vector (&quot;L&quot; suffix specifies an integer type) my_3rd_vector &lt;- c(TRUE, FALSE) #creates a logical vector my_4th_vector &lt;- c(T, F) #creates a logical vector using abbreviations of True and False, #but you should avoid this formulation and instead use the full word. my_5th_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) #creates a character vector my_6th_vector &lt;- c(1+0i, 2+4i) #creates a complex vector (we won&#39;t really use in this class) Cut and paste this code into your script and run it. You will see how all these vectors are added to your global environment and stored there. The beauty of an object-oriented statistical language like R is that once you have these objects, you can use them as inputs in functions, use them in operations, or create other objects. This makes R very flexible. See some examples below: class(my_1st_vector) #a function to figure out the class of the vector ## [1] &quot;numeric&quot; length(my_1st_vector) #a function to figure out the length of the vector ## [1] 2 my_1st_vector + 2 #Add a constant to each element of the vector ## [1] 2.5 2.6 my_7th_vector &lt;- my_1st_vector + 1 #Create a new vector that contains #the elements of my1stvector plus a constant of 1 my_1st_vector + my_7th_vector #Adds the two vectors and Auto-print ## [1] 2.0 2.2 #the results (note how the sum was done) As indicated earlier, when you create objects, you place them in your working memory or workspace. Each R session will be associated with a workspace (called “global environment” in R Studio). In R Studio you can visualise the objects you have created during a session in the Global Environment screen. But if you want to produce a list of what’s there, you can use the ls() function (the results you get may differ from the ones below depending on what you actually have in your global environment). ls() #list all objects in your global environment ## [1] &quot;my_1st_vector&quot; &quot;my_2nd_vector&quot; &quot;my_3rd_vector&quot; &quot;my_4th_vector&quot; ## [5] &quot;my_5th_vector&quot; &quot;my_6th_vector&quot; &quot;my_7th_vector&quot; &quot;my_text&quot; ## [9] &quot;x&quot; If you want to delete a particular object, you can do so using the rm() function. rm(x) #remove x from your global environment It is also possible to remove all objects at once: rm(list = ls()) #remove all objects from your global environment If you mix in vector elements of a different class (for example, numerical and logical), R will coerce to the minimum common denominator so that every element in the vector is of the same class. So, for example, if you input a number and a character, it will coerce the vector to be a character vector - see the example below and notice the use of the class() function to identify the class of an object. my_8th_vector &lt;- c(0.5, &quot;a&quot;) class(my_8th_vector) #The class() function will tell us the class of the vector ## [1] &quot;character&quot; 1.11 R object types: Data frame Ok, so now that you understand some of the basic types of objects you can use in R, let’s start talking about data frames. One of the most common objects you will work with, in this course, are data frames. Data frames can be created with the data.frame() function. Data frames are multiple vectors of possibly different classes (e.g., numeric, factors, character) but of the same length (e.g., all vectors or variables have the same number of rows). This may sound a bit too technical, but it is simply a way of saying that a data frame is what in other programs for data analysis gets represented as data sets, like the tabular spreadsheets you have seen when using Excel. Let’s create a data frame with two variables: #We create a data frame called mydata_1 with two variables, #an integer vector called foo and a logical vector called bar mydata_1 &lt;- data.frame(foo = 1:4, bar = c(T,T,F,F)) mydata_1 ## foo bar ## 1 1 TRUE ## 2 2 TRUE ## 3 3 FALSE ## 4 4 FALSE Or alternatively, for the same result: x &lt;- 1:4 y &lt;- c(T, T, F, F) mydata_2 &lt;- data.frame (foo = x, bar = y) mydata_2 ## foo bar ## 1 1 TRUE ## 2 2 TRUE ## 3 3 FALSE ## 4 4 FALSE As you can see in R, as in any other language, there are multiple ways of saying the same thing. Programmers aim to produce code that has been optimised: it is short and quick. It is likely that as you develop your R skills, you find increasingly more efficient ways of asking R how to do things. What this means, too, is that when you go for help from your peers or us, we may teach you slightly different ways of getting the right result. As long as you get the right result, that’s what matters at this point. These are silly toy examples of data frames. In this course, we will use real data. Next week, we will learn in greater detail how to read data into R. But you should also know that R comes with pre-installed data sets. Some packages, in fact, are nothing but collections of data frames. Let’s have a look at some of them. We are going to look at some data that are part of the fivethirtyeight package. This package contains data sets and codes behind the stories from various online news articles. This package is not part of the base installation of R, so you will need to install it first. I won’t give you the code for it. See if you can figure it out by looking at previous examples. Done? Ok, now we are going to look at the data sets that are included in this package. Remember, first, we have to install and load the package if we want to use it: library(fivethirtyeight) ## Some larger datasets need to be installed separately, like senators and ## house_district_forecast. To install these, we recommend you install the ## fivethirtyeightdata package by running: ## install.packages(&#39;fivethirtyeightdata&#39;, repos = ## &#39;https://fivethirtyeightdata.github.io/drat/&#39;, type = &#39;source&#39;) #This function will return all the data frames that #are available in the named package. data(package=&quot;fivethirtyeight&quot;) Notice that this package has some data sets that relate to stories covered in this journal that had a criminological angle. Let’s look, for example, at the hate_crimes data set. How do you do that? First, we have to load the data frame into our global environment. To do so, use the following code: data(&quot;hate_crimes&quot;) This function will search among all the loaded packages and locate the “hate_crimes” data set. Every object in R can have attributes. These are names; dimensions (for matrices and arrays: number of rows and columns) and dimensions names; class of object (numeric, character, etc.); length (for a vector, this will be the number of elements in the vector); and other user-defined. You can access the attributes of an object using the attributes() function. Let’s query R for the attributes of this data frame. attributes(hate_crimes) ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## [51] 51 ## ## $class ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## ## $names ## [1] &quot;state&quot; &quot;state_abbrev&quot; ## [3] &quot;median_house_inc&quot; &quot;share_unemp_seas&quot; ## [5] &quot;share_pop_metro&quot; &quot;share_pop_hs&quot; ## [7] &quot;share_non_citizen&quot; &quot;share_white_poverty&quot; ## [9] &quot;gini_index&quot; &quot;share_non_white&quot; ## [11] &quot;share_vote_trump&quot; &quot;hate_crimes_per_100k_splc&quot; ## [13] &quot;avg_hatecrimes_per_100k_fbi&quot; These results printed in my console may not make too much sense to you at this point. We will return to this next week, so do not worry. Go now to the global environment panel and left-click on the data frame “hate_crimes”. This will open the data viewer in the top left section of R Studio. What you get there is a spreadsheet with 12 variables and 51 observations. Each variable, in this case, provides you with information (demographics, voting patterns, and hate crime) about each of the US states. 1.12 Exploring data Ok, let’s now have a quick look at the data. There are so many different ways of producing summary stats for data stored in R that it is impossible to cover them all! We will just introduce a few functions that you may find useful for summarising data. Before we do any of that, it is important you get a sense of what is available in this data set. Go to the help tab, and in the search box, input the name of the data frame; this will take you to the documentation for this data frame. Here, you can see a list of the available variables. Let’s start with the mean. This function takes as an argument the numeric variable for which you want to obtain the mean. Because of the way that R works, you cannot simply put the name of the variable; you have to tell R as well which data frame that variable is located in. To do that, you write the name of the data frame, the dollar sign($), and then the name of the variable you want to summarise. If you want to obtain the mean of the variable that gives us the proportion of people who voted for Donald Trump, you can use the following expression: mean(hate_crimes$share_vote_trump) ## [1] 0.49 This code is saying to look inside the “hate_crimes” dataset object and find the “share_vote_trump” variable, then print the mean. The $ is used when you want to find a particular component of an object. In the case of data frames, that component will typically be one of the vectors (variables). However, we will see other uses for other kinds of objects as we move through the course. Another function you may want to use with numeric variables is summary(): summary(hate_crimes$share_vote_trump) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.040 0.415 0.490 0.490 0.575 0.700 This gives you the five-number summary (minimum, first quartile, median, third quartile, and maximum, plus the mean and the count of missing values if there are any). You don’t have to specify a variable; you can ask for these summaries from the whole data frame: summary(hate_crimes) ## state state_abbrev median_house_inc share_unemp_seas ## Length:51 Length:51 Min. :35521 Min. :0.02800 ## Class :character Class :character 1st Qu.:48657 1st Qu.:0.04200 ## Mode :character Mode :character Median :54916 Median :0.05100 ## Mean :55224 Mean :0.04957 ## 3rd Qu.:60719 3rd Qu.:0.05750 ## Max. :76165 Max. :0.07300 ## ## share_pop_metro share_pop_hs share_non_citizen share_white_poverty ## Min. :0.3100 Min. :0.7990 Min. :0.01000 Min. :0.04000 ## 1st Qu.:0.6300 1st Qu.:0.8405 1st Qu.:0.03000 1st Qu.:0.07500 ## Median :0.7900 Median :0.8740 Median :0.04500 Median :0.09000 ## Mean :0.7502 Mean :0.8691 Mean :0.05458 Mean :0.09176 ## 3rd Qu.:0.8950 3rd Qu.:0.8980 3rd Qu.:0.08000 3rd Qu.:0.10000 ## Max. :1.0000 Max. :0.9180 Max. :0.13000 Max. :0.17000 ## NA&#39;s :3 ## gini_index share_non_white share_vote_trump hate_crimes_per_100k_splc ## Min. :0.4190 Min. :0.0600 Min. :0.040 Min. :0.06745 ## 1st Qu.:0.4400 1st Qu.:0.1950 1st Qu.:0.415 1st Qu.:0.14271 ## Median :0.4540 Median :0.2800 Median :0.490 Median :0.22620 ## Mean :0.4538 Mean :0.3157 Mean :0.490 Mean :0.30409 ## 3rd Qu.:0.4665 3rd Qu.:0.4200 3rd Qu.:0.575 3rd Qu.:0.35693 ## Max. :0.5320 Max. :0.8100 Max. :0.700 Max. :1.52230 ## NA&#39;s :4 ## avg_hatecrimes_per_100k_fbi ## Min. : 0.2669 ## 1st Qu.: 1.2931 ## Median : 1.9871 ## Mean : 2.3676 ## 3rd Qu.: 3.1843 ## Max. :10.9535 ## NA&#39;s :1 So you see how now we are getting this info for all variables in one go. There are multiple ways of getting results in R. Particularly for basic and intermediate-level statistical analysis, many core functions and packages can give you the answer that you are looking for. For example, there are a variety of packages that allow you to look at summary statistics using functions defined within those packages. You will need to install these packages before you can use them. I am only going to introduce one of them here skimr. It is neat and is maintained by the criminologist Elin Waring, an example of kindness and dedication to her students. You will need to install it before anything else. Use the code you have learnt to do so and then load it. I won’t be providing you the code for it; by now you should know how to do this. Once you have loaded the skimr package, you can use it. Its main function is skim. Like summary for data frames, skim presents results for all the columns, and the statistics will depend on the class of the variable. However, the results are displayed and stored in a nicer way - though we won’t get into the details right now. skim(hate_crimes) skim_type skim_variable n_missing complete_rate character.min character.max character.empty character.n_unique character.whitespace numeric.mean numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 numeric.hist character state 0 1.0000000 4 20 0 51 0 NA NA NA NA NA NA NA NA character state_abbrev 0 1.0000000 2 2 0 51 0 NA NA NA NA NA NA NA NA numeric median_house_inc 0 1.0000000 NA NA NA NA NA 5.522361e+04 9208.4781698 3.552100e+04 4.865700e+04 5.491600e+04 6.071900e+04 76165.000000 ▂▆▇▅▂ numeric share_unemp_seas 0 1.0000000 NA NA NA NA NA 4.956860e-02 0.0106981 2.800000e-02 4.200000e-02 5.100000e-02 5.750000e-02 0.073000 ▅▇▇▇▂ numeric share_pop_metro 0 1.0000000 NA NA NA NA NA 7.501961e-01 0.1815873 3.100000e-01 6.300000e-01 7.900000e-01 8.950000e-01 1.000000 ▁▂▅▆▇ numeric share_pop_hs 0 1.0000000 NA NA NA NA NA 8.691176e-01 0.0340732 7.990000e-01 8.405000e-01 8.740000e-01 8.980000e-01 0.918000 ▃▅▃▆▇ numeric share_non_citizen 3 0.9411765 NA NA NA NA NA 5.458330e-02 0.0310770 1.000000e-02 3.000000e-02 4.500000e-02 8.000000e-02 0.130000 ▇▆▆▂▂ numeric share_white_poverty 0 1.0000000 NA NA NA NA NA 9.176470e-02 0.0247148 4.000000e-02 7.500000e-02 9.000000e-02 1.000000e-01 0.170000 ▂▇▃▂▁ numeric gini_index 0 1.0000000 NA NA NA NA NA 4.537647e-01 0.0208908 4.190000e-01 4.400000e-01 4.540000e-01 4.665000e-01 0.532000 ▆▇▅▁▁ numeric share_non_white 0 1.0000000 NA NA NA NA NA 3.156863e-01 0.1649152 6.000000e-02 1.950000e-01 2.800000e-01 4.200000e-01 0.810000 ▇▇▆▂▁ numeric share_vote_trump 0 1.0000000 NA NA NA NA NA 4.900000e-01 0.1187097 4.000000e-02 4.150000e-01 4.900000e-01 5.750000e-01 0.700000 ▁▁▆▇▇ numeric hate_crimes_per_100k_splc 4 0.9215686 NA NA NA NA NA 3.040930e-01 0.2527086 6.744680e-02 1.427066e-01 2.261971e-01 3.569347e-01 1.522302 ▇▂▁▁▁ numeric avg_hatecrimes_per_100k_fbi 1 0.9803922 NA NA NA NA NA 2.367613e+00 1.7142450 2.669408e-01 1.293139e+00 1.987068e+00 3.184344e+00 10.953480 ▇▅▁▁▁ Apart from summary statistics, last semester, we discussed a variety of ways to graphically display variables. Week 3 of ‘Making Sense of Criminological Data’ covered scatterplots, a graphical device to show the relationship between two quantitative variables. I don’t know if you remember the number of points and clicks you had to make in Excel to get this done. 1.13 R data types: Factors An important thing to understand in R is that categorical (ordered, also called ordinal, or unordered, also called nominal) data are typically encoded as factors, which are just a special type of vector. A factor is simply an integer vector that can contain only predefined values (this bit is very important) and is used to store categorical data. Factors are treated specially by many data analytic and visualisation functions. This makes sense because they are essentially different from quantitative variables. Although you can use numbers to represent categories, using factors with labels is better than using integers to represent categories because factors are self-describing (having a variable that has values “Male” and “Female” is better than a variable that has values “1” and “2” to represent male and female). When R reads data in other formats (e.g., comma-separated), it will usually convert all character variables into factors by default. If you would rather keep these variables as simple character vectors, you need to explicitly ask R to do so. We will come back to this next week with some examples. Factors can also be created with the factor() function concatenating a series of character elements. You will notice that it is printed differently from a simple character vector and that it tells us the levels of the factor (i.e., each unique value in the factor). the_smiths &lt;- factor(c(&quot;Morrisey&quot;, &quot;Marr&quot;, &quot;Rourke&quot;, &quot;Joyce&quot;)) #create a new factor the_smiths #auto-print the factor ## [1] Morrisey Marr Rourke Joyce ## Levels: Joyce Marr Morrisey Rourke Alternatively, for similar results, use the as.factor() function. Here, you will create the_smiths_char object and then transform it to a factor variable, the_smiths_f. the_smiths_char &lt;- c(&quot;Morrisey&quot;, &quot;Marr&quot;, &quot;Rourke&quot;, &quot;Joyce&quot;) #create a character vector the_smiths_f &lt;- as.factor(the_smiths_char) #create a factor using a character vector the_smiths_f #auto-print factor ## [1] Morrisey Marr Rourke Joyce ## Levels: Joyce Marr Morrisey Rourke Factors in R can be seen as vectors with more information added. This extra information consists of a record of the distinct values in that vector, called levels. If you want to know the levels in a given factor, you can use the levels() function: levels(the_smiths_f) ## [1] &quot;Joyce&quot; &quot;Marr&quot; &quot;Morrisey&quot; &quot;Rourke&quot; Notice that the levels appear printed in alphabetical order (Try levels(the_smiths_char) and see what R says. Yes, the_smiths_char and the_smiths_f are different!). There will be situations when this is not the most convenient order (e.g., Dec, Jan, Mar (Alphabetical order) instead of Jan, Mar, Dec). Later on, we will discuss in these tutorials how to reorder your factor levels when you need to. Let’s look at one more example here. Let’s say we are making data about Hogwarts Houses, which are divided into four houses: Gryffindor, Hufflepuff, Ravenclaw and Slytherin. #We create a data frame called HarryPotter with two variables: #a character vector called name and a character vector called house HarryPotter &lt;- data.frame(name = c(&quot;Potter&quot;, &quot;Malfoy&quot;, &quot;Lovegood&quot;, &quot;Chang&quot;, &quot;Hagrid&quot;, &quot;Diggory&quot;), house = c(&quot;Gryffindor&quot;, &quot;Slytherin&quot;, &quot;Ravenclaw&quot;, &quot;Ravenclaw&quot;, &quot;Gryffindor&quot;, &quot;Hufflepuff&quot;)) HarryPotter ## name house ## 1 Potter Gryffindor ## 2 Malfoy Slytherin ## 3 Lovegood Ravenclaw ## 4 Chang Ravenclaw ## 5 Hagrid Gryffindor ## 6 Diggory Hufflepuff Use str(HarryPotter$house) and see what R says. R will list all observations in the variable and say it’s a character variable, right? Now, we are going to convert house, a character variable, into a factor variable house_f, meaning that R will categorise the variable. HarryPotter$house_f &lt;- as.factor(HarryPotter$house) str(HarryPotter$house_f) ## Factor w/ 4 levels &quot;Gryffindor&quot;,&quot;Hufflepuff&quot;,..: 1 4 3 3 1 2 levels(HarryPotter$house_f) ## [1] &quot;Gryffindor&quot; &quot;Hufflepuff&quot; &quot;Ravenclaw&quot; &quot;Slytherin&quot; #try &#39;levels(HarryPotter$house)&#39; and find the difference Now, can you clearly understand what factor means in R? Factors are used to represent categorical data. Once created, factors can contain pre-defined set values, known as levels. Like we just converted 6-character data (house) into 4-factor data! (house_f). 1.14 How to import data Programmers are lazy, and the whole point of using code-based interfaces is that we get to avoid doing unnecessary work, like point-and-click downloading of files. When data exists online in a suitable format, we can tell R to read the data from the web directly and cut out the middleman (that being ourselves in our pointing-and-clicking activity). How can we do this? Well, think about what we do when we read in a file. We say, “Dear R, I would like to create a new object, please, and I will call this new object my_dataframe”. We do this by typing the name we are giving the object and the assignment function &lt;-(assignment operator). Then, on the right-hand side of the assignment function, there is the value to which we are assigning the variable. So it could be a bit of text (such as when you’re creating a my_text object and you pass it the string “I love stats”), or it could be some function, for example, when you read a CSV file with the read_csv() function. So, if we’re reading a CSV, we also need to specify where to read the CSV from. Where should R look to find this data? This is where normally you are putting in the path to your file, right? Something like: my_dataframe &lt;- read.csv(&#39;PATH_OR_URL_TO_CSV_FILE&#39;) Well, what if your data does not live on your laptop or PC? Well, if there is a way that R can still access this data just by following a path, then this approach will still work! Tips! Please keep your folders simple. Just use one data folder for this module and do not create multiple folders such as ‘data for week 1’, ‘data for week 2’, or something like this. All data we will use in the module are on Canvas, but we will also provide the links to where we saved the data for you. You know when you right-click on the link, and select “Save As…” or whatever you click on to save? You could also select “Copy Link Address”. This just copies the webpage where this data is stored. If you want, find a csv file from a previous module on Canvas, and give this a go. Copy the address, and then paste it into your browser. It will take you to a blank page where a forced download of the data will begin. So what if you pasted this into the read.csv() function? #example: when you download data directly from the webpage, #you will use this code. my_dataframe &lt;- read.csv(&quot;www.data.com/data you want to import.csv&quot;) #example : if you can&#39;t find a URL from Canvas to use yet, #you can try this random dataset. The dataset itself is irrelevant #(although it is about UFO sightings - pretty interesting if you ask me), #but it does show you how to load data in from a real URL. ufo_sightings &lt;- read.csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/ufo_sightings.csv&#39;) In the first example, the my_dataframe object would be assigned the value returned from the read.csv() function reading in the file from the ‘URL’ link you provided. File path is no mysterious thing, file path is simply the path to the file you want to read. If this is a website, then so be it. R also can read Stata (.dta) files using the read_dta() function in the Haven package and SPSS (.sav) files using the read_spss() function also in the Haven Package. There are so many different codes we can use to import data into R. In this course, you will learn one by one! 1.15 How to use ‘comment’ In the bits of code above, you will have noticed parts that were greyed out, for instance, in the last example provided. You can see that after the hashtag, all the text is being greyed out. What is this? What’s going on? These are comments. Comments are simply annotations that R will know are not code (and therefore don’t attempt to understand and execute). We use the hash-tag symbol to specify to R that what comes after is not programming code but simply bits of notes that we write to remind ourselves what the code is actually doing. Including these comments will help you to understand your code when you come back to it. To create a comment, you use the hashtag/ sign # followed by some text. Whenever the R engine sees the hashtag (#), it knows that what follows is not code to be executed. You can use this sign to include annotations when you are coding. These annotations are a helpful reminder to yourself (and others reading your code) of what the code is doing and (even more important) why you are doing it. It is good practice to often use annotations. You can use these annotations in your code to explain your reasoning and to create “scannable” headings in your code. That way, after you save your script, you will be able to share it with others or return to it at a later point and understand what you were doing when you first created it -see here for further details on annotations and how to save a script when working with the basic R interface. Just keep in mind: You need one # per line, and anything after that is a comment that is not executed by R. You can use spaces after. 1.16 How to Quit RStudio At some point, you will quit your R/R Studio session. I know it’s hard to visualise, right? Why would you want to do that? Anyhow, when that happens, R Studio will ask you a hard question: “Save workspace image to bla bla bla/.RData?” What to do? What does that even mean? If you say “yes”, what will happen is that all the objects you have in your environment will be preserved, alongside the History (which you can access in the top right set of windows) listing all the functions you have run within your session. So, next time you open this project, everything will be there. If you think that what is real is those objects and that history, well, then you may think that’s what you want to do. The truth is what is real is your scripts and the data that your scripts use as inputs. You don’t need anything in your environment because you can recreate those things by re-running your scripts. I like keeping things tidy, so when I am asked whether I want to save the image, my answer is always no. Most long-time users of R never save the workspace or care about saving the history. Remember, what is real is your scripts and the data. Just so you know, though, you should not then panic if you open your next R Studio session and you don’t see any objects in your environment. The good news is you can generate them quickly enough (if you really need them) by re-running your scripts. I would suggest that, at this point, it may be helpful for you to get into this habit as well. I suspect otherwise you will be in week 9 of the semester and have an environment full of garbage you don’t really need. What is more. I would suggest you go to the Tools drop-down menu, select Global Options, and make sure you select “Never” where it says “Save workspace”. Then click “Apply”. This way, you will never be asked to save what is in your global environment when you terminate a session. 1.17 Summary This week, we used the following R functions: install and load a package install.packages() library() generate and print data my_text &lt;- “I love stats.” print() say() explore data search() skim() attribute() mean() summary() str() transform variables into factor variables as_factor() "],["getting-to-know-your-data.html", "Chapter 2 Getting to know your data 2.1 Causality in Social Sciences 2.2 Getting data thanks to reproducibility 2.3 Getting a sense of your data 2.4 Data wrangling with dplyr 2.5 Using dplyr single verbs 2.6 Using dplyr for grouped operations 2.7 Making comparisons with numerical outcomes 2.8 Summary", " Chapter 2 Getting to know your data 2.1 Causality in Social Sciences In today’s session, we will refresh themes you have explored in previous research methods courses, specifically causality. This is a central concept in empirical research. We often do research because we want to make causal inferences. We want to be in a position where we establish whether an intervention or a social process is causally linked to crime or some other relevant criminological outcome. Making causal inferences often involves making comparisons. For example, between cases that have been subject to an intervention and cases that have not been subject to the intervention (the causal process we are trying to investigate). But you should already know that not all kinds of research comparisons are the same. In previous methods courses, you must have discussed the differences between experimental and observational studies. These different kinds of research designs have a bearing on your ability to make causal inferences. Let’s think about a specific case so that this makes more sense. Is there discrimination against former offenders in the labour market? In other words, are offenders less likely to find employment after release because of prejudice among employers? Or can we say that the fact that former offenders are less likely to be in employment may be due to other factors? Perhaps they are less skilled. Perhaps they have less social capital: people they know that can help them to get jobs or to learn about job opportunities. Perhaps they are less interested in finding employment? Only in comparisons, when other things are equal, can you make causal inferences. It would only be fair to compare John, an ex-offender with a high school degree and X number of personal connections and Y numbers of professional experience, with Peter, a non-ex-offender, with the same educational and professional credentials as John (and everything else that matters when getting jobs also being equal between Peter and John). How can you do that? How can you create situations when other things are equal? Well, that’s what courses in research design are designed to teach you. What is important for you to remember is that the way data are generated and the way you do your study will, of course, affect the kind of interpretations that you make from your statistical comparisons. And not all studies are created equal. Some research designs put you in a better position than others to make causal inferences. You should have learnt by now that the “bronze” standard for establishing causality in the social sciences is the randomised experiment. In a randomised trial, the researchers change the causal variable of interest for a group using something like a coin toss. As Angrist and Pischke (2015: xiii) highlight: “By changing circumstances randomly, we make it highly likely that the variable of interest is unrelated to the many other factors determining the outcomes we mean to study… Random manipulation makes other things equal hold on average across the groups that did and did not experience manipulation” So, say you want to establish whether arresting a perpetrator may have a deterrent effect on subsequent domestic abuse. You could randomise, basically using the equivalent of a lottery, to decide whether the police officer is going to arrest the perpetrator or not and then compare those you arrest with those you don’t arrest. Because you are randomising your treatment (the arrest), on average, the treatment and the control group, in the long run, should be fairly similar, and any differences you observe between them in the outcome of interest (domestic abuse recidivism) you could link it to your intervention -if you are interested in the answer to this you can read about it here. In this session, we will look at data from a randomised trial that tried to establish whether discrimination against former offenders exists in the labour market. In doing so, we will also learn various functions used in R to read data, transform data, and obtain summary statistics for groups. We will also very quickly introduce a plotting function used in R to generate graphics. 2.2 Getting data thanks to reproducibility Last week we introduced the notion of reproducible research and said that using and publishing code (particularly if using open-source tools like R) is the way that many researchers around the world think that science ought to be done. This way of operating makes research more open, more credible, and more legitimate. It also means that we can more easily access the data used in published research. For this session, we are going to use the data from this and this paper study. In this research project, the authors tried to answer the question of whether criminal antecedents (e.g., prior convinctions) and other personal characteristics have an impact on access to employment. You can find more details about this work in episode 8 of Probable Causation, the criminology podcast. If you want to know more about ‘Ban the Box: fair chance recruitment’ practices in the UK, you can find more information here and also can watch this short video from Leo Burnett who helped promote giving people a chance to explain their past. This could give you a better understanding of this issue. Amanda Agan and Sonja Starr developed a randomised experiment in which they created 15,220 fake resumes randomly generating these critical characteristics (such as having a criminal record) and used these resumes to send online job applications to low-skill, entry-level job openings in New Jersey and New York City. All the fictitious applicants were male and about 21 to 22 years old. These kinds of experiments are very common among researchers who want to explore through these “audits” whether some personal characteristics are discriminated against in the labour market. Because Amanda Agan and Sonja Starr conformed to reproducible standards when doing their research, we can access this data from the Harvard Dataverse (a repository for open research data). Click here to locate the data. On this page, you can see a download section and some files that can be accessed. One of them contains analytic code pertaining to the study, and the other contains the data. You also see a link called metadata. Metadata is data about data – it provides you with some information about the data. If you click on metadata, you will see a reference to the software the authors used (STATA) at the bottom. So we know these files are in STATA, a proprietary format. Let’s download the data file and then read the data into R. You could just click “download” and then place the file in your project directory. Alternatively, and preferably, you may want to use code to make your whole work more reproducible. Think of it this way: every time you click or use drop-down menus, you are doing things that others cannot reproduce because there won’t be a written record of your steps. You will need to do some clicking to find the required URL for writing your code. The file we want is the AganStarrQJEData.dta. Click on the name of this file. You will be taken to another webpage. You will see the download URL on it. Copy and paste this URL into your code below. #First, let&#39;s import the data using an URL address: library(haven) banbox &lt;- read_dta(&quot;https://dataverse.harvard.edu/api/access/datafile/3036350&quot;) ##Window users! R in Windows has some problems with HTTPS addresses; #see below for some tips on this! This data file is a STATA.dta file in our working directory. To read STATA files, we will need the haven package. This is a package developed to import different kinds of data files into R. If you don’t have it, you will need to install it. And then load it. ##IF THE CODE ABOVE DOES NOT WORK, USE THIS CODE. ##Window users! R in Windows have some problems with https addresses, #in that case, try to use this code #First, let&#39;s create an object with the link. Paste the copied address here: urlfile &lt;- &quot;https://dataverse.harvard.edu/api/access/datafile/3036350&quot; #Now we can use the &#39;read_dta&#39; and &#39;url&#39; functions and import the data in the urlfile link library(haven) banbox &lt;- read_dta(url(urlfile)) You will need to pay attention to the file extension to find the appropriate function to read in your file. For example, if something has the extension .sav, it is a file used by the software SPSS. To read this, for example, you would use the read_spss() function in the haven package. Some other file types need other packages. For example, to read in comma-separated values or .csv files, you can use the read_csv() function from the readr package. To read in Excel files, you would use the appropriate function from the readxl package, which might be read_xls() or read_xlsx(), depending on the file extension. 2.3 Getting a sense of your data 2.3.1 First steps What is the first thing you need to ask yourself when you look at a dataset? Data are often too big to look at the whole thing. It is almost always impossible to eyeball the entire dataset and see what you have in terms of interesting patterns or potential problems. It is often a case of information overload, and we want to be able to extract what is relevant and important about it. But where do you start? You can find a brief but very useful overview put together by Steph de Silva in the image below. Read it before we carry on. As Mara Averick suggests, this also makes for good relationship advice! Here, we will introduce a few functions that will help you make sense of what you have just downloaded. Summarising the data is the first step in any analysis, and it is also used to find potential problems with the data. Regarding the latter, you want to look out for missing values, values outside the expected range (e.g., someone aged 200 years), values that seem to be in the wrong units, mislabeled variables, or variables that seem to be the wrong class (e.g., a quantitative variable encoded as a factor). Let’s start with the basic things you always look at first in a dataset. You can see in the Environment window that banbox has 14813 observations (rows) of 62 variables (columns). You can also obtain this information using code. Here, you want the DIMensions of the data frame (the number of rows and columns), so you use the dim() function: dim(banbox) ## [1] 14813 62 Looking at this information will help you to diagnose whether there was any trouble getting your data into R (e.g., imagine you know there should be more cases or more variables). You may also want to have a look at the names of the columns using the names() function. We will see the names of the variables. names(banbox) ## [1] &quot;nj&quot; &quot;nyc&quot; &quot;app_date_d&quot; ## [4] &quot;pre&quot; &quot;post&quot; &quot;storeid&quot; ## [7] &quot;chain_id&quot; &quot;center&quot; &quot;crimbox&quot; ## [10] &quot;crime&quot; &quot;drugcrime&quot; &quot;propertycrime&quot; ## [13] &quot;ged&quot; &quot;empgap&quot; &quot;white&quot; ## [16] &quot;black&quot; &quot;remover&quot; &quot;noncomplier_store&quot; ## [19] &quot;balanced&quot; &quot;response&quot; &quot;response_date_d&quot; ## [22] &quot;daystoresponse&quot; &quot;interview&quot; &quot;cogroup_comb&quot; ## [25] &quot;cogroup_njnyc&quot; &quot;post_cogroup_njnyc&quot; &quot;white_cogroup_njnyc&quot; ## [28] &quot;ged_cogroup_njnyc&quot; &quot;empgap_cogroup_njnyc&quot; &quot;box_white&quot; ## [31] &quot;pre_white&quot; &quot;post_white&quot; &quot;white_nj&quot; ## [34] &quot;post_remover_ged&quot; &quot;post_ged&quot; &quot;remover_ged&quot; ## [37] &quot;post_remover_empgap&quot; &quot;post_empgap&quot; &quot;remover_empgap&quot; ## [40] &quot;post_remover_white&quot; &quot;post_remover&quot; &quot;remover_white&quot; ## [43] &quot;raerror&quot; &quot;retail&quot; &quot;num_stores&quot; ## [46] &quot;avg_salesvolume&quot; &quot;avg_num_employees&quot; &quot;retail_white&quot; ## [49] &quot;retail_post&quot; &quot;retail_post_white&quot; &quot;percblack&quot; ## [52] &quot;percwhite&quot; &quot;tot_crime_rate&quot; &quot;nocrimbox&quot; ## [55] &quot;nocrime_box&quot; &quot;nocrime_pre&quot; &quot;response_white&quot; ## [58] &quot;response_black&quot; &quot;response_ged&quot; &quot;response_hsd&quot; ## [61] &quot;response_empgap&quot; &quot;response_noempgap&quot; As you may notice, these names may be hard to interpret. If you open the dataset in the data viewer of RStudio (using View), you will see that each column has a variable name and underneath a longer and more meaningful variable label that tells you what each variable means. View(banbox) 2.3.2 On tibbles and labelled vectors You also want to understand what the banbox object actually is. You can do that using the class() function: class(banbox) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; What does tbl stand for? It refers to tibbles. This is essentially a new type of data structure introduced into R. Tibbles are data frames, but a particular type. There are some advantages to tibbles, but honestly, they’re not that important. For now, you can just think of tibbles as data frames (and vice versa). The R language has been around for a while, and sometimes, things that made sense a couple of decades ago make less sense now. A number of programmers are trying to create code that is more modern and more useful today. They are doing this by introducing a set of packages that speak to each other in order to modernise R without breaking existing code. You can think of it as an easier and more efficient modern dialect of R. This set of packages is called the tidyverse. Tibbles are data frames that have been optimised for use with this new set of packages. You can read a bit more about tibbles here. You can also look at the class of each individual column. As discussed, the class of the variable lets us know, for example, if it is an integer (number), character, or factor. To get the class of one variable, you pass it to the class() function. For example: class(banbox$crime) ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; class(banbox$num_stores) ## [1] &quot;numeric&quot; We talked about numeric vectors in week one. It is simply a collection of numbers. But what is a labelled vector? This is a new type of vector introduced by the haven package. Labelled vectors are categorical variables that have labels. Go to the Environment panel and left-click in the banbox object. This should open the data browser in the top left quadrant of RStudio. If you look carefully, you will see that the various columns that include categorical variables only contain numbers. In many statistical environments, such as STATA or SPSS, this is a common standard. The variables have a numeric value for each observation, and each of these numeric values is associated with a label. This made sense when computer memory was an issue - for this was an efficient way of saving resources. It also made manual data input quicker. These days, it makes perhaps less sense. However, labelled vectors give you a chance to reproduce data from other statistical environments without losing any fidelity in the import process. See what happens if we try to summarise this labelled vector. We will use the table() to provide a count of observations on the various valid values of the crime variable. It is a function that obtains your frequency distribution. table(banbox$crime) ## ## 0 1 ## 7323 7490 So, we see that we have 7490 observations classed as 1 and 7323 classed as 2. If only we knew what those numbers represent! Well, we actually do. We will use the attributes() function to see the different “compartments” within your “box”, your object. attributes(banbox$crime) ## $label ## [1] &quot;Applicant has Criminal Record&quot; ## ## $format.stata ## [1] &quot;%9.0g&quot; ## ## $class ## [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; ## ## $labels ## No Crime Crime ## 0 1 So, this object has different compartments. The first one is called a label, and it provides a description of what the variable measures. This is what you saw in the RStudio data viewer earlier. The second compartment explains the original format. The third one identifies the class of the vector. Whereas, the final one, labels, provides the labels that allow us to identify what the meaning of 0 and 1 mean in this context. 2.3.3 Turning variables into factors and changing the labels Last week, we said that many R functions expect factors when you have categorical data, so typically, after you import data into R you may want to coerce your labelled vectors into factors. To do that, you need to use the as_factor() function of the haven package. Let’s see how we do that. #This code asks R to create a new column in your banbox tibble #that is going to be called crime_f. Typically, when you alter #variables, you can to create a new one so that the original gets #preserved in case you do something wrong. Then we use the #as_factor() function to explain to R that what we want to do #is to get the original crime variable and mutate it into #a factor, this resulting factor is what will be stored in #the new column. banbox$crime_f &lt;- as_factor(banbox$crime) You will see now that you have 63 variables in your dataset; look at the environment to check. Let’s explore the new variable we have created (you can also look for the new variable in the data browser and see how it looks different to the original crime variable): class(banbox$crime_f) ## [1] &quot;factor&quot; table(banbox$crime_f) ## ## No Crime Crime ## 7323 7490 attributes(banbox$crime_f) ## $levels ## [1] &quot;No Crime&quot; &quot;Crime&quot; ## ## $class ## [1] &quot;factor&quot; ## ## $label ## [1] &quot;Applicant has Criminal Record&quot; So far, we have looked at single columns in your data frame one at a time. But there is a way that you can apply a function to all elements of a vector (list or data frame). You can use the functions sapply(), lapply(), and mapply() . To find out more about when to use each one see here. For example, we can use the lapply() function to look at each column and get its class. To do so, we have to pass two arguments to the lapply() function, the first is the name of the data frame to tell it what to look through, and the second is the function we want it to apply to every column of that function. So we want to type lapply('name of dataframe', 'name of function') Which is: lapply(banbox, class) As you can see, many variables are classed as ‘labelled’. This is common with survey data. Many of the questions in social surveys measure the answers as categorical variables (e.g., these are nominal or ordinal-level measures). In fact, with this dataset there are many variables that are encoded as numeric that really aren’t. Welcome to real-world data, where things can be a bit messy and need tidying! See, for example, the variable black: class(banbox$black) ## [1] &quot;numeric&quot; table(banbox$black) ## ## 0 1 ## 7406 7407 We know that this variable measures whether someone is black or not. When people use 0 and 1 to code binary responses, typically, they use a 1 to denote a positive response, a yes. So, I think it is fair to assume that a 1 here means the respondent is black. Because this variable is of class numeric, we cannot simply use as_factor() to assign the pre-existing labels and create a new factor. In this case, we don’t have preexisting labels since this is not a labelled vector. So what can we do to tidy this variable? We’ll need to do some further work. #We will use a slightly different function as.factor() banbox$black_f &lt;- as.factor(banbox$black) #You can check that the resulting column is a factor class(banbox$black_f) ## [1] &quot;factor&quot; #But if you print the frequency distribution #you will see the data are still presented #in relation to 0 and 1 table(banbox$black_f) ## ## 0 1 ## 7406 7407 #You can use the levels function to see the levels of the categories in your factor levels(banbox$black_f) ## [1] &quot;0&quot; &quot;1&quot; So, all we have done is create a new column that is a factor but still refers to 0 and 1. If we assume (rightly) that 1 means black, we have 7407 black applicants. Of course, it makes sense we only get 0 and 1 here. What else could R do? This is not a labelled vector, so there is no way for R to know that 0 and 1 mean anything other than 0 and 1, which is why those are the levels is using. But now that we have the factor, we can rename those levels. We can use the following code to do just that: #We are using the levels function to access them and change #them to the levels we specify with the c() function. Be #careful here because the order we specify here will map #out to the order of the existing levels. #So given that 1 is black and black is the second level #(as shown when printing the results above) you want to make sure that in the c() #you write black as the second level. levels(banbox$black_f) &lt;- c(&quot;non-Black&quot;, &quot;Black&quot;) table(banbox$black_f) ## ## non-Black Black ## 7406 7407 This gives you an idea of the kind of transformations you often want to perform to make your data more useful for your purposes. But let’s keep looking at functions you can use to explore your dataset. 2.3.4 Looking for missing data and other anomalies You can, for example, use the head() function if you just want to visualise the values for the first few cases in your dataset. The next code, for example, asks for the values for the first two cases. If you want a different number to be shown, you just need to change the number you are passing as an argument. head(banbox, 2) In the same way, you could look at the last two cases in your dataset using tail(): tail(banbox, 2) It is good practice to do this to ensure R has read the data correctly and there’s nothing terribly wrong with your dataset. If you have access to STATA you can open the original file in STATA and check if there are any discrepancies, for example. Glimpsing your data in this way can also give you a first impression of what the data looks like. One thing you may also want to do is to see if there are any missing values. For that, we can use the is.na() function. Missing values in R are coded as NA. The code below, for example, asks for NA values for the variable response_black in the banbox object for observations 1 to 10: is.na(banbox$response_black[1:10]) ## [1] FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE TRUE The result is a logical vector that tells us if it is true that there is missing (NA) data for each of those first ten observations. You can see that there are 6 observations out of those 10 that have missing values for this variable. sum(is.na(banbox$response_black)) ## [1] 7406 The above is asking R to sum up how many cases are TRUE NA in this variable. When reading a logical vector like the one we are creating, R will treat the FALSE elements as 0s and the TRUE elements as 1s. So basically, the sum() function will count the number of TRUE cases returned by the is.na() function. You can use a bit of a hack to get the proportion of missing cases instead of the count: mean(is.na(banbox$response_black)) ## [1] 0.4999662 This code exploits the mathematical fact that the mean of binary outcomes (0 or 1) gives you the proportion of ones in your data. You could verify this in R using a different method, if you’d like (remember that trying stuff out, making mistakes and getting errors is just part of the process of learning R!). As a rule of thumb, if you see more than 5% of the cases declared as NA, you need to start thinking about the implications of this. Beware of formulaic application of rules of thumb such as this, though! In this case, we know that 49%% of the observations have missing values in this variable. When you see things like this the first thing to do is to look at the codebook or documentation to try to get some clues as to why there are so many missing cases. With survey data, you often have questions that are simply not asked to everybody, so it’s not necessarily that something went very wrong with the data collection but simply that the variable in question was only used with a subset of the sample. Therefore, any analysis you do using this question will only relate to that particular subset of cases. There is a whole field of statistics devoted to doing analysis when missing data is a problem. R has extensive capabilities for dealing with missing data -see, for example here. For the purpose of this introductory course, however, we only explain how to do analysis that ignores missing data. This is often referred to as a full/complete case analysis because you only use observations for which you have full information in all the variables you employ. You would cover techniques for dealing with this sort of issue in more advanced courses. 2.4 Data wrangling with dplyr The data analysis workflow has a number of stages. The diagram below (produced by Hadley Wickham) is a nice illustration of this process: We have started to see different ways of bringing data into R. And we have also started to see how we can explore our data. It is now time we start discussing one of the following stages, transform. A good deal of time and effort, in data analysis, is devoted to this. You get your data, and then you have to transform it so that you can answer the questions you want to address in your research. We have already seen, for example, how to turn variables into factors, but there are other things you may want to do. R offers a great deal of flexibility in how to transform your data; here, we are going to illustrate some of the functionality of the dplyr package for data carpentry (a term people use to refer to this kind of operation). This package is part of the tidyverse and it aims to provide a friendly and modern take on how to work with data frames (or tibbles) in R. It offers, as the authors of the package put it, “a flexible grammar of data manipulation”. Dplyr aims to provide a function for each basic verb of data manipulation: filter() to select cases based on their values. arrange() to reorder the cases. select() and rename() to select variables based on their names. mutate() and transmute() to add new variables that are functions of existing variables. summarise() to condense multiple values to a single value. sample_n() and sample_frac() to take random samples. In this session, we will introduce and practice some of these. But we won’t have time to cover everything. There is, however, a very nice set of vignettes for this package in the help files, so you can try to go through those if you want a greater degree of detail or more practice. Now, let’s load the package: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Notice that when you run this package you get a series of warnings in the console. It is telling us that some functions from certain packages are being “masked”. One of the things with a language like R is that sometimes packages introduce functions that have the same name as others that are already loaded into your session. When that happens, the newly loaded ones will override the previous ones. You can still use them, but you will have to refer to them explicitly. Otherwise, R will assume you are using the function most recently loaded: #Example: #If you use load dplyr and then invoke the *filter()* function #R will assume you are using the filter function from dplyr #rather than the *filter()* function that exist in the *stats* #package, which is part of the basic installation of R. If #after loading dplyr you want to use the filter function from #the stats package, you will have to invoke it like this: stats::filter() #Notice the grammar, first you write the name of the package, #then colon twice, and then the name of the function. Don&#39;t #run this code. You would need to pass some valid arguments #for this to produce meaningful results. 2.5 Using dplyr single verbs One of the first operations you may want to carry out when working with data frames is to subset them based on the values of particular variables. Say we want to replicate the results reported by Agan and Starr in 2017. In this earlier paper, these researchers only used data from the period prior to the introduction of Ban the Box legislation and only used data from businesses that asked about criminal records in their online applications. How can we recreate this dataset? For this kind of operation, we use the filter() function. Like all single verbs in dplyr, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame, selecting rows where the expression is TRUE. Ok, let’s filter out some information we are interested in from bandbox. If we look at the dataset, we can see that there is a variable called “crimbox” that identifies ‘applications that require information about criminal antecedents’ and there is a variable called “pre” that identifies ‘whether the application was sent before the legislation was introduced’. In this dataset, the value 1 is being used to denote positive responses. Therefore, if we want to create the 2017 dataset, we would start by selecting only data where the value in these two variables equals 1, as shown below. #We will store the results of filtering the data in a new object that I am calling aer #(short for the name of the journal in which the paper was published) aer2017&lt;- filter(banbox, crimbox == 1, pre == 1) Notice that the number of cases equals the number of cases reported by the authors in their 2017 paper. That’s cool! So far, we have replicated the results. You may have noticed in the code above that I wrote “==” instead of “=”. Logical operators in R are not written exactly the same way as in normal practice. Keep this in mind when you get error messages when running your code. Often, the source of your error may be that you are writing the logical operators the wrong way (as far as R is concerned). Look here for valid logic operators in R. Earlier, we said that real-life data may have hundreds of variables, and only a few of them may be relevant to your analysis. For this week’s analysis, we want to select only a few variables that might be highly related to ‘Discrimination in Employment’. Say you only want “crime”, “ged” (a ged is a high school equivalence diploma rather than a proper high school diploma and is sometimes seen as inferior), “empgap” (a gap year on employment), “black_f”, “response”, and “daystoresponse” from this dataset. For this kind of operation, you use the select() function. The syntax of this function is easy. First, we name the data frame object (“aer2017”), and then we list the variables. The order in which we list the variables within the select function will determine the order in which those columns appear in the new data frame we are creating. So, this is a handy function to use if you want to change the order of your columns for some reason. Since I am pretty confident I am not making a mistake, I will transform the original “aer2017” tibble rather than creating an entirely new object. aer2017 &lt;- select(aer2017, crime, ged, empgap, black_f, response, daystoresponse) If you now look at the global environment, you will see that the “aer2017” tibble has reduced in size and now only has 6 columns. If you view the data, you will see the 6 variables we selected. 2.6 Using dplyr for grouped operations So far, we have used dplyr single verbs for ungrouped operations. However, we can also use some of the functionality of dplyr to obtain answers to questions that relate to groups of cases within our data frame. Imagine that you want to know if applicants with a criminal record are less likely to receive a positive response from employers. How could you figure that one out? To answer this kind of question, we can use the group_by() function in conjunction with other dplyr functions. In particular, we are going to look at the summarise function. First, we group the observations by criminal record in a new object, “aer2017_by_antecedent”. By using as_factor() in the call to the crime variable the results will be labelled later on (even though we are not changing the crime variable in the aer2017 data frame). Keep in mind that we are using as_factor() because the column crime is a labelled vector rather than a factor or a character vector, and we do this to aid interpretation (it is easier to interpret labels than 0 and 1). aer2017_by_antecedent &lt;- group_by(aer2017, as_factor(crime)) #Then we run the summarise function to provide some useful #summaries of the groups we are using: the number of cases #and the mean of the response variable results_1 &lt;- summarise(aer2017_by_antecedent, count = n(), outcome = mean(response, na.rm = TRUE)) results_1 #auto-print the results stored in the newly created object ## # A tibble: 2 × 3 ## `as_factor(crime)` count outcome ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 No Crime 1319 0.136 ## 2 Crime 1336 0.0846 Let’s look at the code in the summarise function above. First, we are asking R to place the results in an object we are calling “results”. Then, we specify that we want to group the data in the way we specified in our group_by() function before, that is, by criminal record. Then, we pass two arguments. Each of these arguments is creating a new variable in the resulting object called “results”. The first variable we are creating is called “count” by saying this equals “n()”, we are specifying to R that this new variable simply counts the number of cases in each of the grouping categories. The second variable we are creating is called “outcome”, and to compute this variable, we are asking R to compute the mean of the variable response for each of the two groups of applicants defined in “by_antecedents” (those with records, those without). Remember that the variable response in the “aer2017” data frame was coded as a numeric variable, even though, in truth, it is categorical in nature (there was a response, or not, from the employers). It doesn’t really matter. Taking the mean of a binary variable, in this case, is mathematically equivalent to computing a proportion, as we discussed earlier. So, what we see here is that about 13.6% of applicants with no criminal record received a positive response from their employers, whereas only 8% of those with criminal records did receive such a response. Given that the assignation of a criminal record was randomised to the applicants, there’s a pretty good chance that no other confounders are influencing this outcome. And that is the beauty of randomised experiments. You may be in a position to make stronger claims about your results. 2.7 Making comparisons with numerical outcomes We have been looking at relationships so far between categorical variables, specifically between having a criminal record (yes or no), race (black or white), and receiving a positive response from employers (yes or no). Often, we may be interested in looking at the impact of a factor on a numerical outcome. The researchers measured such an outcome in the banbox object. The variable “daystoresponse” tells us how long it took the employers to provide a positive response. Let’s look at this variable: summary(banbox$daystoresponse) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 3.00 10.00 19.48 28.00 153.00 14361 The summary() function provides some useful stats for numerical variables. We obtain the minimum and maximum values, the 25th percentile, the median, the mean, the 75th percentile, and the number of missing data (NA). You can see the massive amount of missing data here. Most cases have missing data on this variable. Clearly, this is a function of, first and foremost, the fact that the number of days to receive a positive response will only be collected in cases where there was a positive response! However, even accounting for that, it is clear that this information is also missing in many cases that received a positive response. So, given all of this, we need to be very careful when interpreting this variable. However, because it is the only numeric variable here, we will use it to illustrate some handy functions. We could do as before and get results by groups. Let’s look at the impact of race on days to response: banbox_by_race &lt;- group_by(banbox, black_f) results_2 &lt;- summarise(banbox_by_race, avg_delay = mean(daystoresponse, na.rm = TRUE)) results_2 ## # A tibble: 2 × 2 ## black_f avg_delay ## &lt;fct&gt; &lt;dbl&gt; ## 1 non-Black 18.7 ## 2 Black 20.4 We can see that the average delay seems to be longer for ‘Black’ applicants than ‘White’ applicants. However, we could also try to represent these differences graphically. The problem with comparing groups on quantitative variables using numerical summaries, such as the mean, is that these comparisons hide more than they show. We want to see the full distribution, not just the mean. For this, we are going to use ggplot2, the main graphical package we will use this semester. We won’t get into the details of this package or what the code below means, but just try to run it. We will cover graphics in R in the next section. This is just a taster for it. library(ggplot2) ggplot(banbox, aes(y = daystoresponse, x = black_f)) + geom_boxplot() Watch this video and see if you can interpret the results portrayed here. What do you think? Overall, the outcomes are worse for ‘Black’ applicants, and in fact, the authors find that employers substantially increase discrimination on the basis of race after ban the box goes into effect. You can now replicate these findings with the data provided. by applying the new skills you’ve learned this week! 2.8 Summary This week, we used the following R functions: read data: ‘haven’ package read_dta() explore data dim() names() class() attribute() head() tail() table() lapply() sum() mean() transform variables into factor variables as_factor() missing variable is.na() ‘dplyr’ package filter() select() group_by() summarise() "],["data-visualisation-with-r.html", "Chapter 3 Data visualisation with R 3.1 Introduction 3.2 Anatomy of a plot 3.3 What graph should I use? 3.4 Visualising numerical variables: Histograms 3.5 Visualising numerical variables: Density plots 3.6 Visualising numerical variables: Box plots 3.7 Exploring relationships between two quantitative variables: scatterplots 3.8 Scatterplots conditioning in a third variable 3.9 Scatterplot matrix 3.10 Titles, legends, and themes in ggplot2 3.11 Plotting categorical data: bar charts 3.12 Further resources 3.13 Summary", " Chapter 3 Data visualisation with R 3.1 Introduction A picture is worth a thousand words; when presenting and interpreting data, this basic idea also applies. There has indeed been a growing shift in data analysis toward more visual approaches to both interpreting and disseminating numerical results. Part of the new data revolution involves the integration of ideas from statistical visualisation and visual design. Indeed, data visualisation is one of the most interesting areas of development in the field. Good graphics not only help researchers make their data easier for the general public to understand, but they also help us understand the data ourselves. In many ways, it is often a more intuitive way to understand patterns in our data than looking at numerical results presented in a table. Recent research has revealed that papers which have good graphics are perceived as overall clearer and more interesting, and their authors are perceived as smarter (see this presentation) The preparation for this session includes many great resources on visualising quantitative information. If you have not had time to go through them, we recommend taking some time to do so. As with other aspects of R, there are several core functions for producing graphics. However, these offer limited possibilities for building graphs. The package we will be using throughout this tutorial is ggplot2. The aim of ggplot2 is to implement the grammar of graphics. The ggplot2 package has excellent online documentation and is becoming an industry-standard in some sectors. Here, for example, you can read about how the BBC uses it as part of their News service. If you don’t already have the package installed (check that you do), you will need to install it using the install.packages() function. You will then need to load up the package library(ggplot2) The grammar of graphics upon which this package is based defines various components of a graphic. Some of the most important are: +The data: For use with ggplot2, the data must be stored as a data frame or tibble. +The geoms: They describe the objects that represent the data (e.g., points, lines, polygons, etc.). This is what gets drawn. You can have various types layered over each other in the same visualisation. +The aesthetics: They describe the visual characteristics that represent data (e.g., position, size, colour, shape, transparency). +Facets: They describe how data is split into subsets and displayed as multiple small graphs. +Stats: They describe statistical transformations that typically summarise data. Let’s take it one step at a time. 3.2 Anatomy of a plot The philosophy behind this is that all graphics are made up of layers. The package ggplot2 is based on the grammar of graphics, the idea that you can build every graph from the same few components: a data set, a set of geoms—visual marks that represent data points—and a coordinate system. Take this example (all taken from Wickham, H. (2010). A layered grammar of graphics. Journal of Computational and Graphical Statistics, 19(1), 3-28.) You have a table such as: You then want to plot this. To do so, you want to create a plot that combines the following layers: This will result in a final plot: Let’s have a look at what this looks like for a graph. Let’s have a look at some data about banning orders for different football clubs. First, you need to read the data. We keep this data on a website, and you can download it with the following code: # load readr library and import the data using read_csv() function library(readr) fbo &lt;- read_csv(&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/FootbalBanningOrders.csv&quot;) ## New names: ## Rows: 119 Columns: 4 ## ── Column specification ## ──────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (2): Club.Supported, League.of.the.Club.Supported dbl (2): ...1, Banning.Orders ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ ## Specify the column types or set `show_col_types = FALSE` to quiet this message. ## • `` -&gt; `...1` You can also find this on the Canvas page for this week’s learning materials. If you download it from there, make sure to save it in your project directory, possibly in a subfolder called “Data”. Then, you can read it from there. One thing we mentioned in the first lab was the conventions for naming objects. This also applies to the names of your variables (i.e. your column names) within your data. If you look at the fbo data frame, either with the View() function or by printing the names of the columns with the names() function, you can see this dataset violates that requirement: names(fbo) ## [1] &quot;...1&quot; &quot;Club.Supported&quot; ## [3] &quot;Banning.Orders&quot; &quot;League.of.the.Club.Supported&quot; To address this, we can use a function called clean_names(), which lives inside the janitor package. This will replace spaces with underscores and convert capital letters to lowercase. Much more tidy! library(janitor) fbo &lt;- clean_names(fbo) Now, let’s explore the question of how many banning orders are issued to clubs across different leagues. But as a first step, let’s just plot the number of banning orders for each club. Let’s build this plot: ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_point() + #geometry theme_bw() #backgroud coordinate system The first line above begins a plot by calling the ggplot() function and putting the data into it. You have to name your data frame with the data argument, and then, within the aes() command, you pass the specific variables that you want to plot. In this case, we want to see the distribution of one variable, banning orders, on the y-axis, with the club supported on the x-axis. The second line is where we add the geometry. This is where we tell R what we want the graph to be. Here we say we want it to be points by using geom_points. You can see a list of all possible geoms here. The third line is where we can tweak the display of the graph. Here, I used theme_bw(), a nice, clean theme. You can try with other themes. To get a list of themes, you can also see the resource here. If you want more variety, you can explore the package ggthemes. ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_point() + #geometry theme_dark() #backgroud coordinate system Changing the theme is not all you can do with the third element. For example, here, you can’t really read the axis labels because they’re all overlapping. One solution would be to rotate your axis labels 90 degrees, with the following code: axis.text.x = element_text(angle = 90, hjust = 1). You pass this code to the theme argument. ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) OK, what if we don’t want it to be points, but instead, we want it to be a bar graph? ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_bar(stat = &quot;identity&quot;) + #geometry theme(axis.text.x = element_text(angle = 90, hjust = 1)) #backgroud coordinate system You might notice here we pass an argument stat = \"identity\" to geo_bar() function. This is because you can have a bar graph where the height of the bar shows frequency (stat = \"count\") or where the height is taken from a variable in your data frame (stat = \"identity\"). Here, we specified a y-value (height) as the banning_orders variable. So this is cool! But what if I like both? Well, this is the beauty of the layering approach of ggplot2. You can layer on as many geoms as your little heart desires! XD ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_bar(stat = &quot;identity&quot;) + #geometry 1 geom_point() + #geometry 2 theme(axis.text.x = element_text(angle = 90, hjust = 1)) #backgroud coordinate system You can add other things too. For example, you can add the mean number of banning_orders: ggplot(data = fbo, aes(x = club_supported, y=banning_orders)) + #data geom_bar(stat = &quot;identity&quot;) + #geometry 1 geom_point() + #geometry 2 geom_hline(yintercept = mean(fbo$banning_orders)) + #mean line theme(axis.text.x = element_text(angle = 90, hjust = 1)) #backgroud coordinate system This is basically all you need to know to build a graph! So far, we have introduced a lot of code, some of which you may not fully understand. Don’t worry too much; we just wanted to give you a quick introduction to some of the possibilities. Later in the session, we will revisit some of these functions more slowly. The takeaway from this section is to understand the basic elements of the grammar of graphics. 3.3 What graph should I use? There are many points to consider when choosing which graph to use to visually represent your data. There are some best-practice guidelines, but ultimately, you need to consider what is best for your data. What do you want to show? What graph will best communicate your message? Is it a comparison between groups? Is it the frequency distribution of 1 variable? As some guidance, you can use the below cheatsheet, taken from Nathan Yau’s blog Flowingdata: However, keep in mind that this is more of a guideline aimed to nudge you in the right direction. There are many ways to visualise the same data, and sometimes, you might want to experiment with some of these and see what the differences are. There is also extensive research on how to display quantitative information effectively. The classic book is by Edward Tufte1, but since then, many other researchers have focused on approaches to displaying data. Two useful books to consider are Few (2012)2 and Cairo (2016)3. Claus Wilke is also producing a textbook that is freely available on the internet. These authors tend to produce recommendations on what to use (and not use) in certain contexts. For example, most data visualisation experts agree that you should not use 3D graphics unless there is a meaning to the third dimension. So using 3D graphics just for decoration, as in this case, is normally frowned upon. However, there are cases when including a third dimension is vital to communicating your findings. See this example. Also, often, certain chart types are vilified. For example, the pie chart is one such example. Many people (including your lecturers) really dislike pie charts; see here. If you want to display proportion, research indicates that a square pie chart is more likely to be interpreted correctly by viewers. See here. Also, in some cases, bar plots (if used to visualise quantitative variables) can hide important features of your data and might not be the most appropriate means for comparison: This has led to a Kickstarter campaign around actually banning bar plots…! So, choosing the right plot and designing its elements is, in a sense, an art that requires practice and a good understanding of the data visualisation literature. Here, we can only provide you with an introduction to some of these issues. At the end of the chapter, we will also highlight additional resources you may want to explore on your own. An important consideration is that the plot that you use depends on the data you are plotting, as well as the message you want to convey with the plot, the audience that it is intended for, and even the format in which it will be presented (a website, a printed report, a PowerPoint presentation, etc.). So, for example, returning to the difference in the number of banning orders between clubs in different leagues, what are some ways of plotting these? One suggestion is to make a histogram for each one. You can use ggplot’s facet_wrap() option to split graphs by a grouping variable. For example, to create a histogram of banning orders, you write: ggplot(data = fbo, aes(x = banning_orders)) + geom_histogram() Now, to split this by league_of_the_club_supported, you use facet_wrap() in the coordinate layer of the plot. ggplot(data = fbo, aes(x = banning_orders)) + geom_histogram() + facet_wrap(~league_of_the_club_supported) Well, you can see there’s a different distribution in each league. But is this easy to compare? Maybe another approach would make it easier? Personally, I like boxplots (we will explain them in greater detail below) for showing distribution. So let’s try: ggplot(data = fbo, aes(x = league_of_the_club_supported, y = banning_orders)) + geom_boxplot() This makes the comparison significantly easier, right? But the order is strange! Remember, we talked about factors in previous weeks? Well, the good thing about factors is that we can arrange them in their natural order. If we don’t describe an order, then R uses alphabetical order. So, let’s reorder our factor. To do that, we specify the levels in the order in which we want to be embedded within the factor. We use the code we introduced last week to do this. fbo$league_of_the_club_supported &lt;- factor(fbo$league_of_the_club_supported, levels = c(&quot;Premier League&quot;, &quot;Championship&quot;, &quot;League One&quot;, &quot;League Two&quot;, &quot;Other clubs&quot;)) And now, create the plot again! ggplot(data = fbo, aes(x = league_of_the_club_supported, y = banning_orders)) + geom_boxplot() Now, this is great! We can see that the higher the league, the more banning orders they have. Any ideas why? We’ll now go through some examples of graphs made with the ggplot2 package, spending a bit more time on each. 3.4 Visualising numerical variables: Histograms Histograms are useful ways of representing quantitative variables visually. As mentioned earlier, we will emphasise the use of the ggplot() function in this course. With ggplot(), you start with a blank canvas and keep adding specific layers. The ggplot() function can specify the dataset and the aesthetics (the visual characteristics that represent the data). To get the data we’re going to use here, load the MASS package and then load the Boston data into your environment. library(MASS) data(Boston) This package has a dataframe called Boston. This data shows housing values in the suburbs of Boston (USA). To access the codebook (how you find out what variables are), use the “?”, ?Boxton. OK, so let’s make a graph about the variable which represents the per capita crime rate by town (crim). If you want to produce a histogram with the ggplot function, you would use the following code: ggplot(Boston, aes(x = crim)) + geom_histogram() So you can see that ggplot works by allowing you to add a series of specifications (layers, annotations). In this simple plot, the ggplot function simply maps crim as the variable to be displayed (as one of the aesthetics) and the dataset. Then, you add the geom_histogram to tell R that you want this variable to be represented as a histogram. Later, we will see what other things you can add. A histogram is simply putting cases in “bins” and then creating a bar for each bin. You can think of it as a visually grouped frequency distribution. The code we have used so far has used a bin-width of size range/30, as R kindly reminded us in the output. But you can modify this parameter if you want to get a rougher or a more granular picture. In fact, you should always play around with different specifications of the bin width until you find one that tells the full story in a parsimonious way. ggplot(Boston, aes(x = crim)) + geom_histogram(binwidth = 1) We can pass arguments to the geoms, as you see. Here, we are changing the size of the bins (for further details on other arguments, you can check the help files). Using a bin-width of 1, we are essentially creating a bar for every one-unit increase in the percent rate of crime. We can still see that most towns have a very low level of crime. Let’s sum the number of towns with a value lower than 1 in the per capita crime rate. We use the sum function for this, specifying we are only interested in adding cases where the value of the variable crim is lower than 1. sum(Boston$crim &lt; 1) ## [1] 332 We can see that the large majority of towns, 332 out of 506, have a per capita crime rate below 1%. But we can also see that there are some towns that have a high concentration of crime. This is a spatial feature of crime; it tends to concentrate in particular areas and places. You can see how we can use visualisations to show the data and get a first feeling for how it may be distributed. When plotting a continuous variable, we are interested in the following features: Asymmetry: whether the distribution is skewed to the right or to the left or follows a more symmetrical shape. Outliers: Are there one or more values that seem very unlike the others? Multimodality: How many peaks does the distribution have? More than one peak may indicate that the variable is measuring different groups. Impossibilities or other anomalies: Values that are simply unrealistic given what we are measuring (e.g., somebody with an age of 1000 years). Sometimes, you may come across a distribution of data with a very high (and implausible) frequency count for a particular value. Maybe you measure age, and you have a large number of cases aged 99 (which is often a code used for missing data). Spread: This gives us an idea of the variability of our data. Often, we visualise data because we want to compare distributions. Most data analysis is about making comparisons. We are going to explore whether the distribution of crime in this dataset is different for less affluent areas. The variable medv measures in the Boston dataset the median value of owner-occupied homes. For the purposes of this illustration, I want to dichotomise4 this variable to create a group of towns with particularly low values versus all the others. For further details on how to recode variables with R, you may want to read the relevant sections in Quick R or the R Cookbook. We will learn more about recoding and transforming variables in R soon. How can we create a categorical variable based on information from a quantitative variable? Let’s look at the following code and pay attention to it, along with the explanation below. Boston$lowval[Boston$medv &lt;= 17.02] &lt;- &quot;Low value&quot; Boston$lowval[Boston$medv &gt; 17.02] &lt;- &quot;Higher value&quot; First, we tell R to create a new vector (lowval) in the Boston data frame. This vector will be assigned the character value “Low value” when the condition within the square brackets is met. That is, we are saying that whenever the value in medv is below 17.02, then the new variable lowval will equal “Low value”. I have chosen 17.02 as this is the first quartile for medv (try this code summary(Boston$medv) and find 17.02). Then we tell R that when the value is greater than 17.02, we will assign those cases to a new textual category called “Higher Value”. The variable we created was a character vector (as we can see if we run the class function). So, we are going to transform it into a factor using the as.factor function (many functions designed to work with categorical variables expect a factor as an input, not just a character vector). If we rerun the class function, we will see we changed the original variable. class(Boston$lowval) ## [1] &quot;character&quot; Boston$lowval &lt;- as.factor(Boston$lowval) class(Boston$lowval) ## [1] &quot;factor&quot; Now, we can produce the plot. We will do this using facets. Facets are another element of the grammar of graphics; we use them to define subsets of the data to be represented as multiple groups. Here, we are asking R to produce two plots defined by the two levels of the factor we just created. ggplot(Boston, aes(x = crim)) + geom_histogram(binwidth = 1) + facet_grid(lowval ~ .) Visually, this may not look great, but it begins to tell a story. We can see that there is a considerably lower proportion of towns with low crime levels in the group of towns with cheaper homes. It is a flatter, less skewed distribution. You can see how the facet_grid() expression tells R to create a histogram of the variable specified in the ggplot function, grouped by the categorical input of interest (the factor lowval). We could do a few things that may help emphasise the comparison, such as adding colour to each group. ggplot(Boston, aes(x = crim, fill = lowval)) + geom_histogram(binwidth = 1) + facet_grid(lowval ~ .) + theme(legend.position = &quot;none&quot;) The fill argument within the aes is telling R what variable to assign colours. Now, each of the levels (groups) defined by the lowval variable will have a different colour. The theme statement that we add is telling R not to place a legend in the graphic, explaining that red is a higher value and the greenish colour is a lower value. We can already see that without a label. Instead of using facets, we could overlay the histograms with a bit of transparency. Transparencies work better when projecting on screens than in printed documents, so keep this in mind when deciding whether to use them instead of facets. The code is as follows: ggplot(Boston, aes(x = crim, fill = lowval)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) In the code above, the fill argument identifies the factor variable in the dataset and groups the cases. Also, position = identity tells R to overlay the distributions, and alpha asks for the degree of transparency, so a lower value (e.g., 0.2) will be more transparent. In this case, part of the problem we have is that the skew can make it difficult to appreciate the differences. When you are dealing with skewed distributions such as this, it is sometimes convenient to use a transformation 5. We will come back to this later this semester. For now, it suffices to say that taking the logarithm of a skewed variable helps to reduce the skew and to see patterns more clearly. In order to visualise the differences here a bit better, we could ask for the logarithm of the crime per capita rate. Notice how I also add a constant of 1 to the variable crim; this is to avoid NA values in the newly created variable if the value in crim is zero (you cannot take the log of 0). ggplot(Boston, aes(x = log10(crim + 1), fill = lowval)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) The plot is a bit clearer now. It seems pretty evident that the distribution of crime is quite different between these two types of towns. 3.5 Visualising numerical variables: Density plots For smoother distributions, you can use a density plot. You should have enough data to use these, or you could end up with a lot of unwanted noise. Let’s first look at the single-density plot for all cases. Notice all we are doing is invoking a different kind of geom: ggplot(Boston, aes(x = crim)) + geom_density() In a density plot, we attempt to visualise the underlying probability distribution of the data by drawing an appropriate continuous curve. So, in a density plot, then, the area under the lines sums to 1, and the Y, vertical, axis now gives you the estimated (guessed) probability for the different values in the X, horizontal, axis. This curve is estimated from the data, and the method we use is kernel density estimation. You can read more about density plots here. In this plot, we can see that there is a high estimated probability of observing a town with near zero per capita crime rate and a low estimated probability of seeing towns with large per capita crime rates. As you can observe, it provides a smoother representation of the distribution (as compared to the histograms). You can also use this to compare the distribution of a quantitative variable across the levels in a categorical variable (factor), and, as before, it is possibly better to take the log of skewed variables such as crime: #We are mapping &quot;lowval&quot; as the variable colouring the lines ggplot(Boston, aes(x = log10(crim + 1), colour = lowval)) + geom_density() Or you could use transparencies: ggplot(Boston, aes(x = log10(crim + 1), fill = lowval)) + geom_density(alpha = .3) Did you notice the difference in the comparative histograms? By using density plots, we are rescaling to ensure the same area is used for each of the levels in our grouping variable. This makes it easier to compare two groups that have different frequencies. The areas under the curve add up to 1 for both of the groups, whereas in the histogram, the area within the bars represents the number of cases in each of the groups. If you have many more cases in one group than the other, it may be difficult to make comparisons or to clearly see the distribution for the group with fewer cases. So, this is one of the reasons why you may want to use density plots. Density plots are a good choice when you want to compare up to three groups. If you have many more groups, you may want to consider other alternatives. One such alternative is the ridgeline plot, also often called the Joy Division plot (since it was immortalised in the cover of one of their albums): They can be produced with the ggridges package. Before we dichotomise the variable medv manually, we can use more direct ways of splitting numerical variables into various categories using information already embedded in them. Say we want to split medv into deciles. We could use the mutate function in dplyr for this. library(dplyr) Boston &lt;- mutate(Boston, dec_medv = ntile(medv, 10)) The mutate function adds a new variable to our existing data frame object. We are naming this variable dec_medv because we are going to split medv into ten groups of equal size (this name is arbitrary; you may call it something else). To do this, we will use the ntile function as an argument within mutate. We will define the new dec_medv variable, explaining to R that this variable will be the result of passing the ntile function to medv. So that ntile breaks medv into 10, we pass this value as an argument to the function. So that the result of executing mutate is stored, we assign this to the Boston object. Check the results: table(Boston$dec_medv) ## ## 1 2 3 4 5 6 7 8 9 10 ## 51 51 51 51 51 51 50 50 50 50 We can now use this new variable to illustrate the use of the ggridge package. First, you will need to install this package and then load it. You will see that all this package does is extend the functionality of ggplot2 by adding a new type of geom. Here, the variable defining the groups needs to be a factor, so we will tell ggplot to treat dec_medv as a factor using as.factor. Using as.factor in this way saves us from having to create yet another variable that we are going to store as a factor. Here, we are not creating a new variable; we are just telling R to treat this numeric variable as if it were a factor. Make sure you understand this difference. library(ggridges) ggplot(Boston, aes(x = log10(crim + 1), y = as.factor(dec_medv))) + geom_density_ridges() We can see that the distribution of crime is particularly different when we focus on the three deciles with the lowest level of income. For more details on this kind of plot, you can read the vignette for this package. 3.6 Visualising numerical variables: Box plots Box plots are an interesting way of presenting the 5-number summary (the minimum value, the first quartile, the median, the third quartile, and the maximum value of a set of numbers) in a visual way. If we want to use ggplot to plot a single numerical variable, we need some convoluted code since ggplot assumes you want a boxplot to compare various groups. Therefore, we need to set some arbitrary value for the grouping variable, and we may also want to remove the x-axis tick markers and labels. For this illustration, I am going to display the distribution of the median value of property in the various towns instead of crime. ggplot(Boston, aes(x = 1, y = medv)) + geom_boxplot() + scale_x_continuous(breaks = NULL) + #removes the tick markers from the x axis theme(axis.title.x = element_blank()) Boxplots, however, really come to life when you use them to compare the distribution of a quantitative variable across various groups. Let’s look at the distribution of log(crime) across cheaper and more expensive areas: ggplot(Boston, aes(x = lowval, y=log10(crim + 1))) + geom_boxplot() With a boxplot like this, you can see straight away that the bulk of cheaper areas are very different from the bulk of more expensive areas. The first quartile of the distribution for low areas just about matches the point at which we start to see outliers for the more expensive areas. This can be even more helpful when you have various groups. Let’s try an example using the BCS0708 data frame. This is a dataset from the 2007/08 British Crime Survey. You can download it using the code below. #We create a data frame object reading the data from the webaddress.csv file BCS0708&lt;-read.csv(&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/BCS0708.csv&quot;) This dataset contains a quantitative variable that measures the level of worry about crime (tcviolent): high scores represent high levels of worry. We are going to see how the score in this variable changes according to ethnicity (ethgrp2). #A comparative boxplot of ethnicity and worry about violent crime ggplot(BCS0708, aes(x = ethgrp2, y = tcviolent)) + geom_boxplot() Nice. But it could be nicer. To start with, we could order the groups along the X-axis so that the ethnic groups are positioned according to their level of worry. Secondly, we may want to exclude information on ethnicity for the NA cases (represented by a flat line). #A nicer comparative boxplot (excluding NA and reordering the X variable) ggplot(filter(BCS0708, !is.na(ethgrp2) &amp; !is.na(tcviolent)), aes(x = reorder(ethgrp2, tcviolent, FUN = median), y = tcviolent)) + geom_boxplot() The filter function from dplyr uses a logical argument to tell R to only use the cases that do not have NA values in the two variables that we are using. The exclamation mark followed by is.na and then the name of a variable is R’s way of saying “the contrary of is NA for the specified variable”. So, in essence, we are saying to R to just look at the data that is not NA in these variables. The reorder function, on the other hand, asks R to reorder the levels of ethnicity according to the median value of worry of violent crime. Since we are using those functions within the ggplot function, this subsetting and this reordering (as with as.factor earlier) are not introducing permanent changes in your original dataset. If you prefer to reorder according to the mean, you only need to change that parameter after the FUN option (e.g., FUN = mean). 3.7 Exploring relationships between two quantitative variables: scatterplots So far, we have seen how to use histograms, density plots, and boxplots to visualise numerical variables and compare groups with respect to numerical values. Another way to say this is that you can use comparative histograms, density plots, or boxplots to assess the relationship between a numerical variable and a categorical variable (the variable that defines the groups). How do you explore the relationship between two numerical variables? When examining the relationship between two quantitative variables, nothing beats the scatterplot. This is a lovely article on the history of the scatterplot. A scatterplot plots one variable on the Y-axis and another on the X-axis. Typically, if you have a clear outcome or response variable in mind, you place it in the Y axis, and you place the explanatory variable in the X axis. This is how you produce a scatterplot with ggplot(): #A scatterplot of crime versus median value of the properties ggplot(Boston, aes(x = medv, y = crim)) + geom_point() Each point represents a case in our dataset, and the coordinates attached to it in this two-dimensional plane are given by their value in the Y (crime) and X (median value of the properties) variables. What do you look for in a scatterplot? You want to assess global and local patterns, as well as deviations. We can clearly see that at low levels of medv, there is a higher probability in this data that the level of crime is higher. Once the median value of the property hits $30,000, the crime rate will be nearly zero for all towns. So far, it’s so good and surely predictable. The first reason why we look at scatterplots is to check our hypothesis (e.g., poorer areas, more crime). However, something odd seems to be going on when the median property value is around $50,000. All of a sudden, the variability in crime increases. We seem to have some of the more expensive areas also exhibiting some fairly decent levels of crime. In fact, there is quite a break in the distribution. What’s going on? To be honest, I have no clue. However, the pattern at the higher level of property value is indeed odd; it is just too abrupt to be natural. This is the second reason why you want to plot your data before you do anything else. It helps you to detect apparent anomalies. I say this is an anomaly because the break-in pattern is quite noticeable and abrupt. It is hard to think of a natural process that would generate this sudden, radical increase in crime once the median property value reaches the 50k dollars mark. If you were analysing this for real, you would want to know what’s really driving this pattern (e.g., find out about the original data collection, the codebook, etc.): perhaps the maximum median value was capped at 50K dollars, and we are seeing this as a dramatic increase when the picture is more complex? For now, we are going to let this rest. One of the things you may notice with a scatterplot is that even with a smallish dataset such as this, with just about 500 cases, overplotting may be a problem. When you have many cases with similar (or, even worse, the same) values, it is difficult to tell them apart. Imagine there is only 1 case with a particular combination of X and Y values. What do you see? A single point. Then imagine you have 500 cases with that same combination of values for X and Y. What do you see? Still a single point. There are a variety of ways to deal with overplotting. One possibility is to add some transparency to the points: ggplot(Boston, aes(x = medv, y = crim)) + geom_point(alpha=.4) #you will have to test different values for alpha Why this is an issue may be more evident with the BCS0708 data. Compare the two plots: ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point() ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point(alpha=.2) The second plot gives us a better idea of where the observations seem to concentrate in a way that we could not see with the first. Overplotting can occur when a continuous measurement is rounded to some convenient unit. This has the effect of changing a continuous variable into a discrete ordinal variable. For example, age is measured in years, and body weight is measured in pounds or kilograms. Age is a discrete variable; it only takes integer values. That’s why you see the points lined up in parallel vertical lines. This also contributes to the overplotting in this case. One way of dealing with this particular problem is by jittering. Jittering is the act of adding random noise to data in order to prevent overplotting in statistical graphs. In ggplot, one way of doing this is by passing an argument to geom_point specifying that you want to jitter the points. This will introduce some random noise so that age looks less discrete. ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point(alpha=.2, position=&quot;jitter&quot;) #Alternatively, you could replace geom_point() with geom_jitter(), #in which case you don&#39;t need to specify the position Another alternative for solving overplotting is to bin the data into rectangles and map the density of the points to the fill of the colour of the rectangles. ggplot(BCS0708, aes(x = age, y = tcviolent)) + stat_bin2d() #The same but with nicer graphical parameters ggplot(BCS0708, aes(x = age, y = tcviolent)) + stat_bin2d(bins=50) + #by increasing the number of bins we get more granularity scale_fill_gradient(low = &quot;lightblue&quot;, high = &quot;red&quot;) #change colors What this is doing is creating boxes within the two-dimensional plane, counting the number of points within those boxes, and attaching a colour to the box in function of the density of points within each of the rectangles. When looking at scatterplots, sometimes it is useful to summarise the relationships by means of drawing lines. You could, for example, add a line representing the conditional mean. A conditional mean is simply the mean of your Y variable for each value of X. Let’s go back to the Boston dataset. We can ask R to plot a line connecting these means using geom_line() and specifying that you want the conditional means. ggplot(Boston, aes(x = medv, y = crim)) + geom_point(alpha=.4) + geom_line(stat=&#39;summary&#39;, fun.y=mean) ## Warning in geom_line(stat = &quot;summary&quot;, fun.y = mean): Ignoring unknown ## parameters: `fun.y` With only about 500 cases, there are loads of ups and downs. If you have many more cases for each level of X, the line would look less rough. You can, in any case, produce a smoother line using geom_smooth instead. We will discuss later this semester how this line is computed (although you will see the R output tells you we are using something called the “loess” method). For now, just know that it is a line that tries to estimate, to guess, the typical value for Y for each value of X. ggplot(Boston, aes(x = medv, y = crim)) + geom_point(alpha=.4) + geom_smooth(colour=&quot;red&quot;, size=1, se=FALSE) #We&#39;ll explain later this semester what the se argument does; #colour is simply asking for a red line instead of blue #(which I personally find harder to see). #I&#39;m also making the line a bit thicker with size 1. As you can see here, you produce a smoother line than with the conditional means. The line, as the scatterplot, seems to be suggesting an overall curvilinear relationship that almost flattens out once property values hit $20k. 3.8 Scatterplots conditioning in a third variable There are various ways to plot a third variable in a scatterplot. You could go 3D, and in some contexts, that may be appropriate. But more often than not, it is preferable to use only a two-dimensional plot. If you have a grouping variable, you could map it to the colour of the points as one of the aesthetics arguments. Here, we return to the Boston scatterplot but will add a third variable, which indicates whether the town is located by the river or not. # Scatterplot with two quantitative variables and a grouping variable, # We are telling R to tell &quot;chas&quot;, a numeric vector, as a factor. ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() Curiously, we can see that there are quite a few of those expensive areas with high levels of crime that seem to be located by the river. Maybe that is a particularly attractive area? As before, you can add smooth lines to capture the relationship. What happens now, though, is that ggplot will produce a line for each of the levels in the categorical variable grouping the cases: ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point(alpha=.4) + #I am making the points semi-transparent to see the lines better geom_smooth(se=FALSE, size=1) #I am making the lines thicker to see them better You can see how the relationship between crime and property values is more marked for areas not bordering the river, mostly because you have considerably fewer cheaper areas bordering the river. Notice as well the upward trend in the green line at high values of medv. As we saw, there seem to be quite a few of those, particularly more expensive areas that have high crime and seem to be by the river. We can also map a quantitative variable to the colour aesthetic. When we do that, instead of different colours for each category, we have a gradation in colour from darker to lighter depending on the value of the quantitative variable. Below, we display the relationship between crime and property values, conditioning on the status of the area (high values in lstat represent lower status). ggplot(Boston, aes(x = medv, y = crim, colour = lstat)) + geom_point() As one could predict, lstat and medv seem to be correlated. The areas with low status tend to be the areas with cheaper properties (and more crime), and the areas with higher status tend to be the areas with more expensive properties (and less crime). You could map the third variable to a different aesthetic (rather than colour). For example, you could map lstat to the size of the points. This is called a bubblechart. The problem with this, however, is that it can sometimes cause overplotting to become more acute. ggplot(Boston, aes(x = medv, y = crim, size = lstat)) + geom_point() #You may want to add alpha for some transparency here. If you have larger samples and the patterns are not clear (as we saw when looking at the relationship between age and worry of violent crime), conditioning on a third variable can produce hard-to-read scatterplots (even if you use transparencies and jittering). Let’s look at the relationship between worry of violent crime and age, conditioned on victimisation during the previous year: ggplot(BCS0708, aes(x = age, y = tcviolent, colour = bcsvictim)) + geom_point(alpha=.4, position=&quot;jitter&quot;) You can possibly notice that there are more green points on the left-hand side (since victimisation tends to be more common among youth). But it is hard to read the relationship with age. Could we try to use facets instead (hint: facet_grid)? ggplot(BCS0708, aes(x = age, y = tcviolent)) + geom_point(alpha=.4, position=&quot;jitter&quot;) + facet_grid( .~ bcsvictim) It is still hard to see anything, though perhaps you can notice the lower density of the points in the bottom right corner in the facet displaying victims of crime. In a case like this, it may be helpful to draw a smooth line. ggplot(BCS0708, aes(x = age, y = tcviolent, colour = bcsvictim)) + geom_point(alpha=.1, position=&quot;jitter&quot;) + geom_smooth(size=1.5, se=FALSE) What we see here is that, for the most part, the relationship between age and worry of violent crime looks quite flat, regardless of whether you have been a victim of crime or not, at least for most people. However, once we get to the 60s, things seem to change a bit. Those over 62 who have not been a victim of crime in the past year start to manifest a lower concern with crime as they age (in comparison with those who have been a victim of crime). 3.9 Scatterplot matrix Sometimes, you want to produce many scatterplots simultaneously to have a first peek at the relationship between the various variables in your data frame. The way to do this is by using a scatterplot matrix. There are some packages that are particularly good for this. One of them is GGally, basically an extension for ggplot2. Not to overcomplicate things, we will only use a few variables from the Boston dataset: #I create a new data frame that only #contains 4 variables included in the Boston dataset, #and I am calling this new data frame object Boston_spm Boston_spm &lt;- dplyr::select(Boston, crim, medv, lstat) Then we load GGally and run the scatterplot matrix using the ggpairs function: library(GGally) ## Warning: package &#39;GGally&#39; was built under R version 4.5.2 ggpairs(Boston_spm) The diagonal set of boxes that goes from the top left to the bottom right gives you the univariate density plot for each of the variables. So, for example, at the very top left, you have the density plot for the crim variable. If you look underneath this one, you see a scatterplot between crim and medv. In this case, crim defines the X axis and medv the Y axis, which is why it looks a bit different from the one we saw earlier. The labels at the top and the left tell you what variables are plotted in each faceted rectangle. In the top right-hand side of this matrix, you see that the rectangles say “corr” and then give you a number. These numbers are correlation coefficients, which are a metric we use to indicate the strength of a relationship between two quantitative or numeric variables. The closer to 1.000 (whether positive or negative) this value is, the stronger the relationship is. The closer to zero, the weaker the relationship. The stronger relationship here is between medv and istat (correlation coefficient value: -0.738). The fact that it is negative indicates that as the values in one increase, the values in the other tend to decrease. So, high crime values correspond to low property prices, as we saw earlier. This coefficient is a summary number of this relationship. We will come back to it later on. For now, keep in mind this metric only works well if the relationship shown in the scatterplot is well represented by a straight line. If the relationship is curvilinear, it will be a very bad metric that you should not trust. R gives you a lot of flexibility, and there are often competing packages that aim to do similar things. So, for example, for a scatterplot matrix, you could also use the spm function from the car package. library(car) #The regLine argument is used to avoid displaying #something we will cover in regression analysis. spm(Boston_spm, regLine=FALSE) This is a bit different from the one above because, rather than displaying the values of the correlation coefficient, you get another set of scatterplots with the Y and X axes rotated. You can see the matrix is symmetrical. So, the first scatterplot that you see in the top row (second column from the left) shows the relationship between medv (in the X axis) and crim (in the Y axis). This is the same relationship shown in the first scatterplot in the second row (first column); only here, crim defines the X axis and medv the Y axis. In this scatterplot, you can see, although not very well, that smoothed lines representing the relationship have been added to the plots. library(car) spm(Boston_spm, smooth=list(col.smooth=&quot;red&quot;), regLine=FALSE) You can also condition on a third variable. For example, we could condition on whether the areas bound the Charles River (variable chas). Boston_spm &lt;- dplyr::select(Boston, crim, medv, lstat, chas) spm(~crim+medv+lstat, data=Boston_spm, groups=Boston_spm$chas, by.groups=TRUE, smooth=FALSE, regLine=FALSE) Getting results, once you get the knack of it, is only half the way. The other, and more important, half is trying to make sense of the results. What are the stories this data is telling us? R cannot do that for you. For this, you need to use a better tool: your brain (scepticism, curiosity, creativity, a lifetime of knowledge) and what Kaiser Fung calls “numbersense”. 3.10 Titles, legends, and themes in ggplot2 We have introduced several graphical tools, but what if you want to customise how the produced graphic looks? Here, I am just going to give you some code to modify the titles and legends you use. To add a title to a ggplot graph, you use ggtitle(). #Notice how here we are using an additional function #to ask R to treat the variable chas, which is numeric #in our dataset, as if it were a factor (as.factor()). #You need to do this if your variable is categorical #but is encoded as numeric in your data frame. ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) If you don’t like the default background theme for ggplot, you can use a theme as discussed at the start, for example, creating a black and white background by adding theme_bw() as a layer: ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) + theme_bw() As we said earlier, ggthemes gives you additional themes you can use. For example, you can use the style inspired by The Economist magazine. library(ggthemes) ## Warning: package &#39;ggthemes&#39; was built under R version 4.5.2 ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) + theme_economist() Using labs(), you can change the text of axis labels (and the legend title), which may be handy if your variables have cryptic names. You can also manually name the labels in a legend. The values for chas are 0 and 1. This is not informative. We can change that. ggplot(Boston, aes(x = medv, y = crim, colour = as.factor(chas))) + geom_point() + ggtitle(&quot;Fig 1.Crime, Property Value and River Proximity of Boston Towns&quot;) + labs(x = &quot;Median Property Value (in US Dollars x 1000)&quot;, y = &quot;Per capita crime rate&quot;, colour = &quot;Borders the river&quot;) + scale_colour_discrete(labels = c(&quot;No&quot;, &quot;Yes&quot;)) Sometimes, you may want to present several plots together. For this, the gridExtra package is very good. You will first need to install it and then load it. You can then create several plots and combine them into a single image. #You may need to install it first with install.packages(&quot;gridExtra&quot;) library(gridExtra) #Store your plots in various objects p1 &lt;- qplot(x=crim, data=Boston) p2 &lt;- qplot(x=indus, data=Boston) p3 &lt;- qplot(x=medv, data=Boston) #Then, put them all together using grid.arrange() grid.arrange(p1, p2, p3, ncol=3) #ncol tells R we want them side by side; #if you want them one on top of the other, try ncol=1; #in this case, however, ncol=2 would possibly be the better solution. Try it! We don’t have time to get into the details of all the customisation features available for ggplot2. You can find some additional solutions in the cookbook put together by the data journalists at the BBC or in Kieran Healy’s free online book. 3.11 Plotting categorical data: bar charts You may be wondering, ‘What about categorical data’? So far, we have discussed only visualisations in which at least one of your variables is quantitative. When your variable is categorical, you can use bar plots (similar to histograms). We map the factor variable to aesthetics, then use the geom_bar() function to create a bar chart. ggplot(BCS0708, aes(x=walkday)) + geom_bar() You can see the label count on the Y-axis. This is not a variable in your data. When you run a geom_bar like this, you are invoking a hidden default call to the stat_ function. In this case, what is happening is that this function is counting the number of cases in each of the levels of walkday, and this count is what is used to map the height of each of the bars. The stat_ function, apart from counting the cases in each level, computes their relative frequency and proportion and stores this information in a temporal variable called ..prop... If we want this variable to be represented on the Y-axis, we can change the code as shown below: ggplot(BCS0708, aes(x=walkday)) + geom_bar(mapping = aes(y = ..prop..)) ## Warning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(prop)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. As Kieran Helay (2018) indicates: “The resulting plot is still not right. We no longer have a count on the y-axis, but the proportions of the bars all have a value of 1, so all the bars are the same height. We want them to sum to 1 so that we get the number of observations per” (level) “as a proportion of the total number of observations. This is a grouping issue again… we need to tell ggplot to ignore the x-categories when calculating the denominator of the proportion and use the total number of observations instead. To do so, we specify group = 1 inside the aes() call. The value of 1 is just a kind of “dummy group” that tells ggplot to use the whole dataset when establishing the denominator for its prop calculations.” ggplot(BCS0708, aes(x=walkday)) + geom_bar(mapping = aes(y = ..prop.., group = 1)) Unfortunately, the levels in this factor are ordered alphabetically, which is confusing. We can modify this by reordering the factor levels first — click here for more details. You could do this within the ggplot function (just for the visualisation), but in real life, you would want to sort out your factor levels in an appropriate manner more permanently. As discussed last week, this is the sort of thing you do during data preprocessing. And then plot. #Print the original order print(levels(BCS0708$walkday)) ## NULL #Reordering the factor levels from very safe #to very unsafe (rather than by alphabet). #Notice that I am creating a new variable; #it is often wise to do this to avoid #messing up your original data. BCS0708$walkdayR &lt;- factor(BCS0708$walkday, levels=c(&#39;very safe&#39;, &#39;fairly safe&#39;,&#39;a bit unsafe&#39;,&#39;or very unsafe&#39;)) #Plotting the variable again (and subsetting out the NA data) ggplot(subset(BCS0708, !is.na(walkdayR)), aes(x=walkdayR)) + geom_bar(mapping = aes(y = ..prop.., group = 1)) We can also map a second variable to the aesthetics; for example, let’s look at ethnicity in relation to feelings of safety. For this, we produce a stacked bar chart. bcs_bar &lt;-filter(BCS0708, !is.na(walkdayR), !is.na(ethgrp2)) ggplot(data=bcs_bar, aes(x=walkdayR, fill=ethgrp2)) + geom_bar() These stacked bar charts are not particularly helpful if you are interested in understanding the relationship between the two variables. It is hard to tell whether any group is more likely to feel safe or not. Instead, what you want is a different kind of stacked bar chart that gives you the proportion of your “explanatory variable” (ethnicity) within each of the levels of your “response variable” (here, feelings of safety). ggplot(data=bcs_bar, aes(x=walkdayR, fill=ethgrp2)) + geom_bar(position = &quot;fill&quot;) Now, we can more easily compare proportions across groups since all the bars have the same height. However, it is more difficult to see how many people there are within each level of the X variable. p &lt;- ggplot(data=bcs_bar, aes(x=walkdayR, fill=ethgrp2)) + geom_bar(position = &quot;dodge&quot;, mapping = aes(y = ..prop.., group = ethgrp2)) p Now, we have a bar chart where ethnicity values are broken down by fear-of-crime level, with proportions shown on the y-axis. Looking at the bars, you will see that they do not sum to one within each level of fear. Instead, the bars for any particular ethnicity sum to one across all the levels of fear. You can see, for example, that nearly 75% of the White respondents are in the “Very Safe” level, whereas, for example, less than 60% of the Black respondents feel “Very Safe”. Sometimes, you may want to flip the axis so that the bars are displayed horizontally. You can use the coord_flip() function for that. #First, we invoke the plot we created and stored earlier, #and then we add an additional specification with coord_flip() p + coord_flip() You can also use coord_flip() with other ggplot plots (e.g., boxplots). A particular type of bar chart is the divergent stacked bar chart, often used to visualise Likert scales. You may want to look at some of the options available for it via the HH package or sjPlot. But we won’t cover them here in detail. Keep in mind that knowing how to get R to produce a particular visualisation is only half the job. The other half is knowing when to produce a particular kind of visualisation. This blog, for example, discusses some of the problems with stacked bar charts and the exceptional circumstances in which you may want to use them. There are other tools sometimes used for visualising categorical data. Pie charts are one of them. However, as mentioned at the beginning, many people advocate strongly against using pie charts, and therefore, this is the only pie chart you will see in this course: cute pie chart What I would use instead is a waffle chart. They’re super easy to make with the “waffle” package, but I don’t think there’s time for them right now. You can have a look on your own here. 3.12 Further resources By now, you should know the path to data analysis, but wisdom will take time. The good news is that we live in a time when there are many (often free) resources to help you along the way. The time when this knowledge was just the preserve of a few is long distant. If you are a motivated, disciplined person, there is a lot you can do to further consolidate and expand your data visualisation skills without spending money. I have already recommended the excellent ggplot2 online documentation to you. Here, we want to point you to a few useful resources you can explore in the future. First MOOCS (Massive Online Open Courses). There are many useful MOOCs that provide free training in data visualisation (you can pay for a certificate, but you can also use the resources and learn for free). Second, online tutorials. One of the things you will also soon discover is that R users are keen to write “how to” guides in some of the 750 R devoted blogs or as part of Rpubs. Using Google or similar, you will often find solutions to almost any problem you will encounter using R. For example, in this blog, there are a few tutorials that you may want to look at: Cheatsheet for visualising scatterplots Cheatsheet for visualising distributions Cheatsheet for barplots Third, blogs on data visualisation. If you use Feedly or a similar blog aggregator, you should add the following blogs to your reading list. They are written by leading people in the field and are full of useful advice: + Flowing Data + Visual Business Intelligence + Data Revelations Fourth, resources for visualisations we don’t have time to cover. R is way ahead of some of the more popular data analysis programs you may be more familiar with (e.g. SPSS or Excel). There are many things you can do. For example, if you like maps, R can also be used to produce visualisations of spatial data. There are various resources for learning how to do this, and we will cover it in our Crime Mapping module in the third year. 3.13 Summary This week, we used the following R functions: import data: ‘readr’ package read_csv() explore data names() clean data: janitor package clean_names() visualise data ggplot() geom_bar() geom_point() geom_histogram() geom_density() geom_hline() geom_boxplot() facet_warp() facet_grid transform a variable into a factor variable as_factor() create a categorical variable using a numeric variable Boston$lowval[Boston$medv &lt;= 17.02] &lt;- \"Low value\" Tufte, Edward (2001) The visual display of quantitative information. 2nd Edition. Graphic Press.↩︎ Few, Stephen (2012) Show me the numbers: designing graphics and tables to enlighten. 2nd Edition. Analytics Press.↩︎ Cairo, Alberto (2016) The truthful art: data, charts, and maps for communication. New Riders.↩︎ Split into two groups.↩︎ This is an interesting blog entry on solutions when you have highly skewed data.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Statistical Inference | Modelling Criminological Data CRIM20452</title>
  <meta name="description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Statistical Inference | Modelling Criminological Data CRIM20452" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Statistical Inference | Modelling Criminological Data CRIM20452" />
  
  <meta name="twitter:description" content="This is the course material for Modelling Criminological Data CRIM20452." />
  

<meta name="author" content="" />


<meta name="date" content="2025-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="categorical-variables-and-logistic-regression.html"/>
<link rel="next" href="wrapping-up.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelling Criminological Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html"><i class="fa fa-check"></i><b>1</b> A first lesson about R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#install-r-rstudio"><i class="fa fa-check"></i><b>1.1</b> Install R &amp; RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#open-up-and-explore-rstudio"><i class="fa fa-check"></i><b>1.2</b> Open up and explore RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#customising-the-rstudio-look"><i class="fa fa-check"></i><b>1.3</b> Customising the RStudio look</a></li>
<li class="chapter" data-level="1.4" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#getting-organised-r-projects"><i class="fa fa-check"></i><b>1.4</b> Getting organised: R Projects</a></li>
<li class="chapter" data-level="1.5" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#functions-talk-to-your-computer"><i class="fa fa-check"></i><b>1.5</b> Functions: Talk to your computer</a></li>
<li class="chapter" data-level="1.6" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-packages"><i class="fa fa-check"></i><b>1.6</b> More on packages</a></li>
<li class="chapter" data-level="1.7" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#objects-creating-an-object"><i class="fa fa-check"></i><b>1.7</b> Objects: creating an object</a></li>
<li class="chapter" data-level="1.8" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#more-on-objects"><i class="fa fa-check"></i><b>1.8</b> More on objects</a></li>
<li class="chapter" data-level="1.9" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#naming-conventions-for-objects-in-r"><i class="fa fa-check"></i><b>1.9</b> Naming conventions for objects in R</a></li>
<li class="chapter" data-level="1.10" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-vectors"><i class="fa fa-check"></i><b>1.10</b> R object types: vectors</a></li>
<li class="chapter" data-level="1.11" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-object-types-data-frame"><i class="fa fa-check"></i><b>1.11</b> R object types: Data frame</a></li>
<li class="chapter" data-level="1.12" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#exploring-data"><i class="fa fa-check"></i><b>1.12</b> Exploring data</a></li>
<li class="chapter" data-level="1.13" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#r-data-types-factors"><i class="fa fa-check"></i><b>1.13</b> R data types: Factors</a></li>
<li class="chapter" data-level="1.14" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-import-data"><i class="fa fa-check"></i><b>1.14</b> How to import data</a></li>
<li class="chapter" data-level="1.15" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-use-comment"><i class="fa fa-check"></i><b>1.15</b> How to use ‘comment’</a></li>
<li class="chapter" data-level="1.16" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#how-to-quit-rstudio"><i class="fa fa-check"></i><b>1.16</b> How to Quit RStudio</a></li>
<li class="chapter" data-level="1.17" data-path="a-first-lesson-about-r.html"><a href="a-first-lesson-about-r.html#summary"><i class="fa fa-check"></i><b>1.17</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html"><i class="fa fa-check"></i><b>2</b> Getting to know your data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#causality-in-social-sciences"><i class="fa fa-check"></i><b>2.1</b> Causality in Social Sciences</a></li>
<li class="chapter" data-level="2.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-data-thanks-to-reproducibility"><i class="fa fa-check"></i><b>2.2</b> Getting data thanks to reproducibility</a></li>
<li class="chapter" data-level="2.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#getting-a-sense-of-your-data"><i class="fa fa-check"></i><b>2.3</b> Getting a sense of your data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#first-steps"><i class="fa fa-check"></i><b>2.3.1</b> First steps</a></li>
<li class="chapter" data-level="2.3.2" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#on-tibbles-and-labelled-vectors"><i class="fa fa-check"></i><b>2.3.2</b> On tibbles and labelled vectors</a></li>
<li class="chapter" data-level="2.3.3" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#turning-variables-into-factors-and-changing-the-labels"><i class="fa fa-check"></i><b>2.3.3</b> Turning variables into factors and changing the labels</a></li>
<li class="chapter" data-level="2.3.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#looking-for-missing-data-and-other-anomalies"><i class="fa fa-check"></i><b>2.3.4</b> Looking for missing data and other anomalies</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#data-wrangling-with-dplyr"><i class="fa fa-check"></i><b>2.4</b> Data wrangling with dplyr</a></li>
<li class="chapter" data-level="2.5" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-single-verbs"><i class="fa fa-check"></i><b>2.5</b> Using dplyr single verbs</a></li>
<li class="chapter" data-level="2.6" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#using-dplyr-for-grouped-operations"><i class="fa fa-check"></i><b>2.6</b> Using dplyr for grouped operations</a></li>
<li class="chapter" data-level="2.7" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#making-comparisons-with-numerical-outcomes"><i class="fa fa-check"></i><b>2.7</b> Making comparisons with numerical outcomes</a></li>
<li class="chapter" data-level="2.8" data-path="getting-to-know-your-data.html"><a href="getting-to-know-your-data.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html"><i class="fa fa-check"></i><b>3</b> Data visualisation with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#anatomy-of-a-plot"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a plot</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#what-graph-should-i-use"><i class="fa fa-check"></i><b>3.3</b> What graph should I use?</a></li>
<li class="chapter" data-level="3.4" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-histograms"><i class="fa fa-check"></i><b>3.4</b> Visualising numerical variables: Histograms</a></li>
<li class="chapter" data-level="3.5" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-density-plots"><i class="fa fa-check"></i><b>3.5</b> Visualising numerical variables: Density plots</a></li>
<li class="chapter" data-level="3.6" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#visualising-numerical-variables-box-plots"><i class="fa fa-check"></i><b>3.6</b> Visualising numerical variables: Box plots</a></li>
<li class="chapter" data-level="3.7" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#exploring-relationships-between-two-quantitative-variables-scatterplots"><i class="fa fa-check"></i><b>3.7</b> Exploring relationships between two quantitative variables: scatterplots</a></li>
<li class="chapter" data-level="3.8" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplots-conditioning-in-a-third-variable"><i class="fa fa-check"></i><b>3.8</b> Scatterplots conditioning in a third variable</a></li>
<li class="chapter" data-level="3.9" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#scatterplot-matrix"><i class="fa fa-check"></i><b>3.9</b> Scatterplot matrix</a></li>
<li class="chapter" data-level="3.10" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#titles-legends-and-themes-in-ggplot2"><i class="fa fa-check"></i><b>3.10</b> Titles, legends, and themes in ggplot2</a></li>
<li class="chapter" data-level="3.11" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#plotting-categorical-data-bar-charts"><i class="fa fa-check"></i><b>3.11</b> Plotting categorical data: bar charts</a></li>
<li class="chapter" data-level="3.12" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#further-resources"><i class="fa fa-check"></i><b>3.12</b> Further resources</a></li>
<li class="chapter" data-level="3.13" data-path="data-visualisation-with-r.html"><a href="data-visualisation-with-r.html#summary-2"><i class="fa fa-check"></i><b>3.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html"><i class="fa fa-check"></i><b>4</b> Refresher on descriptive statistics &amp; data carpentry</a>
<ul>
<li class="chapter" data-level="4.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#getting-some-data-from-eurobarometer"><i class="fa fa-check"></i><b>4.2</b> Getting some data from Eurobarometer</a></li>
<li class="chapter" data-level="4.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#thinking-about-your-data-filtering-cases"><i class="fa fa-check"></i><b>4.3</b> Thinking about your data: filtering cases</a></li>
<li class="chapter" data-level="4.4" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#selecting-variables-using-dplyrselect"><i class="fa fa-check"></i><b>4.4</b> Selecting variables: using <code>dplyr::select</code></a></li>
<li class="chapter" data-level="4.5" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#creating-summated-scales"><i class="fa fa-check"></i><b>4.5</b> Creating summated scales</a></li>
<li class="chapter" data-level="4.6" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#collapsing-categories-in-character-variables"><i class="fa fa-check"></i><b>4.6</b> Collapsing categories in character variables</a></li>
<li class="chapter" data-level="4.7" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#working-with-apparently-cryptic-variable-names-and-levels"><i class="fa fa-check"></i><b>4.7</b> Working with apparently cryptic variable names and levels</a></li>
<li class="chapter" data-level="4.8" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#recoding-factors"><i class="fa fa-check"></i><b>4.8</b> Recoding factors</a></li>
<li class="chapter" data-level="4.9" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#understanding-missing-data"><i class="fa fa-check"></i><b>4.9</b> Understanding missing data</a></li>
<li class="chapter" data-level="4.10" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#exploring-data-frames-visually"><i class="fa fa-check"></i><b>4.10</b> Exploring data frames visually</a></li>
<li class="chapter" data-level="4.11" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#a-quick-recap-on-descriptive-statistics"><i class="fa fa-check"></i><b>4.11</b> A quick recap on descriptive statistics</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#central-tendency"><i class="fa fa-check"></i><b>4.11.1</b> Central Tendency</a></li>
<li class="chapter" data-level="4.11.2" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#dispersion"><i class="fa fa-check"></i><b>4.11.2</b> Dispersion</a></li>
<li class="chapter" data-level="4.11.3" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#bivariate-analysis"><i class="fa fa-check"></i><b>4.11.3</b> Bivariate analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#further-resources-1"><i class="fa fa-check"></i><b>4.12</b> Further resources</a></li>
<li class="chapter" data-level="4.13" data-path="refresher-on-descriptive-statistics-data-carpentry.html"><a href="refresher-on-descriptive-statistics-data-carpentry.html#summary-3"><i class="fa fa-check"></i><b>4.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html"><i class="fa fa-check"></i><b>5</b> Regression I: Mean differences</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#dependent-variable-numerical-independent-variable-binary"><i class="fa fa-check"></i><b>5.2</b> Dependent variable: numerical | Independent variable: binary</a></li>
<li class="chapter" data-level="5.3" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#calculating-mean-differences-in-r"><i class="fa fa-check"></i><b>5.3</b> Calculating mean differences in <code>R</code></a></li>
<li class="chapter" data-level="5.4" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#visual-exploration"><i class="fa fa-check"></i><b>5.4</b> Visual exploration</a></li>
<li class="chapter" data-level="5.5" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#using-linear-regression-to-calculate-mean-differences"><i class="fa fa-check"></i><b>5.5</b> Using linear regression to calculate mean differences</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#linear-regression"><i class="fa fa-check"></i><b>5.5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.5.2" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#linear-regression-as-a-difference-in-means-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Linear Regression as a Difference-in-Means Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#effect-size"><i class="fa fa-check"></i><b>5.6</b> Effect size</a></li>
<li class="chapter" data-level="5.7" data-path="regression-i-mean-differences.html"><a href="regression-i-mean-differences.html#lab-exercises"><i class="fa fa-check"></i><b>5.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html"><i class="fa fa-check"></i><b>6</b> Regression II: numerical independent variables</a>
<ul>
<li class="chapter" data-level="6.1" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#motivating-regression"><i class="fa fa-check"></i><b>6.2</b> Motivating regression</a></li>
<li class="chapter" data-level="6.3" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#fitting-a-simple-regression-model"><i class="fa fa-check"></i><b>6.3</b> Fitting a simple regression model</a></li>
<li class="chapter" data-level="6.4" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#residuals-r-squared"><i class="fa fa-check"></i><b>6.4</b> Residuals: R squared</a></li>
<li class="chapter" data-level="6.5" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#is-this-the-same-as-last-week"><i class="fa fa-check"></i><b>6.5</b> Is this the same as last week?</a></li>
<li class="chapter" data-level="6.6" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#regression-assumptions"><i class="fa fa-check"></i><b>6.6</b> Regression assumptions</a></li>
<li class="chapter" data-level="6.7" data-path="regression-ii-numerical-independent-variables.html"><a href="regression-ii-numerical-independent-variables.html#lab-exercises-1"><i class="fa fa-check"></i><b>6.7</b> Lab Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><i class="fa fa-check"></i><b>7</b> Regression III: categorical independent variables and multiple regression models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#fitting-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>7.1</b> Fitting regression with categorical predictors</a></li>
<li class="chapter" data-level="7.2" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#motivating-multiple-regression"><i class="fa fa-check"></i><b>7.2</b> Motivating multiple regression</a></li>
<li class="chapter" data-level="7.3" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#fitting-and-interpreting-a-multiple-regression-model"><i class="fa fa-check"></i><b>7.3</b> Fitting and interpreting a multiple regression model</a></li>
<li class="chapter" data-level="7.4" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#rescaling-input-variables-to-assist-interpretation"><i class="fa fa-check"></i><b>7.4</b> Rescaling input variables to assist interpretation</a></li>
<li class="chapter" data-level="7.5" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#testing-conditional-hypothesis-interactions"><i class="fa fa-check"></i><b>7.5</b> Testing conditional hypothesis: interactions</a></li>
<li class="chapter" data-level="7.6" data-path="regression-iii-categorical-independent-variables-and-multiple-regression-models.html"><a href="regression-iii-categorical-independent-variables-and-multiple-regression-models.html#multicollinearity"><i class="fa fa-check"></i><b>7.6</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Categorical variables and logistic regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#cross-tabulations"><i class="fa fa-check"></i><b>8.1</b> Cross-tabulations</a></li>
<li class="chapter" data-level="8.2" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#regression-modelling-why-not-linear-regression"><i class="fa fa-check"></i><b>8.2</b> Regression modelling: why not linear regression?</a></li>
<li class="chapter" data-level="8.3" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#logistic-regression"><i class="fa fa-check"></i><b>8.3</b> Logistic regression</a></li>
<li class="chapter" data-level="8.4" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#fitting-logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Fitting logistic regression</a></li>
<li class="chapter" data-level="8.5" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#interactions"><i class="fa fa-check"></i><b>8.5</b> Interactions</a></li>
<li class="chapter" data-level="8.6" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#assessing-model-fit-confusion-matrix"><i class="fa fa-check"></i><b>8.6</b> Assessing model fit: confusion matrix</a></li>
<li class="chapter" data-level="8.7" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#lab-exercises-2"><i class="fa fa-check"></i><b>8.7</b> Lab Exercises</a></li>
<li class="chapter" data-level="8.8" data-path="categorical-variables-and-logistic-regression.html"><a href="categorical-variables-and-logistic-regression.html#further-resources-2"><i class="fa fa-check"></i><b>8.8</b> Further resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference.html"><a href="statistical-inference.html#introduction-4"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference.html"><a href="statistical-inference.html#brief-theoretical-overview"><i class="fa fa-check"></i><b>9.2</b> Brief theoretical overview</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#sampling-distribution-central-limit-theorem"><i class="fa fa-check"></i>Sampling distribution &amp; Central limit theorem</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-confidence-intervals"><i class="fa fa-check"></i>Statistical inference: confidence intervals</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-hypothesis-testing"><i class="fa fa-check"></i>Statistical inference: hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-practice"><i class="fa fa-check"></i><b>9.3</b> Statistical inference in practice</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#means-and-proportions"><i class="fa fa-check"></i>Means and proportions</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-linear-regression-models"><i class="fa fa-check"></i>Statistical inference in linear regression models</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-multiple-linear-regression-models"><i class="fa fa-check"></i>Statistical inference in multiple linear regression models</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#presenting-results-from-regression-analysis"><i class="fa fa-check"></i>Presenting results from regression analysis</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-in-logistic-regression-models"><i class="fa fa-check"></i>Statistical inference in logistic regression models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="wrapping-up.html"><a href="wrapping-up.html"><i class="fa fa-check"></i><b>10</b> Wrapping up</a>
<ul>
<li class="chapter" data-level="10.1" data-path="wrapping-up.html"><a href="wrapping-up.html#tips-for-the-assignment"><i class="fa fa-check"></i><b>10.1</b> Tips for the assignment</a></li>
<li class="chapter" data-level="10.2" data-path="wrapping-up.html"><a href="wrapping-up.html#summarising-your-variables"><i class="fa fa-check"></i><b>10.2</b> Summarising your variables</a></li>
<li class="chapter" data-level="10.3" data-path="wrapping-up.html"><a href="wrapping-up.html#some-final-words"><i class="fa fa-check"></i><b>10.3</b> Some final words</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>11</b> Appendix</a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="appendix.html"><a href="appendix.html#expected-frequencies"><i class="fa fa-check"></i><b>11.0.1</b> Expected frequencies</a></li>
<li class="chapter" data-level="11.1" data-path="appendix.html"><a href="appendix.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.1</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="appendix.html"><a href="appendix.html#fitting-logistic-regression-alternative"><i class="fa fa-check"></i><b>11.1.1</b> Fitting logistic regression: alternative</a></li>
<li class="chapter" data-level="11.1.2" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-deviance-and-pseudo-r-squared"><i class="fa fa-check"></i><b>11.1.2</b> Assessing model fit: deviance and pseudo r squared</a></li>
<li class="chapter" data-level="11.1.3" data-path="appendix.html"><a href="appendix.html#assessing-model-fit-roc-curves"><i class="fa fa-check"></i><b>11.1.3</b> Assessing model fit: ROC curves</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelling Criminological Data CRIM20452</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-inference" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Statistical Inference<a href="statistical-inference.html#statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-4" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Introduction<a href="statistical-inference.html#introduction-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When conducting data analysis, we <em>usually</em> need to analyse data from a <strong>sample</strong>. Very rarely do we have access do <strong>population</strong>-level data. In most cases, we only have access to sample data. For example, the <em>Crime Survey for England and Wales</em> does not survey every single resident of the UK. Instead, it surveys a limited number of residents who are supposed to <em>represent</em> the entire population. This limited number of residents is a <em>sample</em>. Yet, we are only interested in information from this sample as long as they can be considered a <em>representative sample</em>—i.e., as long as we can <strong>generalise information from a sample to a population</strong>.</p>
<p>Think of it this way: suppose you are cooking tomato sauce in a large pan and want to check if you have added enough salt. You do not need to eat the entire pan to find out; instead, you take a small spoonful and, based on its taste, draw conclusions about the whole sauce. This approach only works if that small spoonful is truly representative of the entire pan. In this analogy, the spoonful is a sample, while the full pan of sauce represents the population. Similarly, when we analyse data from sources like the Crime Survey for England and Wales, we rely on survey respondents (the sample) to make inferences about the entire population of England and Wales. The sample is valuable only to the extent that it accurately represents the population, allowing us to draw meaningful conclusions about the population.</p>
<p>How do we do that? This is where <strong>statistical inference</strong> comes in. Statistical inference allows us to use information from a sample to draw conclusions about a population—just like deciding whether the tomato sauce needs more salt based on a small spoonful or estimating the proportion of UK residents who have experienced violent crime using data from the CSEW. Whenever we work with a sample but aim to make conclusions about a population, we rely on statistical inference. These tools help us account for and model the <em>uncertainty</em> in our estimates. Even when we have full population data—such as all police-recorded crimes in a given area—statistical inference remains essential for handling uncertainty and making robust conclusions.</p>
<p>Up to now we have introduced a series of concepts and tools that are helpful to describe sample data. Today we are going to revisit those concepts and tools and learn how use them from a statistical inference point of view. In particular, we will approach this topic from the <strong>frequentist</strong> tradition.</p>
<p>It is important you understand this is not the only way of doing data analysis. There is an alternative approach, <strong>Bayesian statistics</strong>, which is very important and increasingly popular. Unfortunately, we do not have the time this semester to also cover Bayesian statistics. Typically, you would learn about this approach in more advanced courses.</p>
<p>We have two main subsections today: <strong>Brief theoretical overview</strong> and <strong>Statistical inference in practice</strong>. Unlike in previous sessions, the focus of the first subsection will be less applied and a bit more theoretical. However, it is important you pay attention, since understanding the foundations of statistical inference is essential for a proper understanding of everything else we have discussed in this course. The code we cover in the first subsection is a bit trickier but won’t be instrumental for your assignment, so don’t worry too much if you don’t fully understand it—it’s just there to help you understand some of the key concepts. In the second subsection, we will revisit some of the analyses we have done in previous weeks—calculating means and proportions, estimating linear regression models, multiple linear regression models, and logistic regression models—but now looking at how these analytic strategies allow us to make conclusions about the population.</p>
</div>
<div id="brief-theoretical-overview" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Brief theoretical overview<a href="statistical-inference.html#brief-theoretical-overview" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s begin with some conceptual clarification. In <em>most</em> data analysis exercises, we distinguish between a <em>sample</em> and a <em>population.</em> The key assumption here is that we want to draw conclusions about the population, but we do not have data on every observation within it. For instance, if we wanted to predict how prospective voters would cast their ballots in an upcoming election, surveying every voter would be impractical—it would be as costly and time-consuming as conducting the actual election. Instead, we rely on samples: a subset of units from the population. We select this subset of units with the goal of representing the whole population.</p>
<style>
details {
  margin-bottom: 1em; /* Adds space below each details block */
}
</style>
<p><strong>Your turn!</strong> In the statements below, identify the targeted population and the analytic sample:</p>
<ul>
<li><strong>Blood Test</strong> – A doctor takes a small vial of blood from a patient to check their overall health</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>All the blood in the patient’s body</i></li>
<li><b>Sample</b>: <i>The small vial of blood taken for testing</i></li>
</ul>
</details>
<ul>
<li><strong>Taste-Testing Coffee</strong> – A barista takes a small sip from a freshly brewed coffee to determine if it has the right balance of flavours.</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>The entire cup of coffee</i></li>
<li><b>Sample</b>: <i>The sip taken for testing</i></li>
</ul>
</details>
<ul>
<li><strong>Election Polling</strong> – A research firm surveys 2,000 registered voters to estimate the percentage of people who support a particular candidate.</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>All registered voters</i></li>
<li><b>Sample</b>: <i>The 2,000 voters surveyed</i></li>
</ul>
</details>
<ul>
<li><strong>Student Satisfaction Survey</strong> – A university wants to assess student satisfaction, so it surveys 10% of its undergraduate students.</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>All undergraduate students at the university</i></li>
<li><b>Sample</b>: <i>The 10% of students surveyed</i></li>
</ul>
</details>
<ul>
<li><strong>Crime Victimisation Survey</strong> – The Crime Survey for England and Wales (CSEW) interviews 35,000 individuals to estimate the national rate of victimisation.</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>All residents of England and Wales</i></li>
<li><b>Sample</b>: <i>The 35,000 survey respondents</i></li>
</ul>
</details>
<ul>
<li><strong>Drug Effectiveness Trial</strong> – A pharmaceutical company tests a new drug on 1,000 patients to evaluate its effectiveness before public approval.</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>All patients who could potentially use the drug</i><br />
</li>
<li><b>Sample</b>: <i>The 1,000 patients in the clinical trial</i></li>
</ul>
</details>
<ul>
<li><strong>Air Quality Monitoring</strong> – Researchers place air quality sensors in 100 locations across a city to estimate overall pollution levels.</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>All air in the city</i></li>
<li><b>Sample</b>: <i>The air quality measurements from the 100 locations</i></li>
</ul>
</details>
<ul>
<li><strong>Police Use of Force Analysis</strong> – A study reviews body-worn camera footage from 100 police stops to assess patterns of force used by officers.</li>
</ul>
<details>
<summary>
<i>Reveal answer!</i>
</summary>
<ul>
<li><b>Population</b>: <i>All police stops in the jurisdiction</i><br />
</li>
<li><b>Sample</b>: <i>The 100 police stops analysed</i></li>
</ul>
</details>
<p>The big question, then, is: how confident can we be that our sample truly represents the population? After all, the small vial of blood taken for testing may not accurately reflect the overall health of the patient, the 1,000 patients in a clinical trial could could happen to be unusually sensitive to the drug, or the 100 police stops analysed might all involve exceptionally aggressive encounters. In other words, the sample could be significantly different from the population.</p>
<p>We address this challenge with one of the most powerful tools in statistics: <strong>randomisation</strong>. As long as we have a <em>random sample</em> (aka ‘probability samples’), we can use statistical inference tools to assess how confident we can be when drawing conclusions about the population. A truly random sample means that every unit in the population has the same probability of being selected into the sample. For example, in the cases above: every student at the university should have an equal chance of being chosen for the student satisfaction survey; every resident of England and Wales should be equally likely to be interviewed by the CSEW team; and every police stop in the jurisdiction should have the same probability of being included in the sample of 100 stops analysed.</p>
<p>Not all analytic samples are random samples. Sometimes, researchers must rely on <em>convenience samples</em> (or <em>non-probability samples</em>), where selection into the sample is not random. This typically happens when reaching the target population is challenging or when participating in the research is costly. For example, student satisfaction surveys often rely on convenience samples: everyone is invited to participate, but only a subset of students actually respond—i.e., a subset self-selects into the sample. Surveys involving police officers also tend to use convenience samples, as only a subset of officers may be willing to participate in research. When working with convenience samples, it is crucial to remember that they are susceptible to <strong>selection bias</strong> (or <em>sampling bias</em>). More importantly, statistical inference tools cannot be used with data drawn from convenience samples, as they do not provide a representative snapshot of the population.</p>
<p><img src="imgs/sampling.png" style="width:50.0%" /></p>
<p>From now on, let’s assume we are working with probability samples—samples that have been collected through random selection of units.</p>
<div id="sampling-distribution-central-limit-theorem" class="section level3 unnumbered hasAnchor">
<h3>Sampling distribution &amp; Central limit theorem<a href="statistical-inference.html#sampling-distribution-central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Okay, this makes sense. If we have a random sample, we can apply statistical inference tools that allow us to assess how confident we can be at drawing conclusions about the population. Now, let’s have a closer look at statistical inference.</p>
<p>The first thing to note is that different random samples can be obtained from the same population. Let’s use the CSEW as an example. From the population of people aged 16 and over living in England and Wales (more than 45 million people, according to the <a href="https://www.gov.uk/government/statistics/population-estimates-for-the-uk-england-wales-scotland-and-northern-ireland-mid-2021?">ONS</a>), we want to draw a sample of 35,000 individuals. How many different combinations of 35,000 individuals could we select from a population of over 45 million? The answer is… <em>a lot</em>. There are so many possible combinations that we can say there are <em>virtually infinite possibilities</em>. In fact, there are infinite possible random samples that could be drawn from this population. So, what ensures that our sample—or any of these infinite possible samples—is actually representative of the population?</p>
<p>In reality, even though there are infinite possible samples, only a single random simple will be drawn from the population—e.g., there is only a single annual CSEW sample.</p>
<p>Let’s take a closer look at this idea. To help us better understand some of the theoretical concepts we’re introducing this week, we will generate some fictitious data. We will use <code>R</code> to create this data, which we will treat as the population data. This is artificial because, in reality, we never have access to complete population-level data. From this fictitious population, we will then draw a number of random samples. Again, this is artificial because, in practice, we only have a single sample that is supposed to represent the population.</p>
<p><strong>Important</strong>: <em>this analysis is not something we typically perform in regular data analysis workflows, nor is it something you will be required to do for your final coursework. It’s simply designed to help us better understand some concepts related to statistical inference</em>.</p>
<p>Let’s start by creating this fictional dataset with 100,000 observations and treat it as our fake population. We can do this using the <code>rnbinom()</code> function, which will randomly generate a variable following a skewed distribution (the technical term is a <em>negative binomial</em> distribution, which is a discrete probability distribution). This is useful because we often work with severely skewed data in criminology. Because the <code>rnbinom()</code> function <em>randomly</em> generates a dataset, we would get a slightly different distribution every time we run the code; so, to ensure that we all get exactly the same results, we can use the <code>set.seed()</code> function—if we all set the same “seed”, we’ll get the same randomly generated distribution.</p>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="statistical-inference.html#cb537-1" tabindex="-1"></a><span class="co"># set seed to ensure we all have the same results</span></span>
<span id="cb537-2"><a href="statistical-inference.html#cb537-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb537-3"><a href="statistical-inference.html#cb537-3" tabindex="-1"></a></span>
<span id="cb537-4"><a href="statistical-inference.html#cb537-4" tabindex="-1"></a><span class="co"># generate a fictional dataset with 100,000 observations</span></span>
<span id="cb537-5"><a href="statistical-inference.html#cb537-5" tabindex="-1"></a>fake_population <span class="ot">&lt;-</span> <span class="fu">rnbinom</span>(<span class="dv">100000</span>, <span class="at">mu =</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="fl">0.15</span>) </span></code></pre></div>
<p>The code above creates data that follow a negative binomial distribution, essentially a highly skewed distribution. Don’t worry too much about the other parameters we are using as arguments at this stage, but if curious look at <code>?rnbinom</code>.</p>
<p>We can also summarise and get the standard deviation for this object using the <code>summary()</code> and <code>sd()</code> functions, respectively:</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="statistical-inference.html#cb538-1" tabindex="-1"></a><span class="co"># describe the fictitious distribution</span></span>
<span id="cb538-2"><a href="statistical-inference.html#cb538-2" tabindex="-1"></a><span class="fu">summary</span>(fake_population)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.9873  1.0000 65.0000</code></pre>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="statistical-inference.html#cb540-1" tabindex="-1"></a><span class="co"># calculate the standard deviation of the fictitious distribution</span></span>
<span id="cb540-2"><a href="statistical-inference.html#cb540-2" tabindex="-1"></a><span class="fu">sd</span>(fake_population)</span></code></pre></div>
<pre><code>## [1] 2.744007</code></pre>
<p>So, we can see that this generated variable, which we are treating as the population, ranges from 0 to 65, with a standard deviation of 2.74. The average score is 0.99. Let’s remember this number.</p>
<p>We can also see what the shape of the distribution looks like:</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="statistical-inference.html#cb542-1" tabindex="-1"></a><span class="co"># load the ggplot2 package</span></span>
<span id="cb542-2"><a href="statistical-inference.html#cb542-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb542-3"><a href="statistical-inference.html#cb542-3" tabindex="-1"></a></span>
<span id="cb542-4"><a href="statistical-inference.html#cb542-4" tabindex="-1"></a><span class="co"># plot the histogram of the generated distribution</span></span>
<span id="cb542-5"><a href="statistical-inference.html#cb542-5" tabindex="-1"></a><span class="fu">qplot</span>(fake_population)</span></code></pre></div>
<p><img src="09_statistical_inference_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>To make our lives a bit easier, let’s pretend this object contains population-level information on whether individuals have previously engaged in offending behaviour in the previous year. So, this is a population of 100,000 individuals, and the distribution indicates how many offences each of these individuals have committed. As expected, the majority of the population has committed 0 crimes, whereas some individuals have engaged in one or more criminal behaviours. Let’s see how many offenders we have in this fake population.</p>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="statistical-inference.html#cb543-1" tabindex="-1"></a><span class="co"># check how many observations are greater than 0</span></span>
<span id="cb543-2"><a href="statistical-inference.html#cb543-2" tabindex="-1"></a><span class="fu">sum</span>(fake_population <span class="sc">&gt;</span> <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 26078</code></pre>
<p>26078 individuals have committed at least one crime in the previous year. That implies 26.08% of the population. Let’s remember this number too.</p>
<p>We are now going to put this variable in a <code>data.frame</code> object. We are also going to create a new categorical variable identifying whether someone offended over the past year (e.g., anybody with a count of crime higher than 0). Let’s start by creating a new data frame (<code>fake_population</code>) with the skewed variable we created rebaptised as <code>crimecount</code>.</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="statistical-inference.html#cb545-1" tabindex="-1"></a><span class="co"># create new data.frame object</span></span>
<span id="cb545-2"><a href="statistical-inference.html#cb545-2" tabindex="-1"></a>fake_population <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">crimecount =</span> fake_population)</span></code></pre></div>
<p>Then let’s define all values above 0 as “Yes” in a variable identifying offenders and everybody else as “No”. We use the <code>mutate</code> and the <code>case_when()</code> functions from the <code>dplyr</code> package for this.</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="statistical-inference.html#cb546-1" tabindex="-1"></a><span class="co"># load the dplyr package</span></span>
<span id="cb546-2"><a href="statistical-inference.html#cb546-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb546-3"><a href="statistical-inference.html#cb546-3" tabindex="-1"></a></span>
<span id="cb546-4"><a href="statistical-inference.html#cb546-4" tabindex="-1"></a><span class="co"># create new variable called &quot;offender&quot; that indicates whether each</span></span>
<span id="cb546-5"><a href="statistical-inference.html#cb546-5" tabindex="-1"></a><span class="co"># respondent has committed at least one crime in the previous year</span></span>
<span id="cb546-6"><a href="statistical-inference.html#cb546-6" tabindex="-1"></a>fake_population <span class="ot">&lt;-</span> <span class="fu">mutate</span>(fake_population, <span class="at">offender =</span> <span class="fu">case_when</span>(crimecount <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">~</span> <span class="st">&quot;Yes&quot;</span>,</span>
<span id="cb546-7"><a href="statistical-inference.html#cb546-7" tabindex="-1"></a>                                                                crimecount <span class="sc">==</span> <span class="dv">0</span> <span class="sc">~</span> <span class="st">&quot;No&quot;</span>))</span>
<span id="cb546-8"><a href="statistical-inference.html#cb546-8" tabindex="-1"></a></span>
<span id="cb546-9"><a href="statistical-inference.html#cb546-9" tabindex="-1"></a><span class="co"># Let&#39;s check the results</span></span>
<span id="cb546-10"><a href="statistical-inference.html#cb546-10" tabindex="-1"></a><span class="fu">table</span>(fake_population<span class="sc">$</span>offender)</span></code></pre></div>
<pre><code>## 
##    No   Yes 
## 73922 26078</code></pre>
<p>Good. So, we have a population in which 26.08% of the individuals have committed at least one offence in the previous year and the average number of offences committed by each individual is 0.99.</p>
<p>However, as mentioned before, we <em>never</em> have access to population-level data. We only have access to samples—and we use sample-level information to draw conclusions about the population. So, let’s <code>R</code> to generate random samples from the <code>fake_population</code> population. We can then compare our sample means with the population mean! They should be similar, right?</p>
<p>Let’s start creating a probability sample of 1000 observations randomly obtained from the <code>fake_population</code> dataset. Note that we need the <code>sample()</code> function from the <code>mosaic</code> package.</p>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="statistical-inference.html#cb548-1" tabindex="-1"></a><span class="co"># install the &#39;mosaic&#39; package if you haven&#39;t yet</span></span>
<span id="cb548-2"><a href="statistical-inference.html#cb548-2" tabindex="-1"></a><span class="do">## install.packages(&quot;mosaic&quot;)</span></span>
<span id="cb548-3"><a href="statistical-inference.html#cb548-3" tabindex="-1"></a></span>
<span id="cb548-4"><a href="statistical-inference.html#cb548-4" tabindex="-1"></a><span class="co"># load the &#39;mosaic&#39; package</span></span>
<span id="cb548-5"><a href="statistical-inference.html#cb548-5" tabindex="-1"></a><span class="fu">library</span>(mosaic)</span>
<span id="cb548-6"><a href="statistical-inference.html#cb548-6" tabindex="-1"></a></span>
<span id="cb548-7"><a href="statistical-inference.html#cb548-7" tabindex="-1"></a><span class="co"># obtain a random sample with 1000 observations from the fake_population object</span></span>
<span id="cb548-8"><a href="statistical-inference.html#cb548-8" tabindex="-1"></a>fake_sample_1 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span></code></pre></div>
<p>We have a new dataset called <code>fake_sample_1</code> in our environment now. It has 1000 observations and 3 columns. Do note the <code>fake_sample_1</code> object has an additional column named <code>orig.id</code>, which indicates the row number of the sampled observation in the original <code>fake_population</code> object.</p>
<p>For starters, let’s check the average number of offences <em>sampled</em> individuals have committed. We expect it to be similar to the <em>population mean</em> of 0.99.</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="statistical-inference.html#cb549-1" tabindex="-1"></a><span class="co"># check the sample mean of the variable &#39;crimecount&#39; in the fake_sample_1 dataset</span></span>
<span id="cb549-2"><a href="statistical-inference.html#cb549-2" tabindex="-1"></a><span class="fu">mean</span>(fake_sample_1<span class="sc">$</span>crimecount)</span></code></pre></div>
<pre><code>## [1] 0.99</code></pre>
<p>It is almost identical to the population mean! That’s great. Now let’s check the proportion of individuals who have committed at least one offence. We expect it to be similar to the <em>population proportion</em> of 26.08%.</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="statistical-inference.html#cb551-1" tabindex="-1"></a><span class="co"># check the sample proportion of the variable &#39;offender&#39; in the fake_sample_1 dataset</span></span>
<span id="cb551-2"><a href="statistical-inference.html#cb551-2" tabindex="-1"></a><span class="fu">table</span>(fake_sample_1<span class="sc">$</span>offender) <span class="sc">%&gt;%</span> <span class="fu">prop.table</span>()</span></code></pre></div>
<pre><code>## 
##    No   Yes 
## 0.734 0.266</code></pre>
<p>Again, <em>very</em> similar to the population proportion!</p>
<p>But hang on. That was just one possible random sample drawn from the population. As we mentioned earlier, there are infinite possible random sample that could be drawn from the same population. Is it possible that a different sample would have different estimates?</p>
<p>Let’s generate more samples and check their estimates.</p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="statistical-inference.html#cb553-1" tabindex="-1"></a><span class="co"># obtain several random samples with 1000 observations each from the fake_population object</span></span>
<span id="cb553-2"><a href="statistical-inference.html#cb553-2" tabindex="-1"></a>fake_sample_2 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-3"><a href="statistical-inference.html#cb553-3" tabindex="-1"></a>fake_sample_3 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-4"><a href="statistical-inference.html#cb553-4" tabindex="-1"></a>fake_sample_4 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-5"><a href="statistical-inference.html#cb553-5" tabindex="-1"></a>fake_sample_5 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-6"><a href="statistical-inference.html#cb553-6" tabindex="-1"></a>fake_sample_6 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-7"><a href="statistical-inference.html#cb553-7" tabindex="-1"></a>fake_sample_7 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-8"><a href="statistical-inference.html#cb553-8" tabindex="-1"></a>fake_sample_8 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-9"><a href="statistical-inference.html#cb553-9" tabindex="-1"></a>fake_sample_9 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-10"><a href="statistical-inference.html#cb553-10" tabindex="-1"></a>fake_sample_10 <span class="ot">&lt;-</span> mosaic<span class="sc">::</span><span class="fu">sample</span>(fake_population, <span class="dv">1000</span>)</span>
<span id="cb553-11"><a href="statistical-inference.html#cb553-11" tabindex="-1"></a></span>
<span id="cb553-12"><a href="statistical-inference.html#cb553-12" tabindex="-1"></a><span class="co"># print the sample means of the variable &#39;crimecounts&#39; across all ten samples</span></span>
<span id="cb553-13"><a href="statistical-inference.html#cb553-13" tabindex="-1"></a>purrr<span class="sc">::</span><span class="fu">map_dbl</span>(<span class="fu">list</span>(fake_sample_1, fake_sample_2, fake_sample_3, fake_sample_4, fake_sample_5,</span>
<span id="cb553-14"><a href="statistical-inference.html#cb553-14" tabindex="-1"></a>                    fake_sample_6, fake_sample_7, fake_sample_8, fake_sample_9, fake_sample_10),</span>
<span id="cb553-15"><a href="statistical-inference.html#cb553-15" tabindex="-1"></a>               <span class="sc">~</span> <span class="fu">mean</span>(.x<span class="sc">$</span>crimecount, <span class="at">na.rm =</span> T))</span></code></pre></div>
<pre><code>##  [1] 0.990 0.983 1.025 0.879 1.103 0.996 1.012 0.991 0.994 0.893</code></pre>
<p>As we can see, not all sample means are exactly the same. Some of them are <em>very</em> similar to the population mean, like <code>fake_sample_1</code> and <code>fake_sample_2</code>, whereas other are slightly more different, such as <code>fake_sample_3</code> and <code>fake_sample_4</code>.</p>
<p>Your exact results may differ from those shown here, but you can surely see the point. We may have a problem with using sample means as a guess for the population mean. Your guesses (or estimates, or <em>guesstimates</em>) will vary. How much of a problem is this? <a href="http://www.nytimes.com/2014/05/02/upshot/how-not-to-be-misled-by-the-jobs-report.html?_r=0">This excellent piece and demonstration</a> by New York Times reporters illustrate the problem well. We are going to learn that something called the <strong>central limit theorem</strong> is of great assistance here.</p>
<p>Say we decided to draw more samples from the same population. Instead of just 10 samples, we decide to draw hundreds, or maybe thousands of samples. In reality, there are infinite possible random samples, but let’s say we were able to draw all of them—every single random sample (with the same sample size, in our case, <span class="math inline">\(n = 1000\)</span>) from the population. The distribution of all possible random samples obtained from the population is called <strong>sampling distribution</strong>. It is <em>the distribution of the statistic for all possible samples from the same population of a given size</em>.</p>
<p>This is a very abstract concept… but let’s try to make sense of it. Let’s generate <strong>50,000</strong> samples (of size <span class="math inline">\(n=1000\)</span>, as before) instead of just 10 as before. Then, for each one of those 50,000 samples, we compute the sample mean indicating the average number of offences individuals have committed. This strategy is called a <em>sampling experiment</em>.</p>
<p>This is computationally intensive (after all, we’re obtaining 50,000 random samples from the fake population!)—so may take a bit. Wait until you see the object appear in your environment.</p>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="statistical-inference.html#cb555-1" tabindex="-1"></a><span class="co"># generate 50,000 random samples from the fake_population object and</span></span>
<span id="cb555-2"><a href="statistical-inference.html#cb555-2" tabindex="-1"></a><span class="co"># compute the mean of &#39;crimecount&#39;</span></span>
<span id="cb555-3"><a href="statistical-inference.html#cb555-3" tabindex="-1"></a>samples_50k_crimecount <span class="ot">&lt;-</span> <span class="fu">do</span>(<span class="dv">50000</span>) <span class="sc">*</span> <span class="fu">with</span>(<span class="fu">sample</span>(fake_population, <span class="dv">1000</span>), <span class="fu">mean</span>(crimecount))</span></code></pre></div>
<p>So now we have 50,000 sample means from samples of size 1000 taken from our fake population. Let’s visually explore the distribution of the sample means.</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="statistical-inference.html#cb556-1" tabindex="-1"></a><span class="co"># Plot the distribution of 50,000 sample means</span></span>
<span id="cb556-2"><a href="statistical-inference.html#cb556-2" tabindex="-1"></a><span class="fu">qplot</span>(samples_50k_crimecount<span class="sc">$</span>with, <span class="at">xlab =</span> <span class="st">&quot;Distribution of means from samples of size 1000&quot;</span>)</span></code></pre></div>
<p><img src="09_statistical_inference_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The sampling distribution follows a normal distribution! When you (1) take many random samples from a variable; (2) compute the means for each of these samples; and (3) plot the means of each of these samples, you end up with something (i.e., the sampling distribution) that is also normally distributed. And what is the mean of the sampling distribution—i.e., if we analyse the means obtained from each one of the 50,000 samples we generated and computed their overall mean, what would it be? Let’s check it:</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="statistical-inference.html#cb557-1" tabindex="-1"></a><span class="co"># computes mean of the sample means</span></span>
<span id="cb557-2"><a href="statistical-inference.html#cb557-2" tabindex="-1"></a><span class="fu">mean</span>(samples_50k_crimecount<span class="sc">$</span>with) </span></code></pre></div>
<pre><code>## [1] 0.9872288</code></pre>
<p>Wait a minute, we remember this number! This exactly the same as the population mean, isn’t it?</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="statistical-inference.html#cb559-1" tabindex="-1"></a><span class="co"># computes population mean of the variable &quot;crimecount&quot;</span></span>
<span id="cb559-2"><a href="statistical-inference.html#cb559-2" tabindex="-1"></a><span class="fu">mean</span>(fake_population<span class="sc">$</span>crimecount)</span></code></pre></div>
<pre><code>## [1] 0.98726</code></pre>
<p>Wow! What we have observed is part of something called the <strong>central limit theorem</strong>, a concept from probability theory. One of the first things that the central limit theorem tells us is that <strong>the mean of the sampling distribution of the means should equal the mean of the population</strong>.</p>
<p>The beauty of it is that, if the sampling distribution (i) is a normal distribution and (ii) has a mean that is the same as the population mean, we can use properties about the normal distribution to help us link data obtained from a random sample to the underlying population—allowing us to make statistical inference! What’s even more impressive is that the original variable does not need to be normally distributed. Our variable today, <code>crimecount</code>, follows a highly skewed distribution (in the population and in each random sample); and yet, the sampling distribution of the several means of <code>crimecount</code> obtained from several samples is normally distributed.</p>
<p>But which properties of the normal distribution are we talking about? The <a href="https://en.wikipedia.org/wiki/Normal_distribution">true normal distribution</a> has known properties that we can now use. For example, we know that 68% of the entire distribution is within 1 standard deviation from the mean, we know that 95.5% of the distribution is 2 standard deviations away from the mean, and that 99.7% of the distribution is 3 standard deviations away from the mean.</p>
<p>Why is that relevant? Because, in practice, we only have one random sample aimed to represent the population. We don’t know if any given simple is similar or not to the population, as some samples might more or less close to the population parameters. The variation across all samples—i.e., the standard deviation of the sampling distribution—we call that <strong>standard error</strong>. Because the sampling distribution follows a normal distribution, we do know that 68% of all possible samples are within one standard error from the population mean, that 95.5% of all possible samples are within 2 standard errors from the population mean, and that 99.7% of all possible samples are within 3 standard deviations from the population mean.</p>
<p><img src="imgs/normpdf.png" style="width:80.0%" /></p>
<p>In other words, it is extremely rare that a random sample would yield sample means (i.e., estimates) that are too different from the population mean. How rare? That depends on our <strong>confidence level</strong>. In social sciences, we typically choose <strong>95% confidence level</strong> as the standard threshold for evidence. Looking at the theoretical standard normal distribution, we know that about 95% of the cases fall within 2 standard deviations on either side of the mean. Therefore, we know then that about 95% of the sample means (95.46% to be more precise) will fall within two standard errors of the population mean (i.e., the mean of the sampling distribution). So, we can say that the margin of error, the largest likely estimation error, equals 2 standard errors. More accurately, the margin of error equals 1.96 standard errors (1.96 corresponds to 95% whereas the value 2 corresponds to 95.46%).</p>
<p>If you want to further consolidate some of these concepts you may find <a href="https://www.khanacademy.org/math/probability/statistics-inferential/sampling_distribution/v/central-limit-theorem">these videos</a> on sampling distributions from Khan Academy useful.</p>
</div>
<div id="statistical-inference-confidence-intervals" class="section level3 unnumbered hasAnchor">
<h3>Statistical inference: confidence intervals<a href="statistical-inference.html#statistical-inference-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let’s take a closer look at this idea of <em>standard errors</em>. If the standard error is the standard deviation of the sampling distribution, in the case of our sampling experiment, we can simply compute the standard deviation of the <code>samples_50k_crimcount</code> object. (<em>Note</em>: usually, the standard error needs to be guessed/estimated as well, as in practice we only have data from a single sample).</p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="statistical-inference.html#cb561-1" tabindex="-1"></a><span class="co"># check the standard deviation of the sampling procedure</span></span>
<span id="cb561-2"><a href="statistical-inference.html#cb561-2" tabindex="-1"></a><span class="fu">sd</span>(samples_50k_crimecount<span class="sc">$</span>with)</span></code></pre></div>
<pre><code>## [1] 0.08585455</code></pre>
<p>The standard error was 0.09. If we multiply 0.09 by 1.96, we obtain `r 0.17. This means that 95% of the samples in this sampling distribution will have an error that won’t be bigger than that. They will only at most differ from the mean of the sampling distribution by (plus and minus) 0.17. However, 5% of the samples will have a margin of error bigger than that (in absolute terms).</p>
<p>The wonderful thing is that we can use the margin of error to provide information about the degree to which our sample estimate may vary from the population mean. We can use it to give a measure of the uncertainty in our estimation. How?</p>
<blockquote>
<p>“We rely on this obvious relation: If M (our sample mean) is likely to be close to μ (the population mean)—as the last page or two has illustrated—then μ is likely to be close to M. As simple as that. The simulation shows us that, for most samples, M (the sample mean) falls pretty close to μ (the population mean), in fact within margin of error of μ. Now, we have only a single M and don’t know μ. But, unless we’ve been unlucky, our M has fallen within the margin of error of μ, and so, if we mark out an interval extending the margin of error on either side of our, most likely we’ve included μ. Indeed, and that interval is the confidence interval (CI)” (Cumming, 2012: 69).</p>
</blockquote>
<p>If we have a large random sample, the 95% confidence interval will then be:</p>
<ul>
<li>Upper limit: sample mean <span class="math inline">\(+ 1.96*\)</span> standard error<br />
</li>
<li>Lower limit: sample mean <span class="math inline">\(- 1.96*\)</span> standard error</li>
</ul>
<p>This will be clearer with a complete example. Let’s have a look at <code>fake_sample_4</code>, a random sample of 1000 observations drawn from the <code>fake_population</code>. (<em>Feel free to try this with other samples!</em>).</p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="statistical-inference.html#cb563-1" tabindex="-1"></a><span class="co"># check the sample mean of &#39;crimecount&#39; in fake_sample_4</span></span>
<span id="cb563-2"><a href="statistical-inference.html#cb563-2" tabindex="-1"></a><span class="fu">mean</span>(fake_sample_4<span class="sc">$</span>crimecount)</span></code></pre></div>
<pre><code>## [1] 0.879</code></pre>
<p>The mean of <code>crimecount</code> in <code>fake_sample_4</code> is 0.88. In this case, we know that this is slightly different but still fairly close to the population mean of NA, but in real-life scenarios we only see the sample mean—never the population mean.</p>
<p>So, we can estimate the confidence interval. Because we want the 95% confidence interval, we define the interval as a range <span class="math inline">\(1.96 *\)</span> the standard error below and above our sample mean of 0.88. Given that the standard error in this case is 0.17, the lower bound of the 95% confidence interval is simply 0.88 <span class="math inline">\(- 1.96 *\)</span> 0.09 <span class="math inline">\(=\)</span> 0.55, whereas the upper bound of the 95% confidence interval is 0.88 <span class="math inline">\(+ 1.96 *\)</span> 0.09 <span class="math inline">\(=\)</span> 1.21.</p>
<p>What does this mean? Simply that <strong>we are 95% confident that the population mean is within the <span class="math inline">\([\)</span> 0.55; 1.21 <span class="math inline">\(]\)</span> range</strong>. More specifically, 95% of all confidence intervals (i.e., from all possible samples, the sampling distribution) will contain the population mean and 5% will miss it.</p>
<p>If we know the population mean, then we can see whether a sample confidence interval overlaps with the population mean. But in real life we run samples precisely because we don’t know the population parameters. So, unfortunately, when you do a sample you can never be sure whether your estimated confidence interval is one of the rare unlucky ones that miss it.</p>
<p>The truth is we will never know whether our confidence interval captures the population parameter or not, <em>we can only say that under certain assumptions if we had repeated the procedure many times it will include it 95% of the time</em>. It is important not to get confused about it. We cannot never know in real life applications if our confidence interval actually covers the population mean. This is one of the reasons why in statistics when making inferences we cannot provide definitive answers,<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> there is always an element of uncertainty that is part of any scientific endeavour—and which goes along way towards explaining why we need to replicate studies, to see if theirs findings hold.</p>
<p>It is generally considered better practice to report your confidence intervals than your point estimates. Why?</p>
<ul>
<li>First, because you are being explicit about the fact you are just guessing. Point estimates such as the sample mean create a false impression of precision.<br />
</li>
<li>But beware the CI can also be misleading! 95% of the times you may get the true parameter, but that’s not the same than to say that 95% of the time your mean will lie between your upper and lower boundaries for your confidence interval. <a href="http://link.springer.com/article/10.3758%2Fs13423-013-0572-3">This is a common interpretative mistake made by researchers and, even, teachers</a>. Do not make it yourself!!! <strong>A confidence interval can only be used to evaluate the procedure not a specific estimated interval</strong>.</li>
</ul>
<p>So to reiterate:</p>
<ul>
<li><strong>INCORRECT INTERPRETATION</strong>: “There is a 95% chance that the average number of offences is between 0.55 and 1.21”. This is a very common misconception! It seems very close to true, but it isn’t because the population mean value is fixed. So, it is either in the interval or not and you can’t possibly know whether that is the case. This is subtle but important.</li>
<li>What is correct? <strong>95% of the time, when we calculate a confidence interval in this way, the true mean will be between the two values. 5% of the time, it will not.</strong> Because the true mean (population mean) is an unknown value, we don’t know if we are in the 5% or the 95%. BUT 95% is pretty good. This is the only correct interpretation of our confidence interval, so do not take it any other as valid.</li>
<li>Is is not terrible to say something like “We are 95% confident that the average number of offences is between 0.55 and 1.21.” This is a common shorthand for the idea that the calculations “work” 95% of the time.</li>
<li>Remember that we can’t have a 100% confidence interval. By definition, the population mean is not known. If we could calculate it exactly we would! But that would mean that we need a census of our population with is often not possible or feasible.</li>
<li>Finally, because if the range of values that you give me for your CI is smaller or bigger I will know that your estimate is more or less precise respectively. That is, <strong>with the CI you are giving me a measure of your uncertainty.</strong> The bigger the CI the more uncertain we are about the true population parameter.</li>
</ul>
</div>
<div id="statistical-inference-hypothesis-testing" class="section level3 unnumbered hasAnchor">
<h3>Statistical inference: hypothesis testing<a href="statistical-inference.html#statistical-inference-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Remember when started talking about null hypotheses in Week 5? We said that, in bivariate (or multivariate) analysis, we always start with a null hypothesis that assumes no relationship between the independent and the dependent variables. Then we conduct data analysis to collect evidence for or against the null hypothesis. We didn’t mention it at the time, but that was actually actually about statistical inference!</p>
<p>The null hypothesis is always a statement about the population. The statement is that there is no relationship between the independent and the dependent variables <em>in the population</em>. Then we conduct a statistical <em>test</em>, using statistical inference tools, that providence evidence indicating whether we can or cannot reject the null hypothesis—thus using data from a sample to make conclusions about the population.</p>
<p>More specifically, the null hypothesis always states that the estimated statistic measuring the association (e.g., the mean difference, the slope coefficient, etc.) is 0 in the population. The alternative hypothesis then states that that statistic is not 0 in the population. The evidence produced by statistical tests that allow us to reject, or not, the null hypothesis is based on statistical inference:</p>
<ul>
<li><p><strong>Confidence intervals</strong>. For every statistic we estimate, we will always estimate a corresponding standard error. We can then use that standard error to compute confidence intervals—e.g., the 95% confidence interval, as we did above: the lower bound of the 95% confidence interval would be given by the estimated statistic <span class="math inline">\(- 1.96 *\)</span> the standard error, whereas the upper bound of the 95% confidence interval would be given by the estimated statistic <span class="math inline">\(+ 1.96 *\)</span> the standard error. As long as the 95% confidence interval does not cross 0, we can reject the null hypothesis with 95% confidence and conclude that the estimated association is statistically significant (i.e., likely to be true in the population).</p></li>
<li><p><strong>p-values</strong>. So in essence we are after a probability, specifically a conditional probability (i.e, <em>the probability of our data if the null hypothesis were true</em>). We are trying to quantify the probability of seeing data like the one we have observed if we take as given that the null hypothesis is true (and the value “should be” zero). We call this probability the <strong>p-value</strong>. You may have heard this term before. All it means, it bears repeating, is <strong>the probability of observing our data if the null hypothesis were true</strong>.</p></li>
</ul>
<blockquote>
<p>“<strong>When the p value is high</strong>, then we can conclude that we have not seeing anything unusual. Events that have a high probability of happening happen often. The data are thus consistent with the model from the null hypothesis, and we have no reason to reject the null hypothesis. But we realize many other similar hypotheses could also account for the data we’ve seen, so we haven’t proven that the null hypothesis is true. The most we can say is that it doesn’t appear to be false. Formally, we fail to reject the null hypothesis. That’s a pretty weak conclusion, but it’s all we’re entitled to. <strong>When the p value is low enough</strong>, it says that it’s very unlikely we’d observed data like these if our null hypothesis were true. We started with a model. Now the model tells us that the data are unlikely to have happened. The model and the data are at odds with each other, so we have to make a choice. Either the null hypothesis is correct and we’ve just seen something remarkable, or the null hypothesis is wrong…” (De Veaux et al. 2012: 480)</p>
</blockquote>
<p>When is a p value high and when is low? Typically, we use criteria similar to those we use when constructing confidence intervals: we would consider a p value low enough if 95% of the time the observed data was considered to be inconsistent with the model proposed by our null hypothesis. So, we look for p values that are smaller or bigger than 0.05. However, there is nothing sacrosanct about 95% and you could have good reasons to depart from this criterion (read page 123 to 128 of Weisburd and Britt, 2010 for further details). You will see that statistics books refer to the threshold we use to define a p value as high or low as our <strong>level of statistical significance</strong> (also often referred to as the <strong>alpha level</strong>). In our example here we will use an alpha level of 0.05. That is, we will reject the null hypothesis <em>only if our p level is below that threshold</em>.</p>
<p>Both approaches are complementary and provide consistent information. If a given p-value is below 0.05, then the 95% confidence interval does not cross zero. If you are using, say, the 1% significance level (in which case, the null hypothesis can be rejected if <span class="math inline">\(p&lt;0.01\)</span>), then you should estimate the 99% confidence interval; and so on.</p>
<p>The ultimate goal of statistical tests for hypothesis testing is to obtain a p value: the probability that the observed statistic (or a more extreme value) occurs if the null model is correct. If the p value is small enough (smaller than our alpha level: such as 0.05) then we will <strong>“reject the null hypothesis”</strong>. If it is not, we will <strong>“fail to reject the null hypothesis”</strong>. The language is important.</p>
<p>Whatever you decide, the <em>American Psychological Association Statistical Committee</em> recommends that it is always a good idea to report the p value as an indication of the strength of the evidence. That is, not only report the finding to be significant or not, also report your actual p value.</p>
<p>One final word. P values have attracted a lot of debate over the years. They are often misunderstood and people often read too much into them. They have also been used in a too simplistic way as a yardstic to decide what research findings are “worthy”. It is important to know what they are and how they work. It is particularly important not to overinterpret them either. The term statistical <strong>significance</strong> is particularly misleading because in common usage we think of something significant as important. But in our context is basically equivalent to say that we have observed in our sample/study may not be noise. That’s it. You will find all sorts of reactions to p values. Some people think we should ban them and use alternative approaches to data analysis (like Bayesian statistics). Others think that we should use more stringent thresholds (like p values below .01 or .001). Yet most scientists still rely on them, so it is important that you learn what they are, their limitations, and how to interpret them in a correct manner.</p>
</div>
</div>
<div id="statistical-inference-in-practice" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Statistical inference in practice<a href="statistical-inference.html#statistical-inference-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You may have spotted a big problem in what came before. How did we compute the confidence interval? We multiplied 1.96 times the standard error. Remember: the standard error is the standard deviation of the sampling distribution of the mean. And… well, unless you are willing to repeat a survey thousands and thousands of times with the same population, you won’t know what the standard error is! The population mean is unknown and we want to estimate it. <em>But the standard error that we need for constructing our confidence interval is also unknown!</em></p>
<p>If you are working with proportions there is an easier way to estimate the standard error only with sample data (for details see the required reading). But with means this is not possible. There is, however, a solution. You can use <em>the standard deviation of your sample</em> as an estimate for the standard error. You would also need to make some adjustments to the formula for the confidence interval (you divide the sample standard deviation by the square root of the sample size). You don’t need to worry to much about the mathematics of it! In practice here we will rely on <code>R</code> to apply these formulas and compute the confidence intervals.</p>
<p>It is important, though, that you know that this approach works reasonably well when applying the Normal probability model to large samples. But with small samples using the sample standard deviation as an estimate of the standard error (so that we can compute the confidence interval) is problematic. The sample standard deviation also varies from sample to sample and this extra variation in smaller samples will mess up the computation of the margin of errors. William Gosset’s suggested we needed to use a different probability distribution for this cases, the <em>t Student distribution</em>.</p>
<p>You can learn more about this distribution and the work of Gosset in the suggested reading. The t-Student distribution and the normal distribution are almost indistinguishable for large samples. In essence that means you will still multiply by 1.96. But with smaller sample sizes that value will be different if you use a normal distribution or a t student distribution. Refer to the recommended textbooks for further clarification.</p>
<p>It is fairly straightforward to get the confidence intervals using <code>R</code>. <em>In order to use the t-Student distribution we need to assume the data were randomly sampled and that the population distribution is unimodal and symmetric.</em></p>
<p>In the following subsections, we are going to rely on data from the Crime Survey for England and Wales 2007-08 and revisit everything we studied this semester applying statistical inference tools. Let’s start loading the data:</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="statistical-inference.html#cb565-1" tabindex="-1"></a><span class="co"># load readr package</span></span>
<span id="cb565-2"><a href="statistical-inference.html#cb565-2" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb565-3"><a href="statistical-inference.html#cb565-3" tabindex="-1"></a></span>
<span id="cb565-4"><a href="statistical-inference.html#cb565-4" tabindex="-1"></a><span class="co"># import the data using read_csv() function</span></span>
<span id="cb565-5"><a href="statistical-inference.html#cb565-5" tabindex="-1"></a>csew_0708 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/uom-resquant/modelling_book/refs/heads/master/datasets/BCS0708.csv&quot;</span>)</span></code></pre></div>
<p>As we know, this is a representative sample of people aged 16 or older living in England and Wales. While we are analysing data from this sample, what we really care about is the population. Therefore, we can use sample-level data to make population-level conclusions.</p>
<div id="means-and-proportions" class="section level3 unnumbered hasAnchor">
<h3>Means and proportions<a href="statistical-inference.html#means-and-proportions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we did a few weeks ago, let’s start looking at the variable <code>tcviolent</code>, which measures fear of violent crime.</p>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb566-1"><a href="statistical-inference.html#cb566-1" tabindex="-1"></a><span class="co"># describe the &#39;tcviolent&#39; variable</span></span>
<span id="cb566-2"><a href="statistical-inference.html#cb566-2" tabindex="-1"></a><span class="fu">summary</span>(csew_0708<span class="sc">$</span>tcviolent)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##  -2.350  -0.672  -0.117   0.046   0.540   3.805    3242</code></pre>
<p>This is an index indicating how fearful of violent crime each individual is. The lowest score in the scale is -2.35, indicating no fear of violent crime whatsoever, and the highest score is 3.81, reflecting very high levels of fear of violent of crime.</p>
<p>The average score is 0.05. Now, this is when things get interesting… 0.05 is the <em>sample mean</em>. We don’t know what the <em>population mean</em> is! Yet, that’s our target, that’s the number we actually want to know. Because we don’t know, we can only guess. And the best, most educated guess we can have is the sample mean. Therefore, we treat the sample mean as our estimate (i.e., best guess) of the population mean.</p>
<p>Still, we could be wrong. The CSEW sample is just one out of infinite possible samples drawn from the population aged 16 or older living in England and Wales. While we know that <em>most</em> samples will do a good job in estimating the population mean, <em>some</em> will be relatively off. Therefore, rather than simply using our point estimate—i.e., rather than simply saying that the population mean is exactly 0.05, we can model the uncertainty and compute a confidence interval around our estimate. Following standard conventions, we can compute a 95% confidence interval.</p>
<p>To do that, we simply use the <code>t.test()</code> function in <code>R</code>.</p>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="statistical-inference.html#cb568-1" tabindex="-1"></a><span class="co"># compute a 95% confidence interval around the mean of &#39;tcviolent&#39;</span></span>
<span id="cb568-2"><a href="statistical-inference.html#cb568-2" tabindex="-1"></a><span class="fu">t.test</span>(csew_0708<span class="sc">$</span>tcviolent)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  csew_0708$tcviolent
## t = 4.1679, df = 8433, p-value = 3.104e-05
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.02414414 0.06702001
## sample estimates:
##  mean of x 
## 0.04558207</code></pre>
<p>Let’s have a closer look at this output. To make our lives easier, let’s analyse the output bottom-up.</p>
<ul>
<li>First, under “sample estimates”, we have the expression “mean of x” indicating sample mean of the variable <code>tcviolent</code>, which is 0.05, as we already knew.</li>
<li>Above that, under “95 percent confidence interval”, we have two numbers. They the lower bound and the upper bound of the 95% confidence interval, respectively. Therefore, the 95% confidence interval is <span class="math inline">\([\)</span> 0.02 ; 0.07 <span class="math inline">\(]\)</span>.</li>
</ul>
<p>So, we can say that we are 95% confident that the population mean is somewhere within the <span class="math inline">\([\)</span> 0.02 ; 0.07 <span class="math inline">\(]\)</span> range. More accurately, we can conclude that, if we were able to draw several other random samples from the population aged 16 or older in England and Wales, 95% of those samples will yield sample estimates between 0.02 and 0.07.</p>
<p>From a hypothesis testing point of view, the null hypothesis here is that the population mean is 0. Given that the confidence interval does not cross 0, we can reject this null hypothesis with a 95% confidence level. This is not a meaningful null hypothesis.</p>
<p>We can also look at the first row of the output.</p>
<ul>
<li><span class="math inline">\(t = 4.1679\)</span> indicates the <em>t statistic</em>. This is obtained by dividing the sample mean by the standard error (which was estimated but not reported). In a nutshell, it measures how distant (in standard errors) the sample mean is from the population mean under the null hypothesis. Give that the null hypothesis states that the population mean is 0, this implies that the sample mean (i.e., 0.05) is <span class="math inline">\(t = 4.1679\)</span> standard errors distant from 0.</li>
<li><span class="math inline">\(df = 8433\)</span> indicates the number of degrees of freedom. In this case, it is given by the total number of observations utilised in the test minus 1.</li>
<li>p-value = 3.104e-05 indicates the estimated p-value. This implies that <span class="math inline">\(p=0.00003104\)</span>. The expression “e-05” uses scientific notation to indicate the number of decimal points before the first character. In this case, there are five decimal points before ‘3’, therefore <span class="math inline">\(p=0.00003104\)</span>.</li>
</ul>
<p>This set of information is about all about hypothesis testing. The <em>t-statistic</em>, degrees of freedom, and <em>p-value</em> are all directly related. We use this information as evidence on whether we can, or cannot, reject the null hypothesis. In this case, because the p-value is lower than our significance level (i.e., <span class="math inline">\(0.00003104 &lt; 0.05\)</span>), we reject the null hypothesis.</p>
<p>If you want a different confidence interval, say 99%, you can pass an additional argument to change the default in the <code>t.test()</code> function. In this case, when saying <code>.99</code> we are saying that if we were to repeat the procedure 99% of the confidence intervals would cover the population parameter.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="statistical-inference.html#cb570-1" tabindex="-1"></a><span class="co"># compute a 99% confidence interval around the mean of &#39;tcviolent&#39;</span></span>
<span id="cb570-2"><a href="statistical-inference.html#cb570-2" tabindex="-1"></a><span class="fu">t.test</span>(csew_0708<span class="sc">$</span>tcviolent, <span class="at">conf.level =</span> .<span class="dv">99</span>)</span></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  csew_0708$tcviolent
## t = 4.1679, df = 8433, p-value = 3.104e-05
## alternative hypothesis: true mean is not equal to 0
## 99 percent confidence interval:
##  0.01740552 0.07375863
## sample estimates:
##  mean of x 
## 0.04558207</code></pre>
<p>The 99% confidence level is <span class="math inline">\([\)</span> 0.02 ; 0.07 <span class="math inline">\(]\)</span>.</p>
<p>Now, suppose that instead of analysing scores of fear of violent crime, you want to analyse the proportion of individuals who have been victimised.</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="statistical-inference.html#cb572-1" tabindex="-1"></a><span class="co"># sample proportion of victimisation</span></span>
<span id="cb572-2"><a href="statistical-inference.html#cb572-2" tabindex="-1"></a><span class="fu">table</span>(csew_0708<span class="sc">$</span>bcsvictim) <span class="sc">%&gt;%</span> prop.table</span></code></pre></div>
<pre><code>## 
## not a victim of crime       victim of crime 
##             0.7980473             0.2019527</code></pre>
<p>The sample percentage of individuals who have been victimised is 20.2%. Yet, this is only true for our sample. The number we actually want to find out is the <em>population</em> percentage of individuals who have been victimised. Since this is a population parameters, it is unknown. We can only guess. Our best guess is to use the sample statistic. Therefore, we <em>estimate</em> that the population percentage of individuals who have been victimised is 20.2%.</p>
<p>Still, we could be wrong. We don’t know what the population parameter is, and our estimate could be very far off from the population proportion. Therefore, we compute a 95% confidence interval. We can use the <code>prop.test()</code> function in these cases:</p>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="statistical-inference.html#cb574-1" tabindex="-1"></a><span class="co"># We want to estimate the proportion of respondents who have been victimised, </span></span>
<span id="cb574-2"><a href="statistical-inference.html#cb574-2" tabindex="-1"></a><span class="co"># which is why we specifically ask for those classified as &quot;victim of crime&quot;</span></span>
<span id="cb574-3"><a href="statistical-inference.html#cb574-3" tabindex="-1"></a><span class="fu">prop.test</span>(csew_0708<span class="sc">$</span>bcsvictim <span class="sc">==</span> <span class="st">&quot;victim of crime&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  1-sample proportions test with continuity correction
## 
## data:  ==  [with success = TRUE]csew_0708$bcsvictim  [with success = TRUE]victim of crime  [with success = TRUE]
## X-squared = 4147.6, df = 1, p-value &lt; 2.2e-16
## alternative hypothesis: true p is not equal to 0.5
## 95 percent confidence interval:
##  0.1947272 0.2093754
## sample estimates:
##         p 
## 0.2019527</code></pre>
<p>Very similar output as before. We have the sample estimate of 0.202 and a 95% confidence interval ranging from 0.195 to 0.209.</p>
<p>So, we can say that we are 95% confident that the population proportion is somewhere within the <span class="math inline">\([\)</span> 0.195; 0.209 <span class="math inline">\(]\)</span> range.</p>
<p>The hypothesis testing part works exactly the same way. We already knew we could reject the null hypothesis (under a 5% significance level) that the population proportion is 0 because the 95% confidence interval does not cross 0. We can also look at the first row of the output, which works in a similar way—instead of a <em>t-statistic</em>, we have a <em>X-squared</em> statistic, given that we now have a proportion, and a p-value really, really low: <span class="math inline">\(p = 0.000000000000000022\)</span>. This hypothesis testing is not substantially relevant either.</p>
<p>As before, You can also specify a different confidence level:</p>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb576-1"><a href="statistical-inference.html#cb576-1" tabindex="-1"></a><span class="co"># Computing a 99% confidence interval for the sample proportion</span></span>
<span id="cb576-2"><a href="statistical-inference.html#cb576-2" tabindex="-1"></a><span class="fu">prop.test</span>(csew_0708<span class="sc">$</span>bcsvictim <span class="sc">==</span> <span class="st">&quot;victim of crime&quot;</span>, <span class="at">conf.level =</span> .<span class="dv">99</span>)</span></code></pre></div>
<pre><code>## 
##  1-sample proportions test with continuity correction
## 
## data:  ==  [with success = TRUE]csew_0708$bcsvictim  [with success = TRUE]victim of crime  [with success = TRUE]
## X-squared = 4147.6, df = 1, p-value &lt; 2.2e-16
## alternative hypothesis: true p is not equal to 0.5
## 99 percent confidence interval:
##  0.1925112 0.2117343
## sample estimates:
##         p 
## 0.2019527</code></pre>
</div>
<div id="statistical-inference-in-linear-regression-models" class="section level3 unnumbered hasAnchor">
<h3>Statistical inference in linear regression models<a href="statistical-inference.html#statistical-inference-in-linear-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, following the logic of the course outline, say we are interested in a bivariate relationship; say, for instance, we want to know whether individuals who have previously been victimised have higher scores of fear of violent crime than individuals who have not been victimised. In this case, fear of violent crime (<code>tcviolent</code>), a numerical variable, is our dependent variable, and crime victimisation, <code>bcsvictim</code>, a binary variable, is our independent variable. We know that to test this association we just need to estimate the mean difference—i.e., the difference between the average score of fear of violent crime among those who have been victims and the average scores of fear of violent crime among those who have not been victims. We also know that the linear regression framework calculates the mean difference for us.</p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="statistical-inference.html#cb578-1" tabindex="-1"></a><span class="co"># use regression to calculate the mean difference in &#39;tcviolent&#39; across groups of &#39;bcsvictim&#39;</span></span>
<span id="cb578-2"><a href="statistical-inference.html#cb578-2" tabindex="-1"></a>linear_regression_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(tcviolent <span class="sc">~</span> bcsvictim, <span class="at">data =</span> csew_0708)</span>
<span id="cb578-3"><a href="statistical-inference.html#cb578-3" tabindex="-1"></a></span>
<span id="cb578-4"><a href="statistical-inference.html#cb578-4" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb578-5"><a href="statistical-inference.html#cb578-5" tabindex="-1"></a>linear_regression_1</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = tcviolent ~ bcsvictim, data = csew_0708)
## 
## Coefficients:
##              (Intercept)  bcsvictimvictim of crime  
##                  0.02483                   0.09812</code></pre>
<p>Based on the regression output above, we know that the average score of fear of crime among respondents who have <em>not</em> been victims of crime is <span class="math inline">\(\widehat{\alpha} = 0.025\)</span>, and we know that respondents who <em>have</em> been previously victimised have an average score of fear of crime that is <span class="math inline">\(\widehat{\beta} = 0.098\)</span> higher than non-victims.</p>
<p>That’s great, but those numbers are also only true in our sample! What we want to know is the mean difference <em>in the population</em>. Because it is a population parameter, we do not know what it is. We can only guess. As ever, our best, most educated guess is the sample mean difference. But we can also handle the uncertainty around our point estimate and compute a confidence interval and conduct hypothesis testing. The null hypothesis, in this case, is that the mean difference <em>in the population</em> is 0.</p>
<p>So, how can test the null hypothesis? The linear regression framework already does it for us! Yes, that’s great. Every single parameter in the regression model—i.e., <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> have their own statistical test conducted. Aside from the point estimate (i.e., the estimates of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>), the <code>lm()</code> function will automatically estimate a standard error, which can be used to compute a t-statistic and the p-value. From that, we can also derive confidence intervals.</p>
<p>And the good news is, we don’t need to do anything new. We just use the <code>summary()</code> function when printing the regression model.</p>
<div class="sourceCode" id="cb580"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb580-1"><a href="statistical-inference.html#cb580-1" tabindex="-1"></a><span class="co"># print results from the regression model with all information</span></span>
<span id="cb580-2"><a href="statistical-inference.html#cb580-2" tabindex="-1"></a><span class="fu">summary</span>(linear_regression_1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = tcviolent ~ bcsvictim, data = csew_0708)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4732 -0.7326 -0.1665  0.4900  3.7806 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)               0.02483    0.01231   2.017 0.043696 *  
## bcsvictimvictim of crime  0.09812    0.02676   3.667 0.000247 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.004 on 8432 degrees of freedom
##   (3242 observations deleted due to missingness)
## Multiple R-squared:  0.001592,   Adjusted R-squared:  0.001474 
## F-statistic: 13.45 on 1 and 8432 DF,  p-value: 0.0002472</code></pre>
<p>Let’s focus on the “Coefficients:” part of the output. For each estimated parameter—in this case, the intercept <span class="math inline">\(\widehat{\alpha}\)</span> (“<em>(Intercept)</em>”) and the slope coefficient <span class="math inline">\(\widehat{\beta}\)</span> (“<em>bcsvictim of crime</em>”)—there are now four columns available. The column “Estimate” just provides the point estimate, exactly as before. The three new columns—Std. Error, t value, and Pr(&gt;|t|)—yield information that allow us to make statistical inference.</p>
<p>We can see, for example, that the estimated mean difference is 0.098. We already knew that. But now we also know the standard error of this estimate, which is 0.027. If we simply divide the Estimate by its standard error (<span class="math inline">\(\frac{0.098}{0.027}\)</span>), we obtain the t-statistic, which in this case is 3.667. So, we know that our sample mean difference is 3.667 standard errors distant from 0 (i.e., the population mean under the null hypothesis). Finally, and crucially, we can see that the p-value is <span class="math inline">\(p=0.000247\)</span>. We are adopting a 5% significance level. Given that the estimated p-value is lower than 0.05, we can confidently reject the null the hypothesis. We can then say that the estimated mean difference is statistically significantly different from 0—implying that we are confident that that population mean difference is not zero.</p>
<p>We could do the same analysis for the estimated intercept (although, in reality, we are usually interested in the slope coefficient).</p>
<p>Similarly, we can also calculate confidence intervals for each estimate. We could manually calculate those intervals with the information available above (remember? The lower bound of the 95% confidence interval is given by <span class="math inline">\(Estimate - 1.96 * standard\_error\)</span> and the upper bound is <span class="math inline">\(Estimate + 1.96 * standard\_error\)</span>), but we can also ask <code>R</code> to do it for us.</p>
<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb582-1"><a href="statistical-inference.html#cb582-1" tabindex="-1"></a><span class="co"># compute 95% confidence interval of regression estimates</span></span>
<span id="cb582-2"><a href="statistical-inference.html#cb582-2" tabindex="-1"></a><span class="fu">confint</span>(linear_regression_1)</span></code></pre></div>
<pre><code>##                                 2.5 %     97.5 %
## (Intercept)              0.0007021627 0.04895231
## bcsvictimvictim of crime 0.0456649822 0.15057526</code></pre>
<p>The 95% confidence interval of the mean difference is <span class="math inline">\([0.046; 0.151]\)</span>. We can also calculate other confidence intervals.</p>
<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb584-1"><a href="statistical-inference.html#cb584-1" tabindex="-1"></a><span class="co"># compute 99% confidence interval of regression estimates</span></span>
<span id="cb584-2"><a href="statistical-inference.html#cb584-2" tabindex="-1"></a><span class="fu">confint</span>(linear_regression_1, <span class="at">level =</span> <span class="fl">0.99</span>)</span></code></pre></div>
<pre><code>##                                 0.5 %     99.5 %
## (Intercept)              -0.006881111 0.05653558
## bcsvictimvictim of crime  0.029176673 0.16706357</code></pre>
<p>What if the independent variable is numerical, and not binary? Exactly the same logic applies. Say we want to assess the association between people’s perceptions of anti-social behaviour in their neighbourhood, <code>tcarea</code>, and fear of violent crime. The hypothesis here is that people who live in areas characterised by more social disorganisation will be more fearful of violent crime. <code>tcarea</code>, as we know, is a numerical variable.</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb586-1"><a href="statistical-inference.html#cb586-1" tabindex="-1"></a><span class="co"># describe the &#39;tcarea&#39; variable</span></span>
<span id="cb586-2"><a href="statistical-inference.html#cb586-2" tabindex="-1"></a><span class="fu">summary</span>(csew_0708<span class="sc">$</span>tcarea)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
## -2.6735 -0.7943 -0.0942  0.0303  0.6420  4.1883     677</code></pre>
<p>This index is measured on a scale that ranges from -2.67, indicating people who do not perceive anti-social behaviour in their area at all, to 4.19, indicating people who perceive high levels of anti-social behaviour in their neighbourhood.</p>
<p>We can fit a regression model in which <code>tcviolent</code> is the dependent variable and <code>tcarea</code> is the independent variable. This model should tell us something about the association between both variables.</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="statistical-inference.html#cb588-1" tabindex="-1"></a><span class="co"># fit linear regression model assessting the association between &#39;tcarea&#39; and &#39;tcviolent&#39;</span></span>
<span id="cb588-2"><a href="statistical-inference.html#cb588-2" tabindex="-1"></a>linear_regression_2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(tcviolent <span class="sc">~</span> tcarea, <span class="at">data =</span> csew_0708)</span>
<span id="cb588-3"><a href="statistical-inference.html#cb588-3" tabindex="-1"></a></span>
<span id="cb588-4"><a href="statistical-inference.html#cb588-4" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb588-5"><a href="statistical-inference.html#cb588-5" tabindex="-1"></a>linear_regression_2</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = tcviolent ~ tcarea, data = csew_0708)
## 
## Coefficients:
## (Intercept)       tcarea  
##     0.03159      0.30921</code></pre>
<p>Now, the estimated slope coefficient is <span class="math inline">\(\widehat{\beta}=0.309\)</span>, indicating that every one-unit increase in the scale of <code>tcarea</code> is associated with an increase of 0.309 in the expected score of <code>tcviolent</code>.</p>
<p>Again, however, <span class="math inline">\(\widehat{\beta}=0.309\)</span> is only true in our sample. What we actually want to know is the population parameter, which is unknown. We can start with the null hypothesis: the slope coefficient is 0 in the population. We can then conduct a statistical test that provides evidence for or against the null hypothesis.</p>
<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb590-1"><a href="statistical-inference.html#cb590-1" tabindex="-1"></a><span class="co"># print results from the regression model with all information</span></span>
<span id="cb590-2"><a href="statistical-inference.html#cb590-2" tabindex="-1"></a><span class="fu">summary</span>(linear_regression_2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = tcviolent ~ tcarea, data = csew_0708)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9336 -0.6354 -0.1317  0.4731  3.9777 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.03159    0.01065   2.965  0.00303 ** 
## tcarea       0.30921    0.01081  28.595  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9536 on 8010 degrees of freedom
##   (3664 observations deleted due to missingness)
## Multiple R-squared:  0.09263,    Adjusted R-squared:  0.09251 
## F-statistic: 817.7 on 1 and 8010 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now, not only do we have a sample estimate of our regression coefficient, but we also have the estimated standard error, 0.011. This allows us to compute the t-statistic, indicating that our sample estimate is 28.6 standard errors distant from 0 (the assumed population parameter under the null hypothesis), and a p-value of <span class="math inline">\(p = 0.000000000000000022\)</span>. Given that the p-value is lower than our significance level of 5% (i.e., <span class="math inline">\(0.000000000000000022 &lt; 0.05\)</span>), we can confidently reject the null hypothesis and conclude that, under a 95% confidence level, the slope coefficient is not zero in the population (it is also true under other confidence levels, such as 99% and 99.9%, but conventionally we use the standard 95% level). The association between <code>tcarea</code> and <code>tcviolent</code> is <em>statistically significant</em>.</p>
</div>
<div id="statistical-inference-in-multiple-linear-regression-models" class="section level3 unnumbered hasAnchor">
<h3>Statistical inference in multiple linear regression models<a href="statistical-inference.html#statistical-inference-in-multiple-linear-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What about multiple linear regressions? No difference whatsoever. Each estimated parameter has its own (conditional) null hypothesis and its own statistical test. For example, let’s estimate a regression model in which fear of violent crime is the dependent variable and <code>tcarea</code>, <code>bcsvictim</code>, and <code>rural2</code> are all independent variables.</p>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb592-1"><a href="statistical-inference.html#cb592-1" tabindex="-1"></a><span class="co"># estimate a multiple linear regression</span></span>
<span id="cb592-2"><a href="statistical-inference.html#cb592-2" tabindex="-1"></a>linear_regression_3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(tcviolent <span class="sc">~</span> tcarea <span class="sc">+</span> bcsvictim <span class="sc">+</span> rural2, <span class="at">data =</span> csew_0708)</span>
<span id="cb592-3"><a href="statistical-inference.html#cb592-3" tabindex="-1"></a></span>
<span id="cb592-4"><a href="statistical-inference.html#cb592-4" tabindex="-1"></a><span class="co"># print results</span></span>
<span id="cb592-5"><a href="statistical-inference.html#cb592-5" tabindex="-1"></a>linear_regression_3</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = tcviolent ~ tcarea + bcsvictim + rural2, data = csew_0708)
## 
## Coefficients:
##              (Intercept)                    tcarea  bcsvictimvictim of crime  
##                 -0.06606                   0.29992                  -0.08577  
##              rural2urban  
##                  0.16274</code></pre>
<p>We can see that, controlling for victimisation and urban dwelling, every unit increase in <code>tcarea</code> is associated with an expected increase of 0.30 scores in <code>tcviolent</code>. We can also see that, controlling for perceived anti-social behaviour and urban dwelling, the expected score of fear of crime among victims is 0.089 scores <em>lower</em> than among non-victims (interesting flip sign in the multivariate model!). Finally, controlling for perceived anti-social behaviour and crime victimisation, the expected score of fear of crime among urban residents is 0.163 scores higher than among rural residents.</p>
<p>Yet, all of those coefficients are only true in the sample—they are sample estimates. They are a good guess for the population parameters, but we need to model the uncertainty around those estimates. Therefore, for each point estimate (i.e., for each coefficient), we also estimate a standard error, which we use to calculate the t-statistic and the p-value.</p>
<div class="sourceCode" id="cb594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb594-1"><a href="statistical-inference.html#cb594-1" tabindex="-1"></a><span class="co"># print results from the regression model with all information</span></span>
<span id="cb594-2"><a href="statistical-inference.html#cb594-2" tabindex="-1"></a><span class="fu">summary</span>(linear_regression_3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = tcviolent ~ tcarea + bcsvictim + rural2, data = csew_0708)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.7890 -0.6346 -0.1353  0.4820  3.9065 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              -0.06606    0.02066  -3.198  0.00139 ** 
## tcarea                    0.29992    0.01133  26.479  &lt; 2e-16 ***
## bcsvictimvictim of crime -0.08577    0.02681  -3.199  0.00138 ** 
## rural2urban               0.16274    0.02413   6.744 1.65e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9505 on 8008 degrees of freedom
##   (3664 observations deleted due to missingness)
## Multiple R-squared:  0.09864,    Adjusted R-squared:  0.09831 
## F-statistic: 292.1 on 3 and 8008 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can see that all three coefficients are statistically significant, as they all yield estimated p-values lower than our significance level of 0.05. Therefore, we conclude that all three null hypotheses can be rejected at the 5% significance level (or at the 95% confidence level), implying that we are confident that neither of those parameters is 0 in the population.</p>
<p><a href="http://www.sumsar.net/blog/2013/12/an-animation-of-the-construction-of-a-confidence-interval/">This blog post</a> provides a nice animation of the confidence interval and hypothesis testing.</p>
</div>
<div id="presenting-results-from-regression-analysis" class="section level3 unnumbered hasAnchor">
<h3>Presenting results from regression analysis<a href="statistical-inference.html#presenting-results-from-regression-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Communicating your results in a clear manner is incredibly important. We have seen the tabular results produced by <code>R</code>. If you want to use them in a paper (e.g., your final coursework!), you may need to do some tidying up of those results. There are a number of packages (e.g., <code>textreg</code>, <code>stargazer</code>) that automatise that process. They take your <code>lm</code> objects and produce tables that you can put straight away in your reports or papers. One popular trend in presenting results is the <strong>coefficient plot</strong> as an alternative to the table of regression coefficients. There are various ways of producing coefficient plots with <code>R</code> for a variety of models. See <a href="https://www.r-statistics.com/2010/07/visualization-of-regression-coefficients-in-r/">here</a>, for example.</p>
<p>We are going to use instead the <code>plot_model()</code> function of the <code>sjPlot</code> package, that makes it easier to produce this sort of plots. You can find a more detailed tutorial about this function <a href="http://rpubs.com/sjPlot/sjplm">here</a>. See below for an example:</p>
<div class="sourceCode" id="cb596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb596-1"><a href="statistical-inference.html#cb596-1" tabindex="-1"></a><span class="co"># install package if you haven&#39;t yet</span></span>
<span id="cb596-2"><a href="statistical-inference.html#cb596-2" tabindex="-1"></a><span class="do">## install.packages(&quot;sjPlot&quot;)</span></span>
<span id="cb596-3"><a href="statistical-inference.html#cb596-3" tabindex="-1"></a></span>
<span id="cb596-4"><a href="statistical-inference.html#cb596-4" tabindex="-1"></a><span class="co"># load sjPlot package</span></span>
<span id="cb596-5"><a href="statistical-inference.html#cb596-5" tabindex="-1"></a><span class="fu">library</span>(sjPlot)</span></code></pre></div>
<p>We can then use the <code>plot_model()</code> function to plot produce a coefficient plot.</p>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="statistical-inference.html#cb597-1" tabindex="-1"></a><span class="co"># produce a coefficient plot</span></span>
<span id="cb597-2"><a href="statistical-inference.html#cb597-2" tabindex="-1"></a><span class="fu">plot_model</span>(linear_regression_3)</span></code></pre></div>
<p><img src="09_statistical_inference_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>You can further customise this:</p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="statistical-inference.html#cb598-1" tabindex="-1"></a><span class="co"># add a title to the coefficient plot</span></span>
<span id="cb598-2"><a href="statistical-inference.html#cb598-2" tabindex="-1"></a><span class="fu">plot_model</span>(linear_regression_3, <span class="at">title =</span> <span class="st">&quot;Fear of violent crime&quot;</span>)</span></code></pre></div>
<p><img src="09_statistical_inference_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>What you see plotted here is the point estimates (the circles), the confidence intervals around those estimates (the longer the line, the less precise the estimate), and the colours represent whether the statistical effect is negative (red) or positive (blue). There are other packages that also provide similar functionality, like the <code>dotwhisker</code> package that you may want to explore, see more details <a href="https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html">here</a>.</p>
<p>The <code>sjPlot</code> package also allows you to produce html tables for more professional presentation of your regression tables. For this we use the <code>tab_model()</code> function. This kind of tabulation may be particularly helpful for your final assignment.</p>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb599-1"><a href="statistical-inference.html#cb599-1" tabindex="-1"></a><span class="co"># produce a nice looking table</span></span>
<span id="cb599-2"><a href="statistical-inference.html#cb599-2" tabindex="-1"></a><span class="fu">tab_model</span>(linear_regression_3)</span></code></pre></div>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
tcviolent
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.07
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.11 – -0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
tcarea
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.30
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.28 – 0.32
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
bcsvictim [victim of<br>crime]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.09
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.14 – -0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
rural2 [urban]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.16
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.12 – 0.21
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
8012
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.099 / 0.098
</td>
</tr>
</table>
<p>As before you can further customise this table. Let’s change for example the name that is displayed for the dependent variable.</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="statistical-inference.html#cb600-1" tabindex="-1"></a><span class="fu">tab_model</span>(linear_regression_3, <span class="at">dv.labels =</span> <span class="st">&quot;Fear of violent crime&quot;</span>)</span></code></pre></div>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
Fear of violent crime
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
(Intercept)
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.07
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.11 – -0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
tcarea
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.30
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.28 – 0.32
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
bcsvictim [victim of<br>crime]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.09
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.14 – -0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
rural2 [urban]
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.16
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.12 – 0.21
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
8012
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.099 / 0.098
</td>
</tr>
</table>
<p>Or you could change the labels for the independent variables:</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="statistical-inference.html#cb601-1" tabindex="-1"></a><span class="fu">tab_model</span>(linear_regression_3, </span>
<span id="cb601-2"><a href="statistical-inference.html#cb601-2" tabindex="-1"></a>          <span class="at">pred.labels =</span> <span class="fu">c</span>(<span class="st">&quot;Intercept&quot;</span>, <span class="st">&quot;Perceived anti-social behaviour&quot;</span>, <span class="st">&quot;Crime victimisation&quot;</span>, <span class="st">&quot;Urban residents&quot;</span>), </span>
<span id="cb601-3"><a href="statistical-inference.html#cb601-3" tabindex="-1"></a>          <span class="at">dv.labels =</span> <span class="st">&quot;Fear of violent crime&quot;</span>)</span></code></pre></div>
<table style="border-collapse:collapse; border:none;">
<tr>
<th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; ">
 
</th>
<th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; ">
Fear of violent crime
</th>
</tr>
<tr>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; ">
Predictors
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
Estimates
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
CI
</td>
<td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  ">
p
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Intercept
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.07
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.11 – -0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Perceived anti-social behaviour
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.30
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.28 – 0.32
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Crime victimisation
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.09
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
-0.14 – -0.03
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Urban residents
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.16
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
0.12 – 0.21
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  ">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;">
Observations
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3">
8012
</td>
</tr>
<tr>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3">
0.099 / 0.098
</td>
</tr>
</table>
<p>It is <strong>crucial</strong> that you always report sample estimates with measures of uncertainty (e.g., standard errors, confidence intervals, p-values, and/or t-statistics).</p>
</div>
<div id="statistical-inference-in-logistic-regression-models" class="section level3 unnumbered hasAnchor">
<h3>Statistical inference in logistic regression models<a href="statistical-inference.html#statistical-inference-in-logistic-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When is comes to the logistic regression, the same logic applies. Let’s again use the <code>Arrests</code> dataset from the <code>effects</code> package, as we did last week.</p>
<div class="sourceCode" id="cb602"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb602-1"><a href="statistical-inference.html#cb602-1" tabindex="-1"></a><span class="co"># install package if you haven&#39;t done so</span></span>
<span id="cb602-2"><a href="statistical-inference.html#cb602-2" tabindex="-1"></a><span class="do">## install.packages(&quot;effects&quot;)</span></span>
<span id="cb602-3"><a href="statistical-inference.html#cb602-3" tabindex="-1"></a></span>
<span id="cb602-4"><a href="statistical-inference.html#cb602-4" tabindex="-1"></a><span class="co"># load the &#39;effects&#39; package</span></span>
<span id="cb602-5"><a href="statistical-inference.html#cb602-5" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb602-6"><a href="statistical-inference.html#cb602-6" tabindex="-1"></a></span>
<span id="cb602-7"><a href="statistical-inference.html#cb602-7" tabindex="-1"></a><span class="co"># load the dataset &quot;Arrests&quot;</span></span>
<span id="cb602-8"><a href="statistical-inference.html#cb602-8" tabindex="-1"></a><span class="fu">data</span>(Arrests)</span></code></pre></div>
<p>Let’s fit the same logistic regression as we did last week, examining the (log) odds of being released with a summons instead of receiving a harsher treatment. We want to assess the extent to which ethnicity, sex, previous number of entries in the criminal justice system, and employment status are associated with highler or lower odds of being released with a summons.</p>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="statistical-inference.html#cb603-1" tabindex="-1"></a><span class="co"># Fit a logistic regression model and store it under &#39;logistic_reg&#39;</span></span>
<span id="cb603-2"><a href="statistical-inference.html#cb603-2" tabindex="-1"></a>logistic_reg <span class="ot">&lt;-</span> <span class="fu">glm</span>(released <span class="sc">~</span> colour <span class="sc">+</span> sex <span class="sc">+</span> checks <span class="sc">+</span> employed, <span class="at">data =</span> Arrests, <span class="at">family =</span> binomial)</span>
<span id="cb603-3"><a href="statistical-inference.html#cb603-3" tabindex="-1"></a></span>
<span id="cb603-4"><a href="statistical-inference.html#cb603-4" tabindex="-1"></a><span class="co"># print the estimated coefficients</span></span>
<span id="cb603-5"><a href="statistical-inference.html#cb603-5" tabindex="-1"></a>logistic_reg</span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = released ~ colour + sex + checks + employed, family = binomial, 
##     data = Arrests)
## 
## Coefficients:
## (Intercept)  colourWhite      sexMale       checks  employedYes  
##     1.40739      0.49608     -0.04215     -0.35796      0.77973  
## 
## Degrees of Freedom: 5225 Total (i.e. Null);  5221 Residual
## Null Deviance:       4776 
## Residual Deviance: 4331  AIC: 4341</code></pre>
<p>Yet, as we have repeatedly learned today, these coefficients are only true in our sample. We can use them to <em>estimate</em> the population parameters, but such parameters will remain unknown. Therefore, accompanying each regression coefficient, we can also estimate standard errors. Such standard errors can be used to compute useful statistical inference tools, such as the confidence intervals and the p-value.</p>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="statistical-inference.html#cb605-1" tabindex="-1"></a><span class="co"># print results from the regression model with all information</span></span>
<span id="cb605-2"><a href="statistical-inference.html#cb605-2" tabindex="-1"></a><span class="fu">summary</span>(logistic_reg)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = released ~ colour + sex + checks + employed, family = binomial, 
##     data = Arrests)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.40739    0.17243   8.162 3.30e-16 ***
## colourWhite  0.49608    0.08264   6.003 1.94e-09 ***
## sexMale     -0.04215    0.14965  -0.282    0.778    
## checks      -0.35796    0.02580 -13.875  &lt; 2e-16 ***
## employedYes  0.77973    0.08386   9.298  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 4776.3  on 5225  degrees of freedom
## Residual deviance: 4330.7  on 5221  degrees of freedom
## AIC: 4340.7
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>With logistic regression, we do not compute a <em>t-statistic</em>. Instead, we compute a <em>z-statistic</em>. This is because the underlying test uses the standard normal distribution instead of the t-student distribution. We do not need to get into that; with large sample sizes (our sample sizes our usually large enough!), they are exactly the same. We obtain z-statistics by dividing each estimated coefficient by its standard error, and the z-statistic will simply indicate how distant the sample estimate is from 0 (i.e., the assumed population parameter under the null hypothesis).</p>
<p>We can see, for example, that even though males have lower log-odds of being released with a summons than females (<span class="math inline">\(\widehat{\beta}=-0.042\)</span>), the standard error around this estimate is fairly large. Crucially, the p-value is also large—more specifically, larger than our significance level of 5% (i.e., <span class="math inline">\(0.778 &gt; 0.05\)</span>). Therefore, we fail to reject that null hypothesis and conclude that the association between sex and the log-odds of being released with a summons is not statistically significant—i.e., the coefficient could possibly be 0 in the population.</p>
<p>We also learned that coefficients in logistic regression are not directly interpretable, and that it is common to use <strong>odds ratios</strong> when interpreting such models. In the interest of modelling and communicating uncertainty around our estimates, it is common practice to produce confidence intervals around odds ratios.</p>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="statistical-inference.html#cb607-1" tabindex="-1"></a><span class="co"># produce odds ratios and 95% confidence intervals of odds ratios</span></span>
<span id="cb607-2"><a href="statistical-inference.html#cb607-2" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(logistic_reg), <span class="fu">confint</span>(logistic_reg))) </span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                    OR     2.5 %    97.5 %
## (Intercept) 4.0852619 2.9299967 5.7638450
## colourWhite 1.6422658 1.3957633 1.9298763
## sexMale     0.9587242 0.7096919 1.2770423
## checks      0.6990998 0.6644842 0.7352174
## employedYes 2.1808765 1.8493825 2.5693054</code></pre>
<p>Finally, we can also use <em>coefficient plots</em> in much the same way than we did for linear regression. One way of doing this is using the <code>plot_model()</code> function of the <code>sjPlot</code> package. Notice that the <code>plot_model()</code> function already produces a plot with odds ratios!</p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="statistical-inference.html#cb610-1" tabindex="-1"></a><span class="co"># produce a coefficient plot</span></span>
<span id="cb610-2"><a href="statistical-inference.html#cb610-2" tabindex="-1"></a><span class="fu">plot_model</span>(logistic_reg, <span class="at">title =</span> <span class="st">&quot;Odds of being released with a summons&quot;</span>)</span></code></pre></div>
<p><img src="09_statistical_inference_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>As an aside, you can use <a href="http://bcs.whfreeman.com/ips4e/cat_010/applets/confidenceinterval.html">this Java applet</a> to see what happens when one uses different parameters with confidence intervals. In the right hand side you will see a button that says “Sample”. Press there. This will produce a horizontal line representing the confidence interval. The left hand side end of the line represents the lower limit of the confidence interval and the right hand side end of the line represents the upper limit of the confidence interval. The red point in the middle is your sample mean, your point estimate. If you are lucky the line will be black and it will cross the green vertical line. This means that your CI covers the population mean. There will be a difference with your point estimate (i.e., your red point is unlikely to be just in top of the green vertical line). But, at least, the population parameter will be included within the range of plausible values for it that our confidence interval is estimating. If you keep pressing the “Sample” button (please do 30 or 50 times), you will see that most confidence intervals include the population parameter: most will cross the green line and will be represented by a black line. Sometimes your point estimate (the red point at the centre of the horizontal lines) will be to the right of the population mean (will be higher) or to the left (will be lower), but the confidence interval will include the population mean (and cross the green vertical line).<a href="statistical-inference.html#fnref11" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="categorical-variables-and-logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="wrapping-up.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
